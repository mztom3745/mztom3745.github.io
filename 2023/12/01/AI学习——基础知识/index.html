<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="AI学习记录1">
<meta property="og:type" content="article">
<meta property="og:title" content="AI学习——基础知识">
<meta property="og:url" content="http://example.com/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/index.html">
<meta property="og:site_name" content="mztom&#39;s Blog">
<meta property="og:description" content="AI学习记录1">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%A2%AF%E5%BA%A6%E5%B8%B8%E7%94%A8%E7%BB%93%E8%AE%BA.png">
<meta property="og:image" content="http://example.com/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E7%9F%A9%E9%98%B5%E5%B8%83%E5%B1%80.png">
<meta property="og:image" content="http://example.com/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%B8%83%E5%B1%80%E5%9B%BE.png">
<meta property="og:image" content="http://example.com/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%A0%87%E9%87%8F%E5%AF%B9%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E7%AE%97%E6%B3%95.png">
<meta property="og:image" content="http://example.com/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%90%91%E9%87%8F%E5%AF%B9%E5%90%91%E9%87%8F%E6%B1%82%E5%AF%BC%E7%AE%97%E6%B3%95.png">
<meta property="og:image" content="http://example.com/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD.png">
<meta property="og:image" content="http://example.com/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/softmax%E5%9B%9E%E5%BD%92.png">
<meta property="article:published_time" content="2023-12-01T07:28:13.000Z">
<meta property="article:modified_time" content="2023-12-13T12:22:46.024Z">
<meta property="article:author" content="mztom">
<meta property="article:tag" content="学习记录">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%A2%AF%E5%BA%A6%E5%B8%B8%E7%94%A8%E7%BB%93%E8%AE%BA.png">

<link rel="canonical" href="http://example.com/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>AI学习——基础知识 | mztom's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">mztom's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mztom">
      <meta itemprop="description" content="学习日志">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mztom's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AI学习——基础知识
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-01 15:28:13" itemprop="dateCreated datePublished" datetime="2023-12-01T15:28:13+08:00">2023-12-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-12-13 20:22:46" itemprop="dateModified" datetime="2023-12-13T20:22:46+08:00">2023-12-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">AI学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>AI学习记录1<br> <span id="more"></span></p>
<h1 id="part1-数学基础"><a href="#part1-数学基础" class="headerlink" title="part1:数学基础"></a>part1:数学基础</h1><h2 id="范数："><a href="#范数：" class="headerlink" title="范数："></a>范数：</h2><p>向量范数是将向量映射到标量的函数<em>f</em>，以下代码表示一个<em>L2</em>范数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure>
<h2 id="梯度："><a href="#梯度：" class="headerlink" title="梯度："></a>梯度：</h2><p>一个多元函数对其所有变量的偏导数。一个函数<em>f(x)</em>的输入是n维的向量，输出是一个标量。则函数<em>f(x)</em>相对于<em>x</em>的梯度是一个包含n个偏导数的向量，由于深度学习中，多元函数往往是复合的，所以一般使用链式法则来对复合函数求微分<br/><br>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。以下代码是一个简单的例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4.0</span>)</span><br><span class="line"><span class="comment">#创建梯度，标量函数关于向量x的梯度是向量，且具有相同形状</span></span><br><span class="line">x.requires_grad_(<span class="literal">True</span>)  <span class="comment"># 等价于x=torch.arange(4.0,requires_grad=True)</span></span><br><span class="line">y = <span class="number">2</span> * torch.dot(x, x) <span class="comment">#计算x和x的点积</span></span><br><span class="line">y.backward()<span class="comment">#4x</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()<span class="comment">#1</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><br>书上的梯度相关的数学的结论如下图：<br/><br><img src="梯度常用结论.png" alt="梯度结论"></p>
<h3 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/273729929">参考链接-知乎</a><br/><br>通过该链接，学习到矩阵求导的基本规则如下图：<img src="矩阵布局.png" alt="矩阵求导图"><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/262751195?utm_oi=628596077562040320&amp;utm_id=0">矩阵求导全解-知乎</a><br/><br>该链接详细介绍了矩阵求导的规范：<br/></p>
<h4 id="矩阵求导布局（视为规定）"><a href="#矩阵求导布局（视为规定）" class="headerlink" title="矩阵求导布局（视为规定）"></a>矩阵求导布局（视为规定）</h4><p>分子布局：导数的维度以分子为主<br/><br>分母布局：倒数的维度以分母为主<br/><br>在机器学习的算法推导里，通常遵循以下布局的规范：<br/></p>
<ul>
<li>如果向量或者矩阵对标量求导，则以分子布局为准。</li>
<li>如果标量对向量或者矩阵求导，则以分母布局为准。</li>
<li>对于向量对对向量求导，有些分歧，一般以分子布局的雅克比矩阵为主。<br><img src="布局图.png" alt="布局图"></li>
</ul>
<h4 id="标量对矩阵求导"><a href="#标量对矩阵求导" class="headerlink" title="标量对矩阵求导"></a>标量对矩阵求导</h4><p>算法如下图：<br/><br><img src="标量对矩阵求导算法.png" alt="标量对矩阵求导算法"></p>
<h4 id="矩阵对矩阵求导"><a href="#矩阵对矩阵求导" class="headerlink" title="矩阵对矩阵求导"></a>矩阵对矩阵求导</h4><p>算法如下图：<br/><br><img src="向量对向量求导算法.png" alt="向量对向量求导算法"></p>
<h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/261375491">参考链接-知乎</a><br/><br>$x \leftarrow x^2 - \eta\frac{df(x)}{dx}$<br/><br>梯度作为一个向量，代表一个方向，朝着梯度的反方向变化可以使得函数值变小，$\eta$代表学习率</p>
<h3 id="反向传播算法-Back-propagation"><a href="#反向传播算法-Back-propagation" class="headerlink" title="反向传播算法(Back propagation)"></a>反向传播算法(Back propagation)</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/71892752">参考链接-知乎</a><br/><br>反向传播仅指用于计算梯度的算法,其具体流程如下图所示，能够从最后一层的梯度算出前面层数的梯度<br><img src="反向传播.png" alt="反向传播"><br>深度学习框架通过自动计算导数，即自动微分来加快求导。实践中，根据设计好的模型，系统会构建一个计算图，来跟踪计算是<strong>哪些数据通过哪些操作组合起来来产生输出</strong>。自动微分使得系统能够随后反向传播梯度，即意味着跟踪整个计算图，填充关于每个参数的偏导数</p>
<h2 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h2><ol>
<li>联合概率(joint probability): $P(A=a,B=b)$</li>
<li>条件概率(conditional probability): $P(A=a|B=b)$</li>
<li>贝叶斯定理(multiplication rule): $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$</li>
<li>边际化: $P(B) = \sum_{A} P(A,B)$，一般和贝叶斯定理结合使用</li>
<li>独立性：$P(A|B)=\frac{P(A,B)}{P(B)}=P(A)$</li>
</ol>
<h1 id="part2-线性神经网络"><a href="#part2-线性神经网络" class="headerlink" title="part2:线性神经网络"></a>part2:线性神经网络</h1><h2 id="线性回归与线性模型"><a href="#线性回归与线性模型" class="headerlink" title="线性回归与线性模型"></a>线性回归与线性模型</h2><p>线性回归基于几个简单的假设：假设自变量<em>x</em>和因变量<em>y</em>之间的关系是线性的， 即<em>y</em>可以表示为<em>x</em>中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。<br/><br>数据集称为训练数据集（training data set） 或训练集（training set）。每行数据（比如一次房屋交易相对应的数据）称为样本（sample），也可以称为数据点（data point）或数据样本（data instance）。试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。<br/><br>基本的模型如下：<br/></p>
<script type="math/tex; mode=display">price = w_{area} * area + w_{age} * age + b</script><p>其中<em>b</em>是输入特征的仿射变换，其特点是通过加权和对特征进行线性变换。并通过偏置项进行平移（translation）。<br/><br>预测模型可用点积表示：</p>
<script type="math/tex; mode=display">\hat{y}=\boldsymbol{w}^T\boldsymbol{x}+b</script><p>或用矩阵向量积表示：</p>
<script type="math/tex; mode=display">\hat{\boldsymbol{y}}=\boldsymbol{X}\boldsymbol{w}+b</script><p>其中 $\boldsymbol{X}\in{\mathbb{R}^{n*d}}$ 其每一行是一个样本，一共有n个预测结果。该模型的模型参数为$\boldsymbol{w}$和$b$,为找到最佳的参数，还需要两个工具：<br/></p>
<ol>
<li>一种模型质量的度量方式</li>
<li>一种能够更新模型以提高模型预测质量的方法</li>
</ol>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>使用损失函数量化目标的<strong><em>实际值</em></strong>与<strong><em>预测值</em></strong>之间的差距，通常选择非负数作为损失，数值越小越好，完美预测时损失为0。线性回归往往用平方误差作为损失函数：</p>
<script type="math/tex; mode=display">\ell^{(i)}(\boldsymbol{w},b) = \frac{1}{2} \left( \hat{y}^{(i)} - y^{(i)} \right)^2</script><p>为了度量模型在整个数据集上的预测质量，需要计算在训练集<em>n</em>个样本上的损失均值:</p>
<script type="math/tex; mode=display">L(\boldsymbol{w},b)=\frac{1}{n}\sum_{i=1}^n\ell^{(i)}(\boldsymbol{w},b)=\frac{1}{n}\sum_{i=1}^n(\boldsymbol{w}^T\boldsymbol{x}^{(i)}+b-y^{(i)})</script><p>训练模型的目的是找到一组参数，这组参数能最小化在所有训练样本上的总损失</p>
<h2 id="解析解"><a href="#解析解" class="headerlink" title="解析解"></a>解析解</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/a755199443/article/details/104198033">参考链接-CSDN</a><br/><br>线性回归的解可用一个公式简单的表示，将<strong><em>b</em></strong>合并到<strong><em>w</em></strong>中，最小化$||\boldsymbol{y}-\boldsymbol{X}\boldsymbol{w}||^2$，得到解析解</p>
<script type="math/tex; mode=display">w^* = (X'X)^{-1}X'y</script><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>在无法得到解析解的情况下，我们也可以有效地训练模型。<strong><em>梯度下降</em></strong>（gradient descent）的通过不断地在损失函数递减的方向上更新参数来降低误差，但是每次更新参数都必须遍历整个数据集，为提高速度，通常在每次需要计算更新的时候随机抽取一小批样本。这种方法称为<strong><em>小批量随机梯度下降</em></strong>（minibatch stochastic gradient descent）。<br/></p>
<p>每次迭代抽取一个小批量$\mathcal{B}$，其由固定数量的训练样本组成。然后计算小批量的平均损失关于模型参数的导数。最后将梯度乘以一个预先确定的参数$\eta$，并从当前参数值减掉。</p>
<p>算法的步骤如下：（1）初始化模型参数的值（随机初始化）（2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。</p>
<script type="math/tex; mode=display">(\mathbf{w}, b) \leftarrow(\mathbf{w}, b)-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w}, b)} l^{(i)}(\mathbf{w}, b)</script><p>其中$|\mathcal{B}|$表示小批量中的样本数，即批量大小，而$\eta$表示学习率。这两个都是手动预先自定的，这些可以调整但不再训练过程中更新的参数成为<em>超参数</em>（hyperparameter）。<em>调参</em>（hyperparameter tuning）是选择超参数的过程。</p>
<h2 id="正态分布与平方损失"><a href="#正态分布与平方损失" class="headerlink" title="正态分布与平方损失"></a>正态分布与平方损失</h2><p>正态分布概率密度函数如下：</p>
<script type="math/tex; mode=display">p(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right)</script><p>均方误差损失函数可以用于线性回归的一个原因是：假设观测中包含了噪声且噪声服从正态分布，即</p>
<script type="math/tex; mode=display">y=\mathbf{w}^{\top} \mathbf{x}+b+\epsilon</script><p>其中，$\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)$.<br>现在可以写出通过给定的<strong>x</strong>观测到特定<em>y</em>的似然函数：</p>
<script type="math/tex; mode=display">P(y \mid \mathbf{x})=\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{1}{2 \sigma^2}\left(y-\mathbf{w}^{\top} \mathbf{x}-b\right)^2\right)</script><p>根据<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26614750">极大似然估计法</a>，参数<strong>w</strong>和<strong>b</strong>的最优值是使得整个数据集似然最大的值:</p>
<script type="math/tex; mode=display">P(\mathbf{y} \mid \mathbf{X})=\prod_{i=1}^n p\left(y^{(i)} \mid \mathbf{x}^{(i)}\right)</script><p>用最小化负对数进行进一步推导：</p>
<script type="math/tex; mode=display">-\log P(\mathbf{y} \mid \mathbf{X})=\sum_{i=1}^n \frac{1}{2} \log \left(2 \pi \sigma^2\right)+\frac{1}{2 \sigma^2}\left(y^{(i)}-\mathbf{w}^{\top} \mathbf{x}^{(i)}-b\right)^2</script><p>这样就使得最小化均方误差等价于对线性模型的极大似然估计了</p>
<h2 id="线性回归从零开始实现"><a href="#线性回归从零开始实现" class="headerlink" title="线性回归从零开始实现"></a>线性回归从零开始实现</h2><p>在每次迭代中，读取小批量训练，通过模型来获得一组预测。计算完损失后开始反向传播，存储每个参数的梯度。最后调用优化算法来更新模型参数。训练的轮数epoch和学习率lr都是超参数，从0开始实现线性回归的算法代码如下，具体参看书的3.2节<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##3.2.1生成数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    <span class="comment">#normal生成了一个张量，包含从均值为0，标准差为1的正态分布中随机抽取的随机数，num_examples表示生成张量的行数,len(w)表示生成张量的列数</span></span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    <span class="comment">#matmul用于矩阵乘法运算</span></span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape) <span class="comment">#加上高斯噪音，标准差是0.01</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"><span class="comment">#features每一行是一个二维数据样本，labels每一行包含一个标签值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;features:&#x27;</span>, features[<span class="number">0</span>],<span class="string">&#x27;\nlabel:&#x27;</span>, labels[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">d2l.plt.scatter(features[:, (<span class="number">1</span>)].detach().numpy(), labels.detach().numpy(), <span class="number">1</span>) <span class="comment">#绘制散点图</span></span><br><span class="line"><span class="comment">#d2l.plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.2读取数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    <span class="comment"># 这些样本是随机读取的，没有特定的顺序</span></span><br><span class="line">    random.shuffle(indices)<span class="comment">#打乱的数据顺序</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):<span class="comment">#batch_size为每个小批量的大小</span></span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])<span class="comment">#取出一组batch_size的索引值</span></span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class="line">        <span class="comment">#yield将一个函数转换为生成器函数，在调用生成器函数的过程中，每次遇到yield语句时，函数会暂停并保存当前所有的运行信息（保留局部变量），返回yield的值</span></span><br><span class="line">        </span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">##3.2.3初始化模型参数</span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.4定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br><span class="line">    <span class="comment">#当我们用一个向量加一个标量时，标量会被加到向量的每个分量上</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.5定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.6定义优化算法</span></span><br><span class="line"><span class="comment">#小批量随机梯度下降更新</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():<span class="comment">#禁用梯度计算，所有的require_grad被自动设置为false</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size <span class="comment">#lr是学习率,用batch_size批量大小来规范化步长</span></span><br><span class="line">            param.grad.zero_()<span class="comment">#清除梯度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.7训练</span></span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.03</span> <span class="comment">#学习率</span></span><br><span class="line">num_epochs = <span class="number">3</span> <span class="comment">#训练轮数</span></span><br><span class="line">net = linreg <span class="comment">#线性回归模型</span></span><br><span class="line">loss = squared_loss <span class="comment">#损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):<span class="comment">#data_iter遍历数据集，将训练数据集中的所有样本都使用一次（1000/10)</span></span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        <span class="comment"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()<span class="comment">#调用反向传播算法进行计算</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h2><p>对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量net，它是一个Sequential类的实例。 Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。这样的一个层称为全连接层，因为<strong>它的每一个输入都通过矩阵-向量乘法得到它的每个输出</strong><br/><br>通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。 我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。 当我们需要更复杂的模型时，高级API的优势将大大增加。 当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。下面是对比<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成数据集</span></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"><span class="comment">#调用框架中现有的API来读取数据，将特征和标签作为API的参数床底，通过数据迭代器指定batch_size</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line"><span class="comment">#定义模型，利用Sequential类直接完成</span></span><br><span class="line">  <span class="comment"># nn是神经网络的缩写</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#使用optim模块中的小批量随机梯度下降算法初始化SGD实例，指定要优化的参数（net.parameters()）以及优化算法所需的超参数字典</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        trainer.zero_grad()<span class="comment">#清零梯度</span></span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span> <span class="comment">#学习率</span></span><br><span class="line">num_epochs = <span class="number">3</span> <span class="comment">#训练轮数</span></span><br><span class="line">net = linreg <span class="comment">#线性回归模型</span></span><br><span class="line">loss = squared_loss <span class="comment">#损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):<span class="comment">#data_iter遍历数据集，将训练数据集中的所有样本都使用一次（1000/10)</span></span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        <span class="comment"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，</span></span><br><span class="line">        <span class="comment"># 并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>回归用于预测多少的问题，分类关心“哪一个”的问题。<br/><br>通过<strong>独热编码</strong>（one-hot encoding）可以进行分类数据的表示,softmax回归解决的就是数据份额里的问题。为了解决线性模型的分类问题，需要一个多个输出的模型和与输出一样多的<strong>仿射函数</strong>（affine function）。softmax回归也是一个单层的神经网络结构，如下图所示：<br/><br><img src="softmax回归.png" alt="softmax回归"><br>对于任何具有<em>d</em>个输入和 <em>q</em>个输出的全连接层，参数开销是$O(dq)$，这个开销可以减少到$O(dq/n)$，<em>n</em>可由我们灵活指定。<br/><br>我们希望模型的输出$\hat{y}_{j}$可以视为属于类$j$的概率,所以必须保证在任何数据上的输出都是非负的且总和为1。这时候就需要使用softmax函数如下式：<br/><br>$\begin{aligned} &amp; o_1=x_1 w_{11}+x_2 w_{12}+x_3 w_{13}+x_4 w_{14}+b_1 \\ &amp; o_2=x_1 w_{21}+x_2 w_{22}+x_3 w_{23}+x_4 w_{24}+b_2 \\ &amp; o_3=x_1 w_{31}+x_2 w_{32}+x_3 w_{33}+x_4 w_{34}+b_3\end{aligned}$<br/><br>$\hat{\mathbf{y}}=\operatorname{softmax}(\mathbf{o})$<br>其中：$\hat{y}_j=\frac{\exp \left(o_j\right)}{\sum_k \exp \left(o_k\right)}$<br/><br>softmax运算不会改变未规范化的预测<strong>o</strong>之间的大小次序，只会确定分配给每个类别的概率，尽管它是一个非线性函数，但其回归的输出仍然由输入特征的仿射变换决定，所以其仍是一个<strong>线性模型</strong>。<br/><br><br/><br>接下来需要一个损失函数，同样使用上文提到过的的<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26614750">极大似然估计</a>。极大似然估计的原理就是：<strong>利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值</strong>。<br/><br>极大似然估计中的采样必须满足独立同分布。对于一些情况，样本太多，无法得出分布的参数值，可以采样小样本后，利用极大似然估计获取假设中分布的参数值。<br/><br>假设整个数据集有{$\mathbf{X}$,$\mathbf{Y}$}具有n个样本，其中索引为<em>i</em>的样本由特征向量$\mathbf{x}^{(i)}$和独热标签向量$\mathbf{y}^{(i)}$组成。</p>
<script type="math/tex; mode=display">P(\mathbf{Y} \mid \mathbf{X})=\prod_{i=1}^n P\left(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}\right)</script><p>根据极大似然估计，可以最大化$P(Y|X)$,相当于最小化负对数似然：</p>
<script type="math/tex; mode=display">-\log P(\mathbf{Y} \mid \mathbf{X})=\sum_{i=1}^n-\log P\left(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}\right)=\sum_{i=1}^n l\left(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}\right)</script><p>其中对于任何标签$y$和模型预测$\hat{y}$，损失函数为：</p>
<script type="math/tex; mode=display">l(\mathbf{y}, \hat{\mathbf{y}})=-\sum_{j=1}^q y_j \log \hat{y}_j</script><p>这种损失函数成为交叉熵损失（cross-entropy loss）,它是一个两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数<br/><br><br/><br>对softmax的损失函数求导过程如下：<br/></p>
<script type="math/tex; mode=display">\begin{aligned} l(\mathbf{y}, \hat{\mathbf{y}}) & =-\sum_{j=1}^q y_j \log \frac{\exp \left(o_j\right)}{\sum_{k=1}^q \exp \left(o_k\right)} \\ & =\sum_{j=1}^q y_j \log \sum_{k=1}^q \exp \left(o_k\right)-\sum_{j=1}^q y_j o_j \\ & =\log \sum_{k=1}^q \exp \left(o_k\right)-\sum_{j=1}^q y_j o_j \end{aligned}</script><script type="math/tex; mode=display">\partial_{o_j}l(\mathbf{y},\hat{\mathbf{y}})=\frac{exp(o_j)}{\sum_{k=1}^{q}exp(o_k)}-y_j=softmax(\mathbf{(o)_j}-y_j</script><h2 id="softmax回归从零开始实现"><a href="#softmax回归从零开始实现" class="headerlink" title="softmax回归从零开始实现"></a>softmax回归从零开始实现</h2><p>此处将$28<em>28$像素的样本图像平展为784的向量（不利用图像空间结构的特征），由于数据集有10个类别，所以权重将构成$784</em>10$的矩阵，偏置为$1*10$的行向量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>接下来定义softmax的操作，利用sum的keepdim参数，指定保持在原始张量的轴数（参数1表示行求和）</p>
<script type="math/tex; mode=display">
\mathrm{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k \exp(\mathbf{X}_{ik})}.</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)<span class="comment">#行求和且保持为一个列向量</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制，将partition进行了扩展</span></span><br></pre></td></tr></table></figure>
<p>接下来定义softmax回归模型，利用reshape将图像展平为向量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape((-<span class="number">1</span>, W.shape[<span class="number">0</span>])), W) + b)</span><br></pre></td></tr></table></figure><br>然后定义损失函数（交叉熵）：</p>
<script type="math/tex; mode=display">l(\mathbf{y}, \hat{\mathbf{y}})=-\sum_{j=1}^q y_j \log \hat{y}_j</script><p>y_hat包含两个样本在三个类别上的预测概率，标签则是y,则使用PyTorch张量的花式索引写法，将y作为y_hat中概率的索引有：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line">y_hat[[<span class="number">0</span>, <span class="number">1</span>], y]</span><br><span class="line"></span><br><span class="line"><span class="comment">#[out]:tensor([0.1000, 0.5000])</span></span><br></pre></td></tr></table></figure><br>实现交叉熵函数如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"></span><br><span class="line">cross_entropy(y_hat, y)</span><br></pre></td></tr></table></figure><br>分类精度是正确预测数和总预测数之比，直接优化精度或许会很困难，但精度是我们最关心的性能度量标准，使用如下函数计算得到预测正确的数量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(y_hat)</span></span><br><span class="line">    <span class="comment">#print(y_hat.type(y.dtype))</span></span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><br>同样，对于任意数据迭代器data_iter可访问的数据集，我们可以评估在任意模型net的精度。Accumulator创建了两个变量用于存储正确预测数量和预测总数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><br>训练模型的一个迭代周期的代码逻辑如下，updater是更新模型参数的常用函数，它接受批量大小作为参数。 它可以是d2l.sgd函数，也可以是框架的内置优化函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            <span class="comment"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用定制的优化器和损失函数</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><br>可与上一节内容进行对比<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        trainer.zero_grad()<span class="comment">#清零梯度</span></span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><br>训练函数如下,在train_iter访问到的训练数据集上训练一个模型net，改训练函数会运行多个迭代周期（由num_epochs指定）。每个迭代周期结束时，利用test_iter问到的测试数据集对模型进行评估：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    <span class="keyword">assert</span> train_loss &lt; <span class="number">0.5</span>, train_loss</span><br><span class="line">    <span class="keyword">assert</span> train_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> train_acc &gt; <span class="number">0.7</span>, train_acc</span><br><span class="line">    <span class="keyword">assert</span> test_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> test_acc &gt; <span class="number">0.7</span>, test_acc</span><br><span class="line"><span class="comment">#使用sgd</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span> d2l.sgd([W, b], lr, batch_size)</span><br><span class="line"><span class="comment">#正式训练</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br></pre></td></tr></table></figure></p>
<h2 id="softmax回归的简洁实现"><a href="#softmax回归的简洁实现" class="headerlink" title="softmax回归的简洁实现"></a>softmax回归的简洁实现</h2><p>由于softmax回归的输出层是一个全连接层，所以我们只需要在Sequential中添加一个带有10个输出的全连接层即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化模型训练参数</span></span><br><span class="line"><span class="comment"># PyTorch不会隐式地调整输入的形状。因此，</span></span><br><span class="line"><span class="comment"># 我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><br>若$o_k$中的一些数值非常大，那么$\exp(o_k)$可能大于数据类型容许的最大数字，即<em>上溢</em>（overflow）。解决这个问题的一个技巧是：在继续softmax计算之前，先从所有$o_k$中减去$\max(o_k)$。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat y_j & =  \frac{\exp(o_j - \max(o_k))\exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k))\exp(\max(o_k))} \\
& = \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.
\end{aligned}</script><p>在减法和规范化步骤之后，可能有些$o_j - \max(o_k)$具有较大的负值。<br>由于精度受限，$\exp(o_j - \max(o_k))$将有接近零的值，即<em>下溢</em>（underflow）。<br>通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\log{(\hat y_j)} & = \log\left( \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\
& = \log{(\exp(o_j - \max(o_k)))}-\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} \\
& = o_j - \max(o_k) -\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)}.
\end{aligned}</script><p>交叉熵损失、优化算法、训练过程如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)<span class="comment">#自带了softmax运算</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment">#trainer可以通用</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"><span class="comment">#替换了net,loss,trainer模块</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure></p>

    </div>

    
    
    
    
    
     <div>
     <div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
     </div>
    
      
      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" rel="tag"># 学习记录</a>
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/11/29/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0debug%E8%AE%B0%E5%BD%951/" rel="prev" title="日常学习debug记录1">
      <i class="fa fa-chevron-left"></i> 日常学习debug记录1
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/12/04/%E7%BB%98%E7%94%BB%E8%AF%BE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="next" title="绘画课学习笔记">
      绘画课学习笔记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#part1-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"><span class="nav-number">1.</span> <span class="nav-text">part1:数学基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8C%83%E6%95%B0%EF%BC%9A"><span class="nav-number">1.1.</span> <span class="nav-text">范数：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%EF%BC%9A"><span class="nav-number">1.2.</span> <span class="nav-text">梯度：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC"><span class="nav-number">1.2.1.</span> <span class="nav-text">矩阵求导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E5%B8%83%E5%B1%80%EF%BC%88%E8%A7%86%E4%B8%BA%E8%A7%84%E5%AE%9A%EF%BC%89"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">矩阵求导布局（视为规定）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%87%E9%87%8F%E5%AF%B9%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">标量对矩阵求导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%AF%B9%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">矩阵对矩阵求导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.2.</span> <span class="nav-text">梯度下降算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95-Back-propagation"><span class="nav-number">1.2.3.</span> <span class="nav-text">反向传播算法(Back propagation)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87"><span class="nav-number">1.3.</span> <span class="nav-text">概率</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#part2-%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.</span> <span class="nav-text">part2:线性神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.</span> <span class="nav-text">线性回归与线性模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="nav-number">2.3.</span> <span class="nav-text">解析解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">2.4.</span> <span class="nav-text">随机梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E4%B8%8E%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1"><span class="nav-number">2.5.</span> <span class="nav-text">正态分布与平方损失</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.6.</span> <span class="nav-text">线性回归从零开始实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.7.</span> <span class="nav-text">线性回归的简洁实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax%E5%9B%9E%E5%BD%92"><span class="nav-number">2.8.</span> <span class="nav-text">softmax回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax%E5%9B%9E%E5%BD%92%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.9.</span> <span class="nav-text">softmax回归从零开始实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.10.</span> <span class="nav-text">softmax回归的简洁实现</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">mztom</p>
  <div class="site-description" itemprop="description">学习日志</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://github.com/CS-BAOYAN/" title="https:&#x2F;&#x2F;github.com&#x2F;CS-BAOYAN&#x2F;" rel="noopener" target="_blank">计算机保研网</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://zh.d2l.ai/" title="http:&#x2F;&#x2F;zh.d2l.ai&#x2F;" rel="noopener" target="_blank">《动手学深度学习》</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://snip.mathpix.com/home" title="https:&#x2F;&#x2F;snip.mathpix.com&#x2F;home" rel="noopener" target="_blank">latex在线转换</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-11 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">mztom</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共10.7k字</span>
</div>


        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
