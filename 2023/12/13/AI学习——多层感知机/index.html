<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="AI学习记录2">
<meta property="og:type" content="article">
<meta property="og:title" content="AI学习——多层感知机">
<meta property="og:url" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/index.html">
<meta property="og:site_name" content="mztom&#39;s Blog">
<meta property="og:description" content="AI学习记录2">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA.png">
<meta property="og:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/Relu.png">
<meta property="og:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/sigmoid-1.png">
<meta property="og:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/sigmoid-2.png">
<meta property="og:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/tanh.png">
<meta property="og:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%AF%B9%E6%8B%9F%E5%90%88%E7%A8%8B%E5%BA%A6%E7%9A%84%E5%BD%B1%E5%93%8D.png">
<meta property="og:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/%E4%B8%89%E9%98%B6%E5%A4%9A%E9%A1%B9%E5%BC%8F.png">
<meta property="og:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/%E9%AB%98%E9%98%B6%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/%E6%9A%82%E9%80%80%E6%B3%95.png">
<meta property="og:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97%E5%9B%BE.png">
<meta property="article:published_time" content="2023-12-13T08:42:02.000Z">
<meta property="article:modified_time" content="2023-12-19T10:54:25.031Z">
<meta property="article:author" content="mztom">
<meta property="article:tag" content="学习记录">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA.png">

<link rel="canonical" href="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>AI学习——多层感知机 | mztom's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">mztom's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mztom">
      <meta itemprop="description" content="学习日志">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mztom's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AI学习——多层感知机
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-13 16:42:02" itemprop="dateCreated datePublished" datetime="2023-12-13T16:42:02+08:00">2023-12-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-12-19 18:54:25" itemprop="dateModified" datetime="2023-12-19T18:54:25+08:00">2023-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">AI学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>AI学习记录2<br> <span id="more"></span></p>
<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><h2 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h2><p>如果标签通过放射变换后确实和输入数据相关，那么上一章的内容就以及足够。但是实际上线性的假设往往难以成立：线性意味着单调，任何特征的增大都会导致模型输出的增大或者模型输出的减小，但如同<em>用体温预测死亡率</em>，对<em>猫狗图像分类</em>的这一系列问题，线性的假设是难以成立的。<br/><br>所以我们需要考虑特征之间的相关交互作用，即使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。可以通过在网络中加入一个或多个隐藏层来突破线性模型的限制。<br><img src="多层感知机.png" alt="多层感知机"><br>该多层感知机由4个输入、3个输出，隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。所以层数为2，且两个层是全连接的，参数开销很大。<br/><br>隐藏层的输出用$\mathbf{H} \in \mathbb{R}^{n \times h}$表示，称为<em>隐藏表示</em>（hidden representations）。也称<em>隐藏层变量</em>（hidden-layer variable）或<em>隐藏变量</em>（hidden variable）。<br>因为隐藏层和输出层都是全连接的，<br>所以我们有隐藏层权重$\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$<br>和隐藏层偏置$\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$<br>以及输出层权重$\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$<br>和输出层偏置$\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$。<br>形式上，我们按如下方式计算单隐藏层多层感知机的输出<br>$\mathbf{O} \in \mathbb{R}^{n \times q}$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.
\end{aligned}</script><p>但如果仅仅如此的话相较于单层模型是没有益处的，因为本质上仍是仿射函数：</p>
<script type="math/tex; mode=display">
\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.</script><p>所以需要非线性的<em>激活函数</em>（activation function）$\sigma$。<br>激活函数的输出（例如，$\sigma(\cdot)$）被称为<em>活性值</em>（activations）。<br>一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{aligned}</script><p>由于$\mathbf{X}$中的每一行对应于小批量中的一个样本，<br>出于记号习惯的考量，<br>我们定义非线性函数$\sigma$也以按行的方式作用于其输入，<br>即一次计算一个样本。<br>但激活函数往往<em>可以按元素操作</em>，意味着在计算每一层的线性部分之后，我们可以计算每个活性值，<br>而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="1-ReLU函数"><a href="#1-ReLU函数" class="headerlink" title="1. ReLU函数"></a>1. ReLU函数</h3><p>ReLU全名修正激活单元（rectified linear unit,ReLU）其实现简单，表现良好。它要么让参数消失，要么让参数通过，缓解了以往神经网络的梯度消失问题。可以采用参数化ReLU（Parameterized ReLU，<em>pReLU</em>）使得负的参数也能够通过。</p>
<script type="math/tex; mode=display">\operatorname{ReLU}(x) = \max(x, 0)</script><script type="math/tex; mode=display">\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)</script><p><img src="Relu.png" alt="Relu"></p>
<h3 id="2-sigmoid函数"><a href="#2-sigmoid函数" class="headerlink" title="2. sigmoid函数"></a>2. sigmoid函数</h3><p><strong>对于一个定义域在$\mathbb{R}$中的输入，<em>sigmoid函数</em>将输入变换为区间(0, 1)上的输出</strong></p>
<script type="math/tex; mode=display">\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}</script><p><img src="sigmoid-1.png" alt="sigmoid-1"><br>其导数是</p>
<script type="math/tex; mode=display">\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right)</script><p><img src="sigmoid-2.png" alt="sigmoid-2"></p>
<h3 id="3-tanh函数"><a href="#3-tanh函数" class="headerlink" title="3. tanh函数"></a>3. tanh函数</h3><p>该函数类似于sigmoid,将输入压缩转换到区间(-1,1)上。</p>
<script type="math/tex; mode=display">\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}</script><p><img src="tanh.png" alt="tanh"></p>
<h2 id="多层感知机从零开始实现"><a href="#多层感知机从零开始实现" class="headerlink" title="多层感知机从零开始实现"></a>多层感知机从零开始实现</h2><p>代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = nn.Parameter(torch.randn(</span><br><span class="line">    num_inputs, num_hiddens, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="literal">True</span>))</span><br><span class="line">W2 = nn.Parameter(torch.randn(</span><br><span class="line">    num_hiddens, num_outputs, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"><span class="comment">#激活函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X, a)</span><br><span class="line"><span class="comment">#模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(X@W1 + b1)  <span class="comment"># 这里“@”代表矩阵乘法</span></span><br><span class="line">    <span class="keyword">return</span> (H@W2 + b2)</span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">num_epochs, lr = <span class="number">10</span>, <span class="number">0.1</span></span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></table></figure></p>
<h2 id="多层感知机的简洁实现"><a href="#多层感知机的简洁实现" class="headerlink" title="多层感知机的简洁实现"></a>多层感知机的简洁实现</h2><p>模型实现如下，与softmax可以进行对比<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line">net.apply(init_weights)<span class="comment">#;</span></span><br></pre></td></tr></table></figure><br>对比softmax代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></p>
<h1 id="模型选择、欠拟合和过拟合"><a href="#模型选择、欠拟合和过拟合" class="headerlink" title="模型选择、欠拟合和过拟合"></a>模型选择、欠拟合和过拟合</h1><p>机器学习的目标是发现某些模式（pattern），这些模式捕获到训练集潜在的总体规律。如何发现可以泛化的模式是机器学习的根本问题。<br/><br>将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。<br/><br><br/><br><em>训练误差</em>（training error）是指，<br>模型在训练数据集上计算得到的误差。<br><em>泛化误差</em>（generalization error）是指，<br>模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。但我们永远不能准确地计算出泛化误差。<br/></p>
<h2 id="1-统计学习理论"><a href="#1-统计学习理论" class="headerlink" title="1.统计学习理论"></a>1.统计学习理论</h2><p>训练数据和测试数据往往假设是从相同的分布中独立提取的。这通常被称为<em>独立同分布假设</em>，意味着对数据进行抽样的过程没有进行“记忆”。但是实际上这样的假设难以成立，如不同地区，不同时间的抽样往往会影响样本，从而违背独立同分布假设。</p>
<h2 id="2-模型复杂性"><a href="#2-模型复杂性" class="headerlink" title="2.模型复杂性"></a>2.模型复杂性</h2><p>统计学家认为，能够轻松解释任意事实的模型是复杂的，<br>而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。</p>
<p>事实上，具有更多参数的模型可能被认为更复杂，<br>参数有更大取值范围的模型可能更为复杂。<br>通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂，<br>而需要<em>早停</em>（early stopping）的模型（即较少训练迭代周期）就不那么复杂。</p>
<h2 id="3-验证集"><a href="#3-验证集" class="headerlink" title="3.验证集"></a>3.验证集</h2><p>在确定超参数之前，不希望用到测试集，即不能依靠测试数据来进行模型选择。<br>但也不能仅仅依靠训练数据来选择模型，因为这样便无法估计训练数据的泛化误差。</p>
<p>常见做法是将数据分成三份，除了训练和测试数据集之外，还需要增加<em>验证数据集</em>（validation dataset），也称<em>验证集</em>（validation set），虽然实际上验证集和测试集的边界十分模糊。</p>
<p>在训练数据稀缺时，存在一种解决方案是是采用$K$<em>折交叉验证</em>。<br>这里，原始训练数据被分成$K$个不重叠的子集。<br>然后执行$K$次模型训练和验证，每次在$K-1$个子集上进行训练，<br>并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。<br>最后，通过对$K$次实验的结果取平均来估计训练和验证误差。</p>
<h1 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h1><p>下面用多项式的例子来说明过拟合和模型复杂性的经典直觉。<br>给定由单个特征$x$和对应实数标签$y$组成的训练数据，<br>我们试图找到下面的$d$阶多项式来估计标签$y$。</p>
<script type="math/tex; mode=display">\hat{y}= \sum_{i=0}^d x^i w_i</script><p>这是一个简单的线性回归问题，而高阶多项式函数的复杂性则会比低阶多项式函数复杂得多，其<strong>参数较多</strong>，<strong>模型函数的选择范围较广</strong>。<br>因此在固定训练数据集的情况下，<br>高阶多项式函数相对于低阶多项式的训练误差应该始终更低。<br>事实上，当数据样本包含了$x$的不同值时， 函数阶数等于数据样本数量的多项式函数可以完美拟合训练集。<br><img src="模型复杂度对拟合程度的影响.png" alt="模型复杂度对拟合程度的影响"></p>
<p>同时数据集的大小会影响拟合程度。训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。给出更多的数据，我们可能会尝试拟合一个更复杂的模型。</p>
<p>对应的代码如下：<br>第一步生成测试数据，采用以下三阶多项式来生成训练和测试数据的标签</p>
<script type="math/tex; mode=display">y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{ where }
\epsilon \sim \mathcal{N}(0, 0.1^2).</script><p>在优化的过程中，我们通常希望避免非常大的梯度值或损失值。<br>这就是我们将特征从$x^i$调整为$\frac{x^i}{i!}$的原因，<br>这样可以避免很大的$i$带来的特别大的指数值。<br>我们将为训练集和测试集各生成100个样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">max_degree = <span class="number">20</span>  <span class="comment"># 多项式的最大阶数</span></span><br><span class="line">n_train, n_test = <span class="number">100</span>, <span class="number">100</span>  <span class="comment"># 训练和测试数据集大小</span></span><br><span class="line">true_w = np.zeros(max_degree)  <span class="comment"># 分配大量的空间</span></span><br><span class="line">true_w[<span class="number">0</span>:<span class="number">4</span>] = np.array([<span class="number">5</span>, <span class="number">1.2</span>, -<span class="number">3.4</span>, <span class="number">5.6</span>])</span><br><span class="line"></span><br><span class="line">features = np.random.normal(size=(n_train + n_test, <span class="number">1</span>))<span class="comment">#偏置1</span></span><br><span class="line">np.random.shuffle(features)</span><br><span class="line">poly_features = np.power(features, np.arange(max_degree).reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_degree):</span><br><span class="line">    poly_features[:, i] /= math.gamma(i + <span class="number">1</span>)  <span class="comment"># gamma(n)=(n-1)!</span></span><br><span class="line"><span class="comment"># labels的维度:(n_train+n_test,)</span></span><br><span class="line">labels = np.dot(poly_features, true_w)</span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>, size=labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy ndarray转换为tensor</span></span><br><span class="line">true_w, features, poly_features, labels = [torch.tensor(x, dtype=</span><br><span class="line">    torch.float32) <span class="keyword">for</span> x <span class="keyword">in</span> [true_w, features, poly_features, labels]]</span><br></pre></td></tr></table></figure>
<p>下面对模型进行训练和测试<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#评估损失</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 损失的总和,样本数量</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#定义训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features, test_features, train_labels, test_labels,</span></span><br><span class="line"><span class="params">          num_epochs=<span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 不设置偏置，因为我们已经在多项式中实现了它</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, <span class="number">1</span>, bias=<span class="literal">False</span>))</span><br><span class="line">    batch_size = <span class="built_in">min</span>(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                                batch_size)</span><br><span class="line">    test_iter = d2l.load_array((test_features, test_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                               batch_size, is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">1e-3</span>, <span class="number">1e2</span>],</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>, net[<span class="number">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure><br>最后是拟合情况：</p>
<ol>
<li>三阶多项式函数拟合<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">4</span>], poly_features[n_train:, :<span class="number">4</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>
<img src="三阶多项式.png" alt="三阶多项式"></li>
<li>线性函数拟合（欠拟合）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前2个维度，即1和x</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>
<img src="线性函数.png" alt="线性函数"></li>
<li>高阶多项式函数拟合（过拟合）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选取所有维度</span></span><br><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br></pre></td></tr></table></figure>
<img src="高阶多项式函数.png" alt="高阶多项式函数"></li>
</ol>
<h1 id="权重衰减（-L-2-正则化）"><a href="#权重衰减（-L-2-正则化）" class="headerlink" title="权重衰减（$L_2$正则化）"></a>权重衰减（$L_2$正则化）</h1><p>假设我们已经拥有了尽可能多的高质量数据，就可以将重点放在正则化技术上来防止过拟合。<br>限制特征的数量是环节过拟合的一种常用技术，简单地丢弃特征对这项工作而言太过于生硬，需要更为细粒度的工具来调整复杂度。</p>
<p>一种简单的方法是通过线性函数<br>$f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$<br>中的权重向量的某个范数来度量其复杂性，<br>例如$| \mathbf{w} |^2$。<br>要保证权重向量比较小，<br>最常用方法是将其范数作为惩罚项加到最小化损失的问题中。<br>将原来的训练目标<em>最小化训练标签上的预测损失</em>，<br>调整为<em>最小化预测损失和惩罚项之和</em>。<br>现在，如果我们的权重向量增长的太大，<br>我们的学习算法可能会更集中于最小化权重范数$| \mathbf{w} |^2$。</p>
<p>举之前的线性函数的例子，损失如下：</p>
<script type="math/tex; mode=display">L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2</script><p>$\mathbf{x}^{(i)}$是样本$i$的特征，<br>$y^{(i)}$是样本$i$的标签，<br>$(\mathbf{w}, b)$是权重和偏置参数。<br>通过<em>正则化常数</em>$\lambda$来平衡$| \mathbf{w} |^2$描述的损失，使用验证数据拟合：</p>
<script type="math/tex; mode=display">L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2,</script><p>使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。<br>这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。<br>在实践中，这可能使它们对单个变量中的观测误差更为稳定。<br>相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上，<br>而将其他权重清除为零。<br>这称为<em>特征选择</em>（feature selection），这可能是其他场景下需要的。</p>
<p>$L_2$正则化回归的小批量随机梯度下降更新如下式，一般网络输出层的偏置项不会被正则化：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).
\end{aligned}</script><p>接下来用一个简单例子来展现权重衰减：</p>
<p>首先是生成数据,问题的位数增加到$d = 200$，<br>并使用一个只包含20个样本的小训练集。</p>
<script type="math/tex; mode=display">y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where }
\epsilon \sim \mathcal{N}(0, 0.01^2)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_train, n_test, num_inputs, batch_size = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">5</span></span><br><span class="line">true_w, true_b = torch.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line">train_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class="line">train_iter = d2l.load_array(train_data, batch_size)</span><br><span class="line">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class="line">test_iter = d2l.load_array(test_data, batch_size, is_train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>初始化模型参数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>():</span><br><span class="line">    w = torch.normal(<span class="number">0</span>, <span class="number">1</span>, size=(num_inputs, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure><br>定义$L_2$范数乘法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">l2_penalty</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(w.<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">2</span></span><br></pre></td></tr></table></figure><br>训练代码如下，其将模型拟合训练数据集，并在测试数据集上进行评估：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">lambd</span>):</span><br><span class="line">    w, b = init_params()</span><br><span class="line">    net, loss = <span class="keyword">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># 增加了L2范数惩罚项，</span></span><br><span class="line">            <span class="comment"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span></span><br><span class="line">            l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item())</span><br></pre></td></tr></table></figure><br>使用权重衰减进行拟合后效果更好。</p>
<p>可以在实例化优化器时直接通过weight_decay指定weight decay超参数。<br>默认情况下，PyTorch同时衰减权重和偏移。<br>这里我们只为权重设置了weight_decay，所以偏置参数不会衰减。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    <span class="comment"># 偏置参数没有衰减</span></span><br><span class="line">    trainer = torch.optim.SGD([</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].weight,<span class="string">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].bias&#125;], lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>,</span><br><span class="line">                         (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                          d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数：&#x27;</span>, net[<span class="number">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure></p>
<h1 id="暂退法（Dropout）"><a href="#暂退法（Dropout）" class="headerlink" title="暂退法（Dropout）"></a>暂退法（Dropout）</h1><p>面对更多特征而样本不足时，线性模型往往会过拟合。<br>给出更多样本而不是特征时，线性模型不会过拟合。<br>但线性模型没有考虑到特征之间的相互作用。<br>对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p>
<p>与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。<br>例如，神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件，但单独出现则不表示垃圾邮件。<br>而且即使我们有比特征多得多的样本，深度神经网络也有可能过拟合。</p>
<p>经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。<br>简单性以<strong>较小维度</strong>的形式展现，在权重衰减的讨论中可以看到参数的范数也代表了一种有用的简单性度量<br>简单性的另一个角度是<strong>平滑性</strong>，即函数不应该对其输入的微小变化敏感。基于此，学者提出了一个想法：在前向传播过程中，计算每一内部层的同时注入噪声，这就是我们要提到的<em>暂退法</em>（dropout）。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。</p>
<p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。<br>换言之，每个中间活性值$h$以<em>暂退概率</em>$p$由随机变量$h’$替换，如下所示：</p>
<script type="math/tex; mode=display">
\begin{aligned}
h' =
\begin{cases}
    0 & \text{ 概率为 } p \\
    \frac{h}{1-p} & \text{ 其他情况}
\end{cases}
\end{aligned}</script><p>根据此模型的设计，其期望值保持不变，即$E[h’] = h$。对之前的多层感知机运用效果如下图：<br><img src="暂退法.png" alt="暂退法"></p>
<p>代码流程如下：</p>
<p>实现dropout_layer函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X, dropout</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= dropout &lt;= <span class="number">1</span></span><br><span class="line">    <span class="comment"># 在本情况中，所有元素都被丢弃</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="comment"># 在本情况中，所有元素都被保留</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    mask = (torch.rand(X.shape) &gt; dropout).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> mask * X / (<span class="number">1.0</span> - dropout)</span><br></pre></td></tr></table></figure><br>定义模型参数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br></pre></td></tr></table></figure><br>定义模型，可以为每一层分别设置暂退概率，常见的技巧是在靠近输入层的地方设置较低的暂退概率，暂退法只在训练期间有效：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">dropout1, dropout2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span></span><br><span class="line"><span class="params">                 is_training = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.training = is_training</span><br><span class="line">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class="line">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class="line">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        H1 = self.relu(self.lin1(X.reshape((-<span class="number">1</span>, self.num_inputs))))</span><br><span class="line">        <span class="comment"># 只有在训练模型时才使用dropout</span></span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">            H1 = dropout_layer(H1, dropout1)</span><br><span class="line">        H2 = self.relu(self.lin2(H1))</span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">            H2 = dropout_layer(H2, dropout2)</span><br><span class="line">        out = self.lin3(H2)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></table></figure><br>训练与测试类似前文。而简洁实现中，只需在每个全连接层之后添加一个Dropout层， 将暂退概率作为唯一的参数传递给它的构造函数。在训练时，Dropout层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。在测试时，Dropout层仅传递数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment">#net.apply(init_weights);</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure></p>
<h1 id="前向传播、反向传播和计算图"><a href="#前向传播、反向传播和计算图" class="headerlink" title="前向传播、反向传播和计算图"></a>前向传播、反向传播和计算图</h1><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。</p>
<script type="math/tex; mode=display">\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x}</script><p>其中$\mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$<br>是隐藏层的权重参数。<br>将中间变量$\mathbf{z}\in \mathbb{R}^h$通过激活函数$\phi$后，<br>我们得到长度为$h$的隐藏激活向量：</p>
<script type="math/tex; mode=display">\mathbf{h}= \phi (\mathbf{z})</script><p>隐藏变量$\mathbf{h}$也是一个中间变量。<br>假设输出层的参数只有权重$\mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$，<br>我们可以得到输出层变量，它是一个长度为$q$的向量：</p>
<script type="math/tex; mode=display">\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}.</script><p>根据$L_2$正则化的定义，给定超参数$\lambda$，正则化项为：</p>
<script type="math/tex; mode=display">s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2\right)</script><p>其中矩阵的Frobenius范数是将矩阵展平为向量后应用的$L_2$范数。<br>最后，模型在给定数据样本上的正则化损失为：</p>
<script type="math/tex; mode=display">J = L + s.</script><p>我们将$J$称为<em>目标函数</em>（objective function）。前向传播的计算图如下：<br> 其中正方形表示变量，圆圈表示操作符。<br> 左下角表示输入，右上角表示输出。<br> 注意显示数据流的箭头方向主要是向右和向上的。<br><img src="前向传播计算图.png" alt="前向传播计算图"></p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="　反向传播"></a>　反向传播</h2><p><em>反向传播</em>（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。<br>简言之，该方法根据微积分中的<em>链式规则</em>，按相反的顺序从输出层到输入层遍历网络。<br>该算法存储了计算某些参数梯度时所需的<strong>任何中间变量</strong>（偏导数）。</p>
<p>假设我们有函数$\mathsf{Y}=f(\mathsf{X})$和$\mathsf{Z}=g(\mathsf{Y})$，<br>其中输入和输出$\mathsf{X}, \mathsf{Y}, \mathsf{Z}$是任意形状的张量。<br>利用链式法则，我们可以计算$\mathsf{Z}$关于$\mathsf{X}$的导数</p>
<script type="math/tex; mode=display">\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right)</script><p>在这里，我们使用$\text{prod}$运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。<br>对于向量，这很简单，它只是矩阵-矩阵乘法。<br>对于高维张量，我们使用适当的对应项。<br>运算符$\text{prod}$指代了所有的这些符号。<br>回想一下，在上文计算图中的单隐藏层简单网络的参数是<br>$\mathbf{W}^{(1)}$和$\mathbf{W}^{(2)}$。<br>反向传播的目的是计算梯度$\partial J/\partial \mathbf{W}^{(1)}$和<br>$\partial J/\partial \mathbf{W}^{(2)}$。<br>为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。<br>计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。具体流程查看<a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html">反向传播</a></p>
<h2 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h2><p>在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。</p>
<p>以上述简单网络为例：一方面，在前向传播期间计算正则项取决于模型参数$\mathbf{W}^{(1)}$和$\mathbf{W}^{(2)}$的当前值。<br>它们是由优化算法根据最近迭代的反向传播给出的。<br>另一方面，反向传播期间参数 :eqref:<code>eq_backprop-J-h</code>的梯度计算，<br>取决于由前向传播给出的隐藏变量$\mathbf{h}$的当前值。</p>
<p>因此，在训练神经网络时，在初始化模型参数后，<br>我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。<br>注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。<br>带来的影响之一是我们需要保留中间值，直到反向传播完成。<br>这也是训练比单纯的预测需要更多的内存（显存）的原因之一。<br>此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。<br>因此，使用更大的批量来训练更深层次的网络更容易导致<em>内存不足</em>（out of memory）错误。</p>
<h1 id="数值稳定性和模型初始化"><a href="#数值稳定性和模型初始化" class="headerlink" title="数值稳定性和模型初始化"></a>数值稳定性和模型初始化</h1><p>若选取的初始参数不当，容易威胁到优化算法的稳定性：要么是要么是梯度爆炸（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛； 要么是梯度消失（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p>
<p>sigmoid函数容易发生梯度消失的问题，可采用ReLU函数</p>
<p>神经网络中存在参数化固有对成型的问题，可以采用暂退法正则化打破这样的对称性</p>
<p>参数初始化可以考虑Xavier初始化，通常，Xavier初始化从均值为零，方差<br>$\sigma^2 = \frac{2}{n_\mathrm{in} + n_\mathrm{out}}$<br>的高斯分布中采样权重。<br>我们也可以将其改为选择从均匀分布中抽取权重时的方差。<br>注意均匀分布$U(-a, a)$的方差为$\frac{a^2}{3}$。<br>将$\frac{a^2}{3}$代入到$\sigma^2$的条件中，将得到初始化值域：</p>
<script type="math/tex; mode=display">U\left(-\sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}, \sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}\right)</script><p>虽然有“不存在非线性”的假设，但实践证明是有效的。</p>
<h1 id="环境与分布偏移"><a href="#环境与分布偏移" class="headerlink" title="环境与分布偏移"></a>环境与分布偏移</h1><p>有时，根据测试集的精度衡量，模型表现得非常出色。<br>但是当数据分布突然改变时，模型在部署中会出现灾难性的失败。<br>更隐蔽的是，有时模型的部署本身就是扰乱数据分布的催化剂。<br>机器学习的许多应用中都存在类似的问题：<br>通过将基于模型的决策引入环境，我们可能会破坏模型。</p>
<h2 id="偏移类型"><a href="#偏移类型" class="headerlink" title="偏移类型"></a>偏移类型</h2><p>偏移的类型有如下几种：</p>
<ol>
<li>协变量偏移：虽然输入的分布可能随时间而改变，<br>但标签函数（即条件分布$P(y \mid \mathbf{x})$）没有改变，<br>在我们认为$\mathbf{x}$导致$y$的情况下，协变量偏移是一种自然假设。</li>
<li>标签偏移：假设标签边缘概率$P(y)$可以改变，<br>但是类别条件分布$P(\mathbf{x} \mid y)$在不同的领域之间保持不变。<br>当我们认为$y$导致$\mathbf{x}$时，标签偏移是一个合理的假设。</li>
<li>概念偏移：类别会随着不同时间的用法而发生变化。<br>精神疾病的诊断标准、所谓的时髦、以及工作头衔等等，都是概念偏移的日常映射。</li>
</ol>
<h2 id="分布偏移纠正"><a href="#分布偏移纠正" class="headerlink" title="分布偏移纠正"></a>分布偏移纠正</h2><p>_ramain to be understood_</p>

    </div>

    
    
    
    
    
     <div>
     <div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
     </div>
    
      
      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" rel="tag"># 学习记录</a>
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/12/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95/" rel="prev" title="论文阅读方法">
      <i class="fa fa-chevron-left"></i> 论文阅读方法
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/12/19/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/" rel="next" title="AI学习——深度学习计算">
      AI学习——深度学习计算 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">1.</span> <span class="nav-text">多层感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82"><span class="nav-number">1.1.</span> <span class="nav-text">隐藏层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-ReLU%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.1.</span> <span class="nav-text">1. ReLU函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-sigmoid%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.2.</span> <span class="nav-text">2. sigmoid函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-tanh%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.3.</span> <span class="nav-text">3. tanh函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.3.</span> <span class="nav-text">多层感知机从零开始实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.4.</span> <span class="nav-text">多层感知机的简洁实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-number">2.</span> <span class="nav-text">模型选择、欠拟合和过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA"><span class="nav-number">2.1.</span> <span class="nav-text">1.统计学习理论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E6%80%A7"><span class="nav-number">2.2.</span> <span class="nav-text">2.模型复杂性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="nav-number">2.3.</span> <span class="nav-text">3.验证集</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="nav-number">3.</span> <span class="nav-text">多项式回归</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F%EF%BC%88-L-2-%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%89"><span class="nav-number">4.</span> <span class="nav-text">权重衰减（$L_2$正则化）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9A%82%E9%80%80%E6%B3%95%EF%BC%88Dropout%EF%BC%89"><span class="nav-number">5.</span> <span class="nav-text">暂退法（Dropout）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="nav-number">6.</span> <span class="nav-text">前向传播、反向传播和计算图</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">6.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">6.2.</span> <span class="nav-text">　反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">6.3.</span> <span class="nav-text">训练神经网络</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">7.</span> <span class="nav-text">数值稳定性和模型初始化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%B8%8E%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB"><span class="nav-number">8.</span> <span class="nav-text">环境与分布偏移</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%81%8F%E7%A7%BB%E7%B1%BB%E5%9E%8B"><span class="nav-number">8.1.</span> <span class="nav-text">偏移类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB%E7%BA%A0%E6%AD%A3"><span class="nav-number">8.2.</span> <span class="nav-text">分布偏移纠正</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">mztom</p>
  <div class="site-description" itemprop="description">学习日志</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://github.com/CS-BAOYAN/" title="https:&#x2F;&#x2F;github.com&#x2F;CS-BAOYAN&#x2F;" rel="noopener" target="_blank">计算机保研网</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://zh.d2l.ai/" title="http:&#x2F;&#x2F;zh.d2l.ai&#x2F;" rel="noopener" target="_blank">《动手学深度学习》</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://snip.mathpix.com/home" title="https:&#x2F;&#x2F;snip.mathpix.com&#x2F;home" rel="noopener" target="_blank">latex在线转换</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-11 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">mztom</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共23.4k字</span>
</div>


        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
