<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>读书日记</title>
      <link href="/2023/12/04/%E8%AF%BB%E4%B9%A6%E6%97%A5%E8%AE%B0/"/>
      <url>/2023/12/04/%E8%AF%BB%E4%B9%A6%E6%97%A5%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="c2dbc6532e8486eea7dc358f0a2da6d3b55addf9e42c12a0129b9aef9e076aa0">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 日常摸鱼 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随便写写 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>绘画课学习笔记</title>
      <link href="/2023/12/04/%E7%BB%98%E7%94%BB%E8%AF%BE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/04/%E7%BB%98%E7%94%BB%E8%AF%BE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="77b1c85eddfcedc15bbb1f7b12d0a8a19de03f56fed201f0d496546304ef8bf3">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 日常摸鱼 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI学习1——基础知识</title>
      <link href="/2023/12/01/AI%E5%AD%A6%E4%B9%A01%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
      <url>/2023/12/01/AI%E5%AD%A6%E4%B9%A01%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>AI学习记录1<br> <span id="more"></span></p><h1 id="part1-数学基础"><a href="#part1-数学基础" class="headerlink" title="part1:数学基础"></a>part1:数学基础</h1><h2 id="范数："><a href="#范数：" class="headerlink" title="范数："></a>范数：</h2><p>向量范数是将向量映射到标量的函数<em>f</em>，以下代码表示一个<em>L2</em>范数<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = torch.tensor([3.0, -4.0])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure></p><h2 id="梯度："><a href="#梯度：" class="headerlink" title="梯度："></a>梯度：</h2><p>一个多元函数对其所有变量的偏导数。一个函数<em>f(x)</em>的输入是n维的向量，输出是一个标量。则函数<em>f(x)</em>相对于<em>x</em>的梯度是一个包含n个偏导数的向量，由于深度学习中，多元函数往往是复合的，所以一般使用链式法则来对复合函数求微分<br/><br>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。以下代码是一个简单的例子<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(4.0)</span><br><span class="line">#创建梯度，标量函数关于向量x的梯度是向量，且具有相同形状</span><br><span class="line">x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)</span><br><span class="line">y = 2 * torch.dot(x, x) #计算x和x的点积</span><br><span class="line">y.backward()#4x</span><br><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x.sum()</span><br><span class="line">y.backward()#1</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><br>书上的梯度相关的数学的结论如下图：<br/><br><img src="梯度常用结论.png" alt="梯度结论"></p><h3 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h3><p><a href="https://zhuanlan.zhihu.com/p/273729929">参考链接-知乎</a><br/><br>通过该链接，学习到矩阵求导的基本规则如下图：<img src="矩阵布局.png" alt="矩阵求导图"><br><a href="https://zhuanlan.zhihu.com/p/262751195?utm_oi=628596077562040320&amp;utm_id=0">矩阵求导全解-知乎</a><br/><br>该链接详细介绍了矩阵求导的规范：<br/></p><h4 id="矩阵求导布局（视为规定）"><a href="#矩阵求导布局（视为规定）" class="headerlink" title="矩阵求导布局（视为规定）"></a>矩阵求导布局（视为规定）</h4><p>分子布局：导数的维度以分子为主<br/><br>分母布局：倒数的维度以分母为主<br/><br>在机器学习的算法推导里，通常遵循以下布局的规范：<br/></p><ul><li>如果向量或者矩阵对标量求导，则以分子布局为准。</li><li>如果标量对向量或者矩阵求导，则以分母布局为准。</li><li>对于向量对对向量求导，有些分歧，一般以分子布局的雅克比矩阵为主。<br><img src="布局图.png" alt="布局图"></li></ul><h4 id="标量对矩阵求导"><a href="#标量对矩阵求导" class="headerlink" title="标量对矩阵求导"></a>标量对矩阵求导</h4><p>算法如下图：<br/><br><img src="标量对矩阵求导算法.png" alt="标量对矩阵求导算法"></p><h4 id="矩阵对矩阵求导"><a href="#矩阵对矩阵求导" class="headerlink" title="矩阵对矩阵求导"></a>矩阵对矩阵求导</h4><p>算法如下图：<br/><br><img src="向量对向量求导算法.png" alt="向量对向量求导算法"></p><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p><a href="https://zhuanlan.zhihu.com/p/261375491">参考链接-知乎</a><br/><br>$x \leftarrow x^2 - \eta\frac{df(x)}{dx}$<br/><br>梯度作为一个向量，代表一个方向，朝着梯度的反方向变化可以使得函数值变小，$\eta$代表学习率</p><h3 id="反向传播算法-Back-propagation"><a href="#反向传播算法-Back-propagation" class="headerlink" title="反向传播算法(Back propagation)"></a>反向传播算法(Back propagation)</h3><p><a href="https://zhuanlan.zhihu.com/p/71892752">参考链接-知乎</a><br/><br>反向传播仅指用于计算梯度的算法</p><h2 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h2><ol><li>联合概率(joint probability): $P(A=a,B=b)$</li><li>条件概率(conditional probability): $P(A=a|B=b)$</li><li>贝叶斯定理(multiplication rule): $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$</li><li>边际化: $P(B) = \sum_{A} P(A,B)$，一般和贝叶斯定理结合使用</li><li>独立性：$P(A|B)=\frac{P(A,B)}{P(B)}=P(A)$</li></ol><h2 id="遗留问题"><a href="#遗留问题" class="headerlink" title="遗留问题"></a>遗留问题</h2><p>P43 2.5.2非标量变量的反向传播、2.5.3分离计算</p><h1 id="part2-线性神经网络"><a href="#part2-线性神经网络" class="headerlink" title="part2:线性神经网络"></a>part2:线性神经网络</h1><h2 id="线性回归与线性模型"><a href="#线性回归与线性模型" class="headerlink" title="线性回归与线性模型"></a>线性回归与线性模型</h2><p>线性回归基于几个简单的假设：假设自变量<em>x</em>和因变量<em>y</em>之间的关系是线性的， 即<em>y</em>可以表示为<em>x</em>中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。<br/><br>数据集称为训练数据集（training data set） 或训练集（training set）。每行数据（比如一次房屋交易相对应的数据）称为样本（sample），也可以称为数据点（data point）或数据样本（data instance）。试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。<br/><br>基本的模型如下：<br/></p><script type="math/tex; mode=display">price = w_{area} * area + w_{age} * age + b</script><p>其中<em>b</em>是输入特征的仿射变换，其特点是通过加权和对特征进行线性变换。并通过偏置项进行平移（translation）。<br/><br>预测模型可用点积表示：</p><script type="math/tex; mode=display">\hat{y}=\boldsymbol{w}^T\boldsymbol{x}+b</script><p>或用矩阵向量积表示：</p><script type="math/tex; mode=display">\hat{\boldsymbol{y}}=\boldsymbol{X}\boldsymbol{w}+b</script><p>其中 $\boldsymbol{X}\in{\mathbb{R}^{n*d}}$ 其每一行是一个样本，一共有n个预测结果。该模型的模型参数为$\boldsymbol{w}$和$b$,为找到最佳的参数，还需要两个工具：<br/></p><ol><li>一种模型质量的度量方式</li><li>一种能够更新模型以提高模型预测质量的方法</li></ol><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>使用损失函数量化目标的<strong><em>实际值</em></strong>与<strong><em>预测值</em></strong>之间的差距，通常选择非负数作为损失，数值越小越好，完美预测时损失为0。线性回归往往用平方误差作为损失函数：</p><script type="math/tex; mode=display">\ell^{(i)}(\boldsymbol{w},b) = \frac{1}{2} \left( \hat{y}^{(i)} - y^{(i)} \right)^2</script><p>为了度量模型在整个数据集上的预测质量，需要计算在训练集<em>n</em>个样本上的损失均值:</p><script type="math/tex; mode=display">L(\boldsymbol{w},b)=\frac{1}{n}\sum_{i=1}^n\ell^{(i)}(\boldsymbol{w},b)=\frac{1}{n}\sum_{i=1}^n(\boldsymbol{w}^T\boldsymbol{x}^{(i)}+b-y^{(i)})</script><p>训练模型的目的是找到一组参数，这组参数能最小化在所有训练样本上的总损失</p><h2 id="解析解"><a href="#解析解" class="headerlink" title="解析解"></a>解析解</h2><p><a href="https://blog.csdn.net/a755199443/article/details/104198033">参考链接-CSDN</a><br/><br>线性回归的解可用一个公式简单的表示，将<strong><em>b</em></strong>合并到<strong><em>w</em></strong>中，最小化$||\boldsymbol{y}-\boldsymbol{X}\boldsymbol{w}||^2$，得到解析解</p><script type="math/tex; mode=display">w^* = (X'X)^{-1}X'y</script><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>在无法得到解析解的情况下，我们也可以有效地训练模型。<strong><em>梯度下降</em></strong>（gradient descent）的通过不断地在损失函数递减的方向上更新参数来降低误差，但是每次更新参数都必须遍历整个数据集，为提高速度，通常在每次需要计算更新的时候随机抽取一小批样本。这种方法称为<strong><em>小批量随机梯度下降</em></strong>（minibatch stochastic gradient descent）。<br/></p><p>每次迭代抽取一个小批量$\mathcal{B}$，其由固定数量的训练样本组成。然后计算小批量的平均损失关于模型参数的导数。最后将梯度乘以一个预先确定的参数$\eta$，并从当前参数值剪掉。</p><p>算法的步骤如下：（1）初始化模型参数的值（随机初始化）（2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。</p><script type="math/tex; mode=display">(\mathbf{w}, b) \leftarrow(\mathbf{w}, b)-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w}, b)} l^{(i)}(\mathbf{w}, b)</script><p>其中$|\mathcal{B}|$表示小批量中的样本数，即批量大小，而$\eta$表示学习率。这两个都是手动预先自定的，这些可以调整但不再训练过程中更新的参数成为<em>超参数</em>（hyperparameter）。<em>调参</em>（hyperparameter tuning）是选择超参数的过程。</p><h2 id="矢量化加速"><a href="#矢量化加速" class="headerlink" title="矢量化加速"></a>矢量化加速</h2>]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日常学习debug记录1</title>
      <link href="/2023/11/29/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0debug%E8%AE%B0%E5%BD%951/"/>
      <url>/2023/11/29/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0debug%E8%AE%B0%E5%BD%951/</url>
      
        <content type="html"><![CDATA[<p>记录日常的遇到的一些问题以及解决方法<br> <span id="more"></span></p><h1 id="AI-studying"><a href="#AI-studying" class="headerlink" title="AI studying"></a>AI studying</h1><ol><li><p>虚拟环境<br/><br>在anaconda Prompt中，使用<code>conda create --name d2l python=3.9 -y</code>创建虚拟环境，使用<code>activate pytorch</code>激活虚拟环境，用<code>pip list</code>查看安装列表，用<code>conda deactivate</code>来退出环境<br/><br>在虚拟环境中安装AI全家桶时注意版本的对应，使用<code>print(torch.__version__)</code>查看torch的版本；使用<code>python --version</code>查看python版本；使用<code>nvidia-smi</code>查看CUDA版本,若安装CUDA Toolkit则使用<code>nvcc -V</code>查看运行时API版本</p></li><li><p>cuda的安装<br/><br>参考连接如下：<br/><br><a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">conda换源-清华</a><br/><br><a href="https://blog.csdn.net/jhsignal/article/details/111401628">cuda和cuDNN的安装</a><br/><br><a href="https://blog.csdn.net/qq_46126258/article/details/112708781">torch的GPU版本安装的系列问题</a>,<a href="https://blog.csdn.net/youtiankeng/article/details/127759029">版本对应图</a><br/><br>1.13.0版本的pytorch对应0.14.0的torchvision和3.7到3.10的python以及0.13.0的torchaudio，为了安装GPU正确版本（0.13.0及以上才有gpu 版本）的pytorch,先删除CPU版本的pytorch,使用<code>conda uninstall pytorch</code>或是使用<code>pip uninstall torch</code>去卸载（前者未换国内源或会失败）<br/><br>然后去官网安装对应的版本<a href="https://download.pytorch.org/whl/torch/">torch</a>，<a href="https://download.pytorch.org/whl/torchvision/">torchvision</a>，<a href="https://download.pytorch.org/whl/torchaudio/">torchaudio</a><br/><br>torch-1.13.0+cu116-cp37-cp37m-win_amd64.whl，意为torch1.13.0、cuda11.6、python3.7、win 64位系统的版本,故我们安装<code>torch-1.13.0+cu117-cp37-cp37m-win_amd64.whl</code>和<code>torchvision-0.14.0+cu117-cp37-cp37m-win_amd64</code>和<code>torchaudio-0.13.0+cu117-cp37-cp37m-win_amd64</code>三个whl文件,然后进入虚拟环境安装，分别使用<code>pip install torch-1.13.0.whl+cu117-cp37-cp37m-win_amd64.whl</code>和<code>pip install torchvision-0.14.0+cu117-cp37-cp37m-win_amd64.whl</code>以及<code>pip install torchaudio-0.13.0+cu117-cp37-cp37m-win_amd64.whl</code>安装</p></li><li><p>vscode配置python运行调试环境</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 日常debug </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建记录</title>
      <link href="/2023/11/29/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/"/>
      <url>/2023/11/29/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>该文章记录了操作该博客系统所需要的指令和注意事项<br> <span id="more"></span></p><h1 id="主要参考教程"><a href="#主要参考教程" class="headerlink" title="主要参考教程"></a>主要参考教程</h1><p><a href="https://www.zhihu.com/search?type=content&amp;q=%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA">知乎博客搭建详解</a><br/></p><h1 id="相关指令"><a href="#相关指令" class="headerlink" title="相关指令"></a>相关指令</h1><ol><li>使用<code>hexo new &lt;title&gt;</code>更新文章</li><li>使用<code>hexo clean</code>清除旧的数据；使用<code>hexo g</code>生成新页面；使用<code>hexo d</code>部署到网页上，即<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></li><li>用<code>hexo s</code>预览网页效果</li><li>通过<code>https://mztom3745.com.cn</code>访问博客页面</li></ol><hr><h1 id="markdown写作"><a href="#markdown写作" class="headerlink" title="markdown写作"></a>markdown写作</h1><p>主要参考:<br/><br><a href="https://markdown.com.cn/cheat-sheet.html#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95">markdown官方教程</a><br/><br><a href="https://blog.csdn.net/wzk4869/article/details/126863936">markdown公式语法</a><br/><br><a href="https://zhuanlan.zhihu.com/p/544093361">hexo插入公式</a><br/><br><a href="https://blog.csdn.net/qq_42951560/article/details/123596899">hexo-markdown渲染器</a><br/><br>发现hexo的mathjax不支持求和、属于等公式<br><a href="https://bugwz.com/2019/09/17/hexo-markdown-renderer/#1-2%E3%80%81hexo-renderer-kramed">各个渲染版本</a><br/><br><a href="https://zhuanlan.zhihu.com/p/35988761">高级公式支持</a><br/><br><a href="https://blog.csdn.net/weixin_44441126/article/details/119745642">冲突解决</a><br/><br><a href="https://www.cnblogs.com/iHeling/p/14095986.html">kramed语义冲突</a><br/><br><a href="https://blog.csdn.net/qq_43401552/article/details/104805466">markdown+hexo+next图片插入</a><br/><br><a href="https://blog.csdn.net/qq_42951560/article/details/122552109">博客加密码</a><br/></p>]]></content>
      
      
      <categories>
          
          <category> 网页搭建 </category>
          
          <category> 个人博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
