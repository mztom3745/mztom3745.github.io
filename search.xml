<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>AI学习——深度学习计算</title>
      <link href="/2023/12/19/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/"/>
      <url>/2023/12/19/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<p>AI学习记录4<br> <span id="more"></span></p><h1 id="层与块"><a href="#层与块" class="headerlink" title="层与块"></a>层与块</h1><p>线性模型中，它<br>（1）接收一些输入；<br>（2）生成相应的标量输出；<br>（3）具有一组相关参数。<br>考虑多个输出的网络时，层<br>（1）接受一组输入；<br>（2）生成相应的输出；<br>（3）由一组可调整参数描述。</p><p>为了实现更为复杂的网络，引入神经网络块的概念。<br><em>块</em>（block）可以描述单个层、由多个层组成的组件或整个模型本身。<br>使用块进行抽象的一个好处是可以将一些块组合成更大的组件，<br>这一过程通常是递归的，<br>可以通过定义代码来按需生成任意复杂度的块，<br>我们可以通过简洁的代码实现复杂的神经网络。</p><p>回顾之前的多层感知机,其中<code>net(X)</code>实际上是<code>net.__call__(X)</code>的简写<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">20</span>)</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><br>下面的代码实现了一个多层感知机，其具有256个单元的隐藏层和10维的输出层。实现需要提供构造函数和前向传播函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span></span><br><span class="line">        <span class="comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#1.初始化nn.Module类中定义的任何属性</span></span><br><span class="line">        <span class="comment">#2.执行父类的构造逻辑</span></span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># 隐藏层</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br></pre></td></tr></table></figure></p><h1 id="顺序块"><a href="#顺序块" class="headerlink" title="顺序块"></a>顺序块</h1><p>接下来看如何实现我们自己的Sequential类，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">            <span class="comment"># 这里，module是Module子类的一个实例。我们把它保存在&#x27;Module&#x27;类的成员</span></span><br><span class="line">            <span class="comment"># 变量_modules中。_module的类型是OrderedDict</span></span><br><span class="line">            self._modules[<span class="built_in">str</span>(idx)] = module</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><br>其中每一个Module都有个_modules属性，其主要的优点是：在模块的参数初始化过程中， 系统知道在_modules字典中查找需要初始化参数的子块。</p><p>有时我们可能希望合并既不是上一层的结果也不是可更新参数的项，<br>我们称之为<em>常数参数</em>（constant parameter）。<br>例如，我们需要一个计算函数<br>$f(\mathbf{x},\mathbf{w}) = c \cdot \mathbf{w}^\top \mathbf{x}$的层，<br>其中$\mathbf{x}$是输入，<br>$\mathbf{w}$是参数，<br>$c$是某个在优化过程中没有更新的指定常量。<br>所以我们实现了一个<code>FixedHiddenMLP</code>类，如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FixedHiddenMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span></span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 使用创建的常量参数以及relu和mm函数</span></span><br><span class="line">        X = F.relu(torch.mm(X, self.rand_weight) + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 复用全连接层。这相当于两个全连接层共享参数</span></span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 控制流</span></span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:</span><br><span class="line">            X /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><br>在这个FixedHiddenMLP模型中，我们实现了一个隐藏层， 其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 <br/><br>这个权重不是一个模型参数，因此它永远不会被反向传播更新<br>然后，神经网络将这个固定层的输出通过一个全连接层。</p><p>在返回输出之前，模型做了一些不寻常的事情：<br>它运行了一个while循环，在$L_1$范数大于$1$的条件下，<br>将输出向量除以$2$，直到它满足条件为止。<br>最后，模型返回了<code>X</code>中所有项的和。<br>（这些行为仅供代码添加方法展示，并无理论上的含义）</p><p>我们还可以<strong>混合搭配各种组合块的方法</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)<span class="comment">#nn.Linear是全连接层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure></p><h1 id="参数-parameters-管理"><a href="#参数-parameters-管理" class="headerlink" title="参数(parameters)管理"></a>参数(parameters)管理</h1><h2 id="参数访问"><a href="#参数访问" class="headerlink" title="参数访问"></a>参数访问</h2><p>在选择了架构并设置了超参数后，我们就进入了训练阶段。<br>此时，我们的目标是找到使损失函数最小化的模型参数值。<br>经过训练后，我们将需要使用这些参数来做出未来的预测。</p><p>下面以具有单隐藏层的多层感知机为例来介绍参数管理：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><br>可以使用<code>print(net[2].state_dict())</code>获得第0层的参数</p><p>以下代码从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)<span class="comment">#获取值、梯度和额外信息</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br></pre></td></tr></table></figure><br>参数是复合的对象，包含值、梯度和额外信息。 这就是我们需要显式参数值的原因。<br>由于还未调用反向传播，所以参数的梯度还处于初始状态,即<code>net[2].weight.grad == None</code>为真</p><p>以下代码能够一次性地访问所有的参数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br></pre></td></tr></table></figure></p><p>同时块也能实现一些嵌套：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 在这里嵌套</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br></pre></td></tr></table></figure><br>可以通过<code>print(rgnet)</code>去查看嵌套的细节</p><h2 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h2><p>内置的初始化器如下,其将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_normal)</span><br></pre></td></tr></table></figure><br>也可以对某些块应用不同的初始化方法:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_xavier</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_42</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">net[<span class="number">0</span>].apply(init_xavier)</span><br><span class="line">net[<span class="number">2</span>].apply(init_42)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data)</span><br></pre></td></tr></table></figure></p><p>也可以自定义初始化方法对参数进行初始化，举例如下：<br><img src="自定义初始化.png" alt="自定义初始化"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Init&quot;</span>, *[(name, param.shape)</span><br><span class="line">                        <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>])</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line">net[<span class="number">0</span>].weight[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI学习——多层感知机</title>
      <link href="/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>AI学习记录2<br> <span id="more"></span></p><h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><h2 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h2><p>如果标签通过放射变换后确实和输入数据相关，那么上一章的内容就以及足够。但是实际上线性的假设往往难以成立：线性意味着单调，任何特征的增大都会导致模型输出的增大或者模型输出的减小，但如同<em>用体温预测死亡率</em>，对<em>猫狗图像分类</em>的这一系列问题，线性的假设是难以成立的。<br/><br>所以我们需要考虑特征之间的相关交互作用，即使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。可以通过在网络中加入一个或多个隐藏层来突破线性模型的限制。<br><img src="多层感知机.png" alt="多层感知机"><br>该多层感知机由4个输入、3个输出，隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。所以层数为2，且两个层是全连接的，参数开销很大。<br/><br>隐藏层的输出用$\mathbf{H} \in \mathbb{R}^{n \times h}$表示，称为<em>隐藏表示</em>（hidden representations）。也称<em>隐藏层变量</em>（hidden-layer variable）或<em>隐藏变量</em>（hidden variable）。<br>因为隐藏层和输出层都是全连接的，<br>所以我们有隐藏层权重$\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$<br>和隐藏层偏置$\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$<br>以及输出层权重$\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$<br>和输出层偏置$\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$。<br>形式上，我们按如下方式计算单隐藏层多层感知机的输出<br>$\mathbf{O} \in \mathbb{R}^{n \times q}$：</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\end{aligned}</script><p>但如果仅仅如此的话相较于单层模型是没有益处的，因为本质上仍是仿射函数：</p><script type="math/tex; mode=display">\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.</script><p>所以需要非线性的<em>激活函数</em>（activation function）$\sigma$。<br>激活函数的输出（例如，$\sigma(\cdot)$）被称为<em>活性值</em>（activations）。<br>一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\\end{aligned}</script><p>由于$\mathbf{X}$中的每一行对应于小批量中的一个样本，<br>出于记号习惯的考量，<br>我们定义非线性函数$\sigma$也以按行的方式作用于其输入，<br>即一次计算一个样本。<br>但激活函数往往<em>可以按元素操作</em>，意味着在计算每一层的线性部分之后，我们可以计算每个活性值，<br>而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="1-ReLU函数"><a href="#1-ReLU函数" class="headerlink" title="1. ReLU函数"></a>1. ReLU函数</h3><p>ReLU全名修正激活单元（rectified linear unit,ReLU）其实现简单，表现良好。它要么让参数消失，要么让参数通过，缓解了以往神经网络的梯度消失问题。可以采用参数化ReLU（Parameterized ReLU，<em>pReLU</em>）使得负的参数也能够通过。</p><script type="math/tex; mode=display">\operatorname{ReLU}(x) = \max(x, 0)</script><script type="math/tex; mode=display">\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)</script><p><img src="Relu.png" alt="Relu"></p><h3 id="2-sigmoid函数"><a href="#2-sigmoid函数" class="headerlink" title="2. sigmoid函数"></a>2. sigmoid函数</h3><p><strong>对于一个定义域在$\mathbb{R}$中的输入，<em>sigmoid函数</em>将输入变换为区间(0, 1)上的输出</strong></p><script type="math/tex; mode=display">\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}</script><p><img src="sigmoid-1.png" alt="sigmoid-1"><br>其导数是</p><script type="math/tex; mode=display">\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right)</script><p><img src="sigmoid-2.png" alt="sigmoid-2"></p><h3 id="3-tanh函数"><a href="#3-tanh函数" class="headerlink" title="3. tanh函数"></a>3. tanh函数</h3><p>该函数类似于sigmoid,将输入压缩转换到区间(-1,1)上。</p><script type="math/tex; mode=display">\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}</script><p><img src="tanh.png" alt="tanh"></p><h2 id="多层感知机从零开始实现"><a href="#多层感知机从零开始实现" class="headerlink" title="多层感知机从零开始实现"></a>多层感知机从零开始实现</h2><p>代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = nn.Parameter(torch.randn(</span><br><span class="line">    num_inputs, num_hiddens, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="literal">True</span>))</span><br><span class="line">W2 = nn.Parameter(torch.randn(</span><br><span class="line">    num_hiddens, num_outputs, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"><span class="comment">#激活函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X, a)</span><br><span class="line"><span class="comment">#模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(X@W1 + b1)  <span class="comment"># 这里“@”代表矩阵乘法</span></span><br><span class="line">    <span class="keyword">return</span> (H@W2 + b2)</span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">num_epochs, lr = <span class="number">10</span>, <span class="number">0.1</span></span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></table></figure></p><h2 id="多层感知机的简洁实现"><a href="#多层感知机的简洁实现" class="headerlink" title="多层感知机的简洁实现"></a>多层感知机的简洁实现</h2><p>模型实现如下，与softmax可以进行对比<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line">net.apply(init_weights)<span class="comment">#;</span></span><br></pre></td></tr></table></figure><br>对比softmax代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></p><h1 id="模型选择、欠拟合和过拟合"><a href="#模型选择、欠拟合和过拟合" class="headerlink" title="模型选择、欠拟合和过拟合"></a>模型选择、欠拟合和过拟合</h1><p>机器学习的目标是发现某些模式（pattern），这些模式捕获到训练集潜在的总体规律。如何发现可以泛化的模式是机器学习的根本问题。<br/><br>将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。<br/><br><br/><br><em>训练误差</em>（training error）是指，<br>模型在训练数据集上计算得到的误差。<br><em>泛化误差</em>（generalization error）是指，<br>模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。但我们永远不能准确地计算出泛化误差。<br/></p><h2 id="1-统计学习理论"><a href="#1-统计学习理论" class="headerlink" title="1.统计学习理论"></a>1.统计学习理论</h2><p>训练数据和测试数据往往假设是从相同的分布中独立提取的。这通常被称为<em>独立同分布假设</em>，意味着对数据进行抽样的过程没有进行“记忆”。但是实际上这样的假设难以成立，如不同地区，不同时间的抽样往往会影响样本，从而违背独立同分布假设。</p><h2 id="2-模型复杂性"><a href="#2-模型复杂性" class="headerlink" title="2.模型复杂性"></a>2.模型复杂性</h2><p>统计学家认为，能够轻松解释任意事实的模型是复杂的，<br>而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。</p><p>事实上，具有更多参数的模型可能被认为更复杂，<br>参数有更大取值范围的模型可能更为复杂。<br>通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂，<br>而需要<em>早停</em>（early stopping）的模型（即较少训练迭代周期）就不那么复杂。</p><h2 id="3-验证集"><a href="#3-验证集" class="headerlink" title="3.验证集"></a>3.验证集</h2><p>在确定超参数之前，不希望用到测试集，即不能依靠测试数据来进行模型选择。<br>但也不能仅仅依靠训练数据来选择模型，因为这样便无法估计训练数据的泛化误差。</p><p>常见做法是将数据分成三份，除了训练和测试数据集之外，还需要增加<em>验证数据集</em>（validation dataset），也称<em>验证集</em>（validation set），虽然实际上验证集和测试集的边界十分模糊。</p><p>在训练数据稀缺时，存在一种解决方案是是采用$K$<em>折交叉验证</em>。<br>这里，原始训练数据被分成$K$个不重叠的子集。<br>然后执行$K$次模型训练和验证，每次在$K-1$个子集上进行训练，<br>并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。<br>最后，通过对$K$次实验的结果取平均来估计训练和验证误差。</p><h1 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h1><p>下面用多项式的例子来说明过拟合和模型复杂性的经典直觉。<br>给定由单个特征$x$和对应实数标签$y$组成的训练数据，<br>我们试图找到下面的$d$阶多项式来估计标签$y$。</p><script type="math/tex; mode=display">\hat{y}= \sum_{i=0}^d x^i w_i</script><p>这是一个简单的线性回归问题，而高阶多项式函数的复杂性则会比低阶多项式函数复杂得多，其<strong>参数较多</strong>，<strong>模型函数的选择范围较广</strong>。<br>因此在固定训练数据集的情况下，<br>高阶多项式函数相对于低阶多项式的训练误差应该始终更低。<br>事实上，当数据样本包含了$x$的不同值时， 函数阶数等于数据样本数量的多项式函数可以完美拟合训练集。<br><img src="模型复杂度对拟合程度的影响.png" alt="模型复杂度对拟合程度的影响"></p><p>同时数据集的大小会影响拟合程度。训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。给出更多的数据，我们可能会尝试拟合一个更复杂的模型。</p><p>对应的代码如下：<br>第一步生成测试数据，采用以下三阶多项式来生成训练和测试数据的标签</p><script type="math/tex; mode=display">y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{ where }\epsilon \sim \mathcal{N}(0, 0.1^2).</script><p>在优化的过程中，我们通常希望避免非常大的梯度值或损失值。<br>这就是我们将特征从$x^i$调整为$\frac{x^i}{i!}$的原因，<br>这样可以避免很大的$i$带来的特别大的指数值。<br>我们将为训练集和测试集各生成100个样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">max_degree = <span class="number">20</span>  <span class="comment"># 多项式的最大阶数</span></span><br><span class="line">n_train, n_test = <span class="number">100</span>, <span class="number">100</span>  <span class="comment"># 训练和测试数据集大小</span></span><br><span class="line">true_w = np.zeros(max_degree)  <span class="comment"># 分配大量的空间</span></span><br><span class="line">true_w[<span class="number">0</span>:<span class="number">4</span>] = np.array([<span class="number">5</span>, <span class="number">1.2</span>, -<span class="number">3.4</span>, <span class="number">5.6</span>])</span><br><span class="line"></span><br><span class="line">features = np.random.normal(size=(n_train + n_test, <span class="number">1</span>))<span class="comment">#偏置1</span></span><br><span class="line">np.random.shuffle(features)</span><br><span class="line">poly_features = np.power(features, np.arange(max_degree).reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_degree):</span><br><span class="line">    poly_features[:, i] /= math.gamma(i + <span class="number">1</span>)  <span class="comment"># gamma(n)=(n-1)!</span></span><br><span class="line"><span class="comment"># labels的维度:(n_train+n_test,)</span></span><br><span class="line">labels = np.dot(poly_features, true_w)</span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>, size=labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy ndarray转换为tensor</span></span><br><span class="line">true_w, features, poly_features, labels = [torch.tensor(x, dtype=</span><br><span class="line">    torch.float32) <span class="keyword">for</span> x <span class="keyword">in</span> [true_w, features, poly_features, labels]]</span><br></pre></td></tr></table></figure><p>下面对模型进行训练和测试<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#评估损失</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 损失的总和,样本数量</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#定义训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features, test_features, train_labels, test_labels,</span></span><br><span class="line"><span class="params">          num_epochs=<span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 不设置偏置，因为我们已经在多项式中实现了它</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, <span class="number">1</span>, bias=<span class="literal">False</span>))</span><br><span class="line">    batch_size = <span class="built_in">min</span>(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                                batch_size)</span><br><span class="line">    test_iter = d2l.load_array((test_features, test_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                               batch_size, is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">1e-3</span>, <span class="number">1e2</span>],</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>, net[<span class="number">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure><br>最后是拟合情况：</p><ol><li>三阶多项式函数拟合<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">4</span>], poly_features[n_train:, :<span class="number">4</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure><img src="三阶多项式.png" alt="三阶多项式"></li><li>线性函数拟合（欠拟合）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前2个维度，即1和x</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure><img src="线性函数.png" alt="线性函数"></li><li>高阶多项式函数拟合（过拟合）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选取所有维度</span></span><br><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br></pre></td></tr></table></figure><img src="高阶多项式函数.png" alt="高阶多项式函数"></li></ol><h1 id="权重衰减（-L-2-正则化）"><a href="#权重衰减（-L-2-正则化）" class="headerlink" title="权重衰减（$L_2$正则化）"></a>权重衰减（$L_2$正则化）</h1><p>假设我们已经拥有了尽可能多的高质量数据，就可以将重点放在正则化技术上来防止过拟合。<br>限制特征的数量是环节过拟合的一种常用技术，简单地丢弃特征对这项工作而言太过于生硬，需要更为细粒度的工具来调整复杂度。</p><p>一种简单的方法是通过线性函数<br>$f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$<br>中的权重向量的某个范数来度量其复杂性，<br>例如$| \mathbf{w} |^2$。<br>要保证权重向量比较小，<br>最常用方法是将其范数作为惩罚项加到最小化损失的问题中。<br>将原来的训练目标<em>最小化训练标签上的预测损失</em>，<br>调整为<em>最小化预测损失和惩罚项之和</em>。<br>现在，如果我们的权重向量增长的太大，<br>我们的学习算法可能会更集中于最小化权重范数$| \mathbf{w} |^2$。</p><p>举之前的线性函数的例子，损失如下：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2</script><p>$\mathbf{x}^{(i)}$是样本$i$的特征，<br>$y^{(i)}$是样本$i$的标签，<br>$(\mathbf{w}, b)$是权重和偏置参数。<br>通过<em>正则化常数</em>$\lambda$来平衡$| \mathbf{w} |^2$描述的损失，使用验证数据拟合：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2,</script><p>使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。<br>这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。<br>在实践中，这可能使它们对单个变量中的观测误差更为稳定。<br>相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上，<br>而将其他权重清除为零。<br>这称为<em>特征选择</em>（feature selection），这可能是其他场景下需要的。</p><p>$L_2$正则化回归的小批量随机梯度下降更新如下式，一般网络输出层的偏置项不会被正则化：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).\end{aligned}</script><p>接下来用一个简单例子来展现权重衰减：</p><p>首先是生成数据,问题的位数增加到$d = 200$，<br>并使用一个只包含20个样本的小训练集。</p><script type="math/tex; mode=display">y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where }\epsilon \sim \mathcal{N}(0, 0.01^2)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_train, n_test, num_inputs, batch_size = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">5</span></span><br><span class="line">true_w, true_b = torch.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line">train_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class="line">train_iter = d2l.load_array(train_data, batch_size)</span><br><span class="line">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class="line">test_iter = d2l.load_array(test_data, batch_size, is_train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>初始化模型参数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>():</span><br><span class="line">    w = torch.normal(<span class="number">0</span>, <span class="number">1</span>, size=(num_inputs, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure><br>定义$L_2$范数乘法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">l2_penalty</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(w.<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">2</span></span><br></pre></td></tr></table></figure><br>训练代码如下，其将模型拟合训练数据集，并在测试数据集上进行评估：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">lambd</span>):</span><br><span class="line">    w, b = init_params()</span><br><span class="line">    net, loss = <span class="keyword">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># 增加了L2范数惩罚项，</span></span><br><span class="line">            <span class="comment"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span></span><br><span class="line">            l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item())</span><br></pre></td></tr></table></figure><br>使用权重衰减进行拟合后效果更好。</p><p>可以在实例化优化器时直接通过weight_decay指定weight decay超参数。<br>默认情况下，PyTorch同时衰减权重和偏移。<br>这里我们只为权重设置了weight_decay，所以偏置参数不会衰减。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    <span class="comment"># 偏置参数没有衰减</span></span><br><span class="line">    trainer = torch.optim.SGD([</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].weight,<span class="string">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].bias&#125;], lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>,</span><br><span class="line">                         (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                          d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数：&#x27;</span>, net[<span class="number">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure></p><h1 id="暂退法（Dropout）"><a href="#暂退法（Dropout）" class="headerlink" title="暂退法（Dropout）"></a>暂退法（Dropout）</h1><p>面对更多特征而样本不足时，线性模型往往会过拟合。<br>给出更多样本而不是特征时，线性模型不会过拟合。<br>但线性模型没有考虑到特征之间的相互作用。<br>对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p><p>与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。<br>例如，神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件，但单独出现则不表示垃圾邮件。<br>而且即使我们有比特征多得多的样本，深度神经网络也有可能过拟合。</p><p>经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。<br>简单性以<strong>较小维度</strong>的形式展现，在权重衰减的讨论中可以看到参数的范数也代表了一种有用的简单性度量<br>简单性的另一个角度是<strong>平滑性</strong>，即函数不应该对其输入的微小变化敏感。基于此，学者提出了一个想法：在前向传播过程中，计算每一内部层的同时注入噪声，这就是我们要提到的<em>暂退法</em>（dropout）。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。</p><p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。<br>换言之，每个中间活性值$h$以<em>暂退概率</em>$p$由随机变量$h’$替换，如下所示：</p><script type="math/tex; mode=display">\begin{aligned}h' =\begin{cases}    0 & \text{ 概率为 } p \\    \frac{h}{1-p} & \text{ 其他情况}\end{cases}\end{aligned}</script><p>根据此模型的设计，其期望值保持不变，即$E[h’] = h$。对之前的多层感知机运用效果如下图：<br><img src="暂退法.png" alt="暂退法"></p><p>代码流程如下：</p><p>实现dropout_layer函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X, dropout</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= dropout &lt;= <span class="number">1</span></span><br><span class="line">    <span class="comment"># 在本情况中，所有元素都被丢弃</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="comment"># 在本情况中，所有元素都被保留</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    mask = (torch.rand(X.shape) &gt; dropout).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> mask * X / (<span class="number">1.0</span> - dropout)</span><br></pre></td></tr></table></figure><br>定义模型参数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br></pre></td></tr></table></figure><br>定义模型，可以为每一层分别设置暂退概率，常见的技巧是在靠近输入层的地方设置较低的暂退概率，暂退法只在训练期间有效：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">dropout1, dropout2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span></span><br><span class="line"><span class="params">                 is_training = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.training = is_training</span><br><span class="line">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class="line">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class="line">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        H1 = self.relu(self.lin1(X.reshape((-<span class="number">1</span>, self.num_inputs))))</span><br><span class="line">        <span class="comment"># 只有在训练模型时才使用dropout</span></span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">            H1 = dropout_layer(H1, dropout1)</span><br><span class="line">        H2 = self.relu(self.lin2(H1))</span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">            H2 = dropout_layer(H2, dropout2)</span><br><span class="line">        out = self.lin3(H2)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></table></figure><br>训练与测试类似前文。而简洁实现中，只需在每个全连接层之后添加一个Dropout层， 将暂退概率作为唯一的参数传递给它的构造函数。在训练时，Dropout层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。在测试时，Dropout层仅传递数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment">#net.apply(init_weights);</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure></p><h1 id="前向传播、反向传播和计算图"><a href="#前向传播、反向传播和计算图" class="headerlink" title="前向传播、反向传播和计算图"></a>前向传播、反向传播和计算图</h1><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。</p><script type="math/tex; mode=display">\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x}</script><p>其中$\mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$<br>是隐藏层的权重参数。<br>将中间变量$\mathbf{z}\in \mathbb{R}^h$通过激活函数$\phi$后，<br>我们得到长度为$h$的隐藏激活向量：</p><script type="math/tex; mode=display">\mathbf{h}= \phi (\mathbf{z})</script><p>隐藏变量$\mathbf{h}$也是一个中间变量。<br>假设输出层的参数只有权重$\mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$，<br>我们可以得到输出层变量，它是一个长度为$q$的向量：</p><script type="math/tex; mode=display">\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}.</script><p>根据$L_2$正则化的定义，给定超参数$\lambda$，正则化项为：</p><script type="math/tex; mode=display">s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2\right)</script><p>其中矩阵的Frobenius范数是将矩阵展平为向量后应用的$L_2$范数。<br>最后，模型在给定数据样本上的正则化损失为：</p><script type="math/tex; mode=display">J = L + s.</script><p>我们将$J$称为<em>目标函数</em>（objective function）。前向传播的计算图如下：<br> 其中正方形表示变量，圆圈表示操作符。<br> 左下角表示输入，右上角表示输出。<br> 注意显示数据流的箭头方向主要是向右和向上的。<br><img src="前向传播计算图.png" alt="前向传播计算图"></p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="　反向传播"></a>　反向传播</h2><p><em>反向传播</em>（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。<br>简言之，该方法根据微积分中的<em>链式规则</em>，按相反的顺序从输出层到输入层遍历网络。<br>该算法存储了计算某些参数梯度时所需的<strong>任何中间变量</strong>（偏导数）。</p><p>假设我们有函数$\mathsf{Y}=f(\mathsf{X})$和$\mathsf{Z}=g(\mathsf{Y})$，<br>其中输入和输出$\mathsf{X}, \mathsf{Y}, \mathsf{Z}$是任意形状的张量。<br>利用链式法则，我们可以计算$\mathsf{Z}$关于$\mathsf{X}$的导数</p><script type="math/tex; mode=display">\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right)</script><p>在这里，我们使用$\text{prod}$运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。<br>对于向量，这很简单，它只是矩阵-矩阵乘法。<br>对于高维张量，我们使用适当的对应项。<br>运算符$\text{prod}$指代了所有的这些符号。<br>回想一下，在上文计算图中的单隐藏层简单网络的参数是<br>$\mathbf{W}^{(1)}$和$\mathbf{W}^{(2)}$。<br>反向传播的目的是计算梯度$\partial J/\partial \mathbf{W}^{(1)}$和<br>$\partial J/\partial \mathbf{W}^{(2)}$。<br>为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。<br>计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。具体流程查看<a href="https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html">反向传播</a></p><h2 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h2><p>在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。</p><p>以上述简单网络为例：一方面，在前向传播期间计算正则项取决于模型参数$\mathbf{W}^{(1)}$和$\mathbf{W}^{(2)}$的当前值。<br>它们是由优化算法根据最近迭代的反向传播给出的。<br>另一方面，反向传播期间参数 :eqref:<code>eq_backprop-J-h</code>的梯度计算，<br>取决于由前向传播给出的隐藏变量$\mathbf{h}$的当前值。</p><p>因此，在训练神经网络时，在初始化模型参数后，<br>我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。<br>注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。<br>带来的影响之一是我们需要保留中间值，直到反向传播完成。<br>这也是训练比单纯的预测需要更多的内存（显存）的原因之一。<br>此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。<br>因此，使用更大的批量来训练更深层次的网络更容易导致<em>内存不足</em>（out of memory）错误。</p><h1 id="数值稳定性和模型初始化"><a href="#数值稳定性和模型初始化" class="headerlink" title="数值稳定性和模型初始化"></a>数值稳定性和模型初始化</h1><p>若选取的初始参数不当，容易威胁到优化算法的稳定性：要么是要么是梯度爆炸（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛； 要么是梯度消失（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p><p>sigmoid函数容易发生梯度消失的问题，可采用ReLU函数</p><p>神经网络中存在参数化固有对成型的问题，可以采用暂退法正则化打破这样的对称性</p><p>参数初始化可以考虑Xavier初始化，通常，Xavier初始化从均值为零，方差<br>$\sigma^2 = \frac{2}{n_\mathrm{in} + n_\mathrm{out}}$<br>的高斯分布中采样权重。<br>我们也可以将其改为选择从均匀分布中抽取权重时的方差。<br>注意均匀分布$U(-a, a)$的方差为$\frac{a^2}{3}$。<br>将$\frac{a^2}{3}$代入到$\sigma^2$的条件中，将得到初始化值域：</p><script type="math/tex; mode=display">U\left(-\sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}, \sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}\right)</script><p>虽然有“不存在非线性”的假设，但实践证明是有效的。</p><h1 id="环境与分布偏移"><a href="#环境与分布偏移" class="headerlink" title="环境与分布偏移"></a>环境与分布偏移</h1><p>有时，根据测试集的精度衡量，模型表现得非常出色。<br>但是当数据分布突然改变时，模型在部署中会出现灾难性的失败。<br>更隐蔽的是，有时模型的部署本身就是扰乱数据分布的催化剂。<br>机器学习的许多应用中都存在类似的问题：<br>通过将基于模型的决策引入环境，我们可能会破坏模型。</p><h2 id="偏移类型"><a href="#偏移类型" class="headerlink" title="偏移类型"></a>偏移类型</h2><p>偏移的类型有如下几种：</p><ol><li>协变量偏移：虽然输入的分布可能随时间而改变，<br>但标签函数（即条件分布$P(y \mid \mathbf{x})$）没有改变，<br>在我们认为$\mathbf{x}$导致$y$的情况下，协变量偏移是一种自然假设。</li><li>标签偏移：假设标签边缘概率$P(y)$可以改变，<br>但是类别条件分布$P(\mathbf{x} \mid y)$在不同的领域之间保持不变。<br>当我们认为$y$导致$\mathbf{x}$时，标签偏移是一个合理的假设。</li><li>概念偏移：类别会随着不同时间的用法而发生变化。<br>精神疾病的诊断标准、所谓的时髦、以及工作头衔等等，都是概念偏移的日常映射。</li></ol><h2 id="分布偏移纠正"><a href="#分布偏移纠正" class="headerlink" title="分布偏移纠正"></a>分布偏移纠正</h2><p>_ramain to be understood_</p>]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI学习-论文阅读1</title>
      <link href="/2023/12/12/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB1/"/>
      <url>/2023/12/12/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>读书日记</title>
      <link href="/2023/12/04/%E8%AF%BB%E4%B9%A6%E6%97%A5%E8%AE%B0/"/>
      <url>/2023/12/04/%E8%AF%BB%E4%B9%A6%E6%97%A5%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="7a75ece90184b2ca11b9e167b254716d988dcd0c0ed354964b6c19b3a974938d">a46f637dae47b598d230519d9010266293819c88a8abe3cbfbfcc78e1dc08b39fbd451be6a698c4c22f60ba6ca04605ee97ec97cb207d4eafaac0a1ced7fa8ba8ae30b821c2be040dc50efee950b02b2c9c534dbaa013f34ce7392535b869a65dd4d7fc9fd57deb1f2ea9e4967d95b11b940d9a13bbb09dea367ac3e060539ba000020db989fca9b029181f2639c063bee63910c476ed6bb9cf986e9f988a55dff650c655287f929b52c7418d8fa43fc381880e3218da4f010171609bf349541a84fa4f6e864fa74ef96e2eca62fa30c05d0cc0396a40fd90095178c5633cf5d219826f1cfac13d9ff7f676bedc439a134a4f9ba637bda744361ce887a4e4912e3cd3866baef2d6a9f5b3b1c609ed6a9c6fec90823f3fd9b919074c196a1aaae27d0b53458fba0a4bc663e967ddb0100a3c8ed828cd7226478fcfa953dc451fb49b1131682ce539749f474af41fda643300c8b9e539de6b95e8cb3b00c4c196eb47958cdc51b1390770443918cac2950d234c5f9453ec018e256ab30f30f1e446c4de36546e65e855f7d3c5681e00db4e189d1cf70c0f52c0b5751b6214264ab5f67302c1fcbf14af7149c4e6a6ffaa4d4eca5ee234bba8caabdd14a01ef1ee090ee6d25bfce1b05e119ba9533417ec14db7f21a88f162a86c1ee176a8b9f3b2a8857979f074c5a282a49c1d1f3124a6bf894fc602f15e6685bcae8ab5a2aaba749a280bd0a34128e0bf3a68db982052f8bc61d9c6a325113d83edac09cfd8dd9ba33ff9584b885ba69cc10b63e929f29966ba1b3dbccb927afbba3a8f94062260fc24bfebb305b24dcb1849999fa90236799b576c1b3578736f6fa931079d20b72206190831eaae1abdae53899163f0beb481cfe1601ca5e55060b9b17d88def67a1686756999670ad75996764dd85596645cdb483ffc7765a37c50dc43af5abd22090ec194dceda2a457bb2d41640639fe1d48fe86cb7c0103d622687ed1b3fb8373b1bc8e62f7ea392887308b7fc53a30244c84bb364fcc67344d0076bca38e8ff7722a251d4d11babb72da821e0e3f81f84f405865ab6455bba66fbf4d4e616a110d1e9af6e8b4ac16b09cada785a749a4752b460d3389a9c6b136d502c7d195a889bd325a3ecaf4805c631f34521e272a58ad1d8dddb59e9667a75b7796bb486fd48b87dc5365019994bb22ee0242174a5d5024b6e7bc169b6aabd7e46e7825e0d4ac1ff731e8eeb1606b4b114b4a493fc47ad5bdb0646c8c43a82838ffb184f60d2551426c7cf907dca1101f59707db237164eb26414a9e239c4aa8258a488d3c9baaae074de8e28e35abdb20576d4e9a9e88f05b10de095cdf7720a5334035ad2e4c9de4be1880cb7228c34a73cfadb567f3875594fd9236a1be7e3abc172a88dde522d061074570b15c26d6823c179571b4a414cd09ac602a78b3c61fa033451440f05974725a5e2cef9d9c1028c0e530a67af4e3c67eee4a653b6fdcf9c192c5f7099ca154ede93614ca3100c634efee84e4e59c0e1a7298d2a79a010ed88cc4a36071134062724b04c35230574bea349858ac4ee237e1cf9cb09b70d8c70606412c6b911f91c4a3cd0ee1198bf3e03ae84eea2c89aebaf15f0507e8891ee5204d57447147e239607a20fbfdddd9110352e5fd9597e43b6086255654e1a71750384106a10f30dd32e6c10bf50210bc6954e6f73047a64bae49655e56d7542b090f38b54c51cff17306c2ad7c982fb12c38d99928d190d201445153907ec2e71bf2a768d15aaba8498998a938ba8aa30bb36088e7e26a02138eaa07efc9d4737dc25f135f37af0890badebcaa6f74519a5d96673870c90f029dbf8daa07b77a66c9720a3a785fb12a279634c30e2700f4c01e6a521fb97f285484c23c6e41db4192834e4d0c0c532f7b855ce77dd1d4f3c962e2b5ebe20171acbf49e362ee0544f1b3717de1f9ef673b932fd8d2202728edf1bce1d4e6816fd514304ea100e27987d044bb9bd66d3db1fd3327f60c511d335d20dc51f826065955e9120a0ea4e54f0e9b45f0d0333c080a74b4384941c8c8eddc5ee56fedad86d03a17e83e5affbb71b0d81fcb1339b961b81c2b32b9298b197f1e73eba1c1301df81b5e338c590aa7d9465e59f8ae2d3b8456dc058ebbbb6b8f5c619790e18e32053c0808958a702f140fff55dd1f99e1977644c17ba69d65f8d7c22011c66361245c8e48c745be290fb947e7f323cded82c5d0fd1421f01fabf7b490f6868c4b3f19b92ea5a8a1205f7bf2afc85d84a15d86054360375c454f2b55b5aeda4dddad25676d9a462e5a860282ba5dfbac870f2e45da266ed43fc81dfddaad026bd711f3ccd6db19b154a84eb6a437f3cbc60140c29db1b10efabc092dc14451c3d2b91b4e4cf0db6fa7fb2a837839fb9242899268a65ac143d0d16c1002a99d22717ebe9f6e43a0b53a3c69be460fac692cc39f806d646b1e9e5e77886666610ab6620e402cd080cba98f9e567ae36bccb58b3f9d215c217b07029ff9ea87c9a8fecba7869c0c9dc32fd8faf8f3d8f705d1f71c637e8996ca2a4971c66af180f5bb135d7381844a04c2c5cf89b3536053ddd45089666f7d4f56e9fdc3ceee25802e2a28f7fc45a307281da8d4998621b0f65a8406eba90f1e834e7585fec88955e3da54131b390db0d6c0b9132eae41f960956fac5ba9393446dd18b002cc914d8a67d8c9015ce385a93b08b93c8af48c883f19b2816ae2e09ba48de2e166da712efcff6e650f7cac0fca93211c127088901dc108af14ed0fb8aff5273cf01b139039e2850e52a42e342ea9164c01ae83785a426e0763e9c1b4671856a1ef751ed63639e8a87f872a47799150115ec602df359da21f02192ef3f985282ffd972e428e3b52c1c3e7459e4ff50c83122ef46184198212d0f837ad98719d6ffe425a8969275f703157713cada9f8139939cdbadf063c048f10d2842244f7671c77a6fa53576b1517f3dd07940e4753053333118ca4502904a68e9e707ea4630679df5f063cb4f40b2289f16070ec077bf9a6635c0d774a0ab2599de016794226484f529ddb1abd6cba8dafc140ea7ff9f640c36069220987c7108435dc44ecd23fcabf090887462aff7a8e01e6e06d9ccccc25ac383ac61e457cee18d2c40977cd9ac543b777953c9cc1e449eff60e1242cb2b4e929a7f777eaa202883289af692ad1cb92f5ad25b59e9340e15d7740c2279e79096e122dffc2d737cb073a30186d39682eb6d4b2bac72d967c81402458c3c9d4a3c6ee3fd60bb73bebaf838f4970dc1a8d70624420e4a03c4570ce63afb8fb1ca2ce90a5669b06d8892faf846958c6ff605ad09c973dccf31c3d081292ed485919f48c0d0ce45e9ab54641a35b82d4bbb4d804148089bafb51df7c5be32e7a1e8918c1a25d22176842db1dd788d34b21bb9d4b7a27945e5476608ba8f8d3e455213056b5f689443f011637a1a2a4dbcf1191ee7f323a94078ca44d3abff1ec6f1eeada622ae8934660b070f5643a052833cc6637cb4637351ecd3f8be6f4fdb4d9a022a7ec7eaed1707b557820e248663655051e09e645b31c31f12f4ddc3e84d462f65e74e88431c25450a7d1ab416daf9a22a676668535fecc3c07c31889d89de42a8fa85e84a08a4a71502dda5b7219fdedd70f99a75b9816412c44735204633241bbcfc7acea4e3b0e19e561c8b8aa28a21a56b3df6cd3dd88de39f1e6525d325c5659c9b8cadedb8ec0f7367d38202be6f041d15097acd2895c264d1aea7762b718c376f134d36087001b9980654f9119fcfd0b1886f23492c32616b53778aa01f31c8fa4fe81f853a983246f0ee78bc5b6a5b9145441e0f2b5b75a20bf2ed7c0d0fc8a66cd8709b67872b52d1121d34bd88c0be3985157e648a8f891c77033f976cf910016b1ef34fe1c14c495dc187a8b255cac33247fe0588392d6bf9cbb869039e156c455a3b64438bf5325b12448d303ae569dc33b77d92ca5e1f692e5f953d154aa5ac83273ae705bd9de3bc8e1bf46303cb116e0a09038d002364167422ce14573cccc445292ed185513ef0db1f3856648ef90e447cbb903d4ada55e51cd1a8b4a5b1dbb7fed5964ab960ed2262ddabe921f699f01106bf399dfdcb91bb8dd03e44d95d255645a5b2c64741b097c49c1dd9751b8011ec23af6fc64e337559efc8876061fe44a2d1f58a517088a1fee3669019085ab4ee8c34c0d655abd0c3319c3daf0b7dd632b25fe76f0c7115fd4885ff00f6e50f9ca04b7a1bffe06da212ec93bd06862ee28c13e920889af8eb19f8d28d5024d2e02872cf1eebba05c476488cf2e55442c0a5131054aa78cf5eeaf9172640296ca21bea16e6dcfad732d32be0b9481eb0dd361d81162aa06b24efe9c9fef9d7c8810cf4a08cbe4654134cb83d66246a132dab87aa9a526ddb945c206f772d723e5d4d25c934dd8bdf289d9ec496e355a5047d79c27e658faa24ef679a777398ba8b50b02464add22e1542aa77134d142914ea5ced1667af992e579fffcc3318c2ced7ee8e6e2e300fba6ec580f4dc764064a02c10f5a7616b4691afab761b3b253a772f29697b0a01a40c612387c1fb578bf29353cad70bf4a78a1fe6a3be8c3df014747a87b96c11d9409da10e066c4227e55e2f3027ef31fd2b2caed8519424ce04d51dcaf8f962670b6c4413211d945e82d954c63267b230fe61f7ec1b3204cf6245046b4edf1750509da5129c0209a8033b8f19a84943dd5b88944b1918149552f0cf3afad7c715e185030cbf69871fb0c0de3f16845003f32ae78a70cdf8f2b1b70de67f8d85c14454ff98cb3ea1fc259989881190b3a7ff4de5e94adde8dc7c477cff6f5f1d1a44e0af2f65474d197006a0801c3e555a016f0e9a4bcbba8bd873e3feda28a4a066db3326ddf92abb4e9acae066c5ea88bfae387fe6db0e3f79466316a2545ed501b1375183746d8210166ab2c56d8344a0a3f73347b670ecffd1521369dba879c54caad76ef11007d9f368aa1a2e5219e976549274d446415d3e589ed6b658fb9aea890c94c93329c430a55308e01b5639e4881511b8434de12bd3d73e695ed876e7f10868e22f098cd0a1db8b89dead51b1777b1d49f61fc9c2c8a34ec2c0f72abd459b5f8124d16a57a27446e5931f121b255011b976c401e24bb05879f54f0ee62db12c3cdd9bfe93715bb7b748604d50cee713398304f62672e6c67c036d5d0e99fa93e796be6a4856977b83350c701882eb0cc27ca6cddc384704fc9d082a552be5d20e236d4f20b70cbf40704240a7c12f8e04d713394d0d278af0a8e097f93cd618c89d678132a82aa8c2164de29497dd07c0ccea1ed5fe2240843d0013959593e0809b47497e34e392b994fb600ff4e984b8504b935428c774aecfe9d81f59dd10a4eb77bddd786e8a5b64a15b3021d4d7978fa2e299b9984c2010adedb4648e075e2b9de376ac34d73493a24fc530f8d5d86e24cd40aae517cd008997eadfd92021541c0ddbaa618065f889e5463963c054db4e3e59896358ca265d7962274d33ca4be4d29a32c6b8d732bc7460c88267cb34c81f2f96fadfc6072bcd54443627ee8c1cffe71ab7546b42aefd4be6c3e3152eecd9fe652b274035ff8b65b49239f5fa488cc2a6e1c380958b284bc1f854de8d1b9d8ec16ad53a7f47c762c2f6aa0034a365535a43fdd3d9aeb3838d88c39d5b997dcf92018716ee3cd659f8c69ce0e940c0d767ab14b1b281190d53fd8d5900a8ba1a8320959af4679cd75c5480a610eea4e3b60cc9ae5a7d7152aa480e87bb061e58c406c682f71c086f1d3b3f937e89b7e029934cbd2a3f7f59132daaaecc89ab40f03f769ee25928062fcd565832c5a592e02098b56bd19b11b455d567f7128dbc7f3b690fd7266415c5c0f8b42493bb2da7954503eea24a8c4adda1676187cf7c91a471fafbf11325d1026ace241389ebb8a3e61f81ea4ac1eb48c93705c96840230cd783638c0b6aff8c8d651de68de79b01bb32f8d2a51f07f8bc937f928894f34bb5e6000e5d197dce76a93c29dc3b7ace0ccdb42a5882cecfcb0dc06f872878aa6bccb2fa7dd12543baf311d02f9c02664a9b034e30c98569fd46753e9f14f81086b4b564ce742b9cfe0d2f5e647001ff43382468c014777e020e3bdf29b12b2a5a38dc4b37914bbded893ec61118bf79268d55145fc635eb4b31b8f6a1720ff0ed7088432f920c75b78b24d11c3174f4a8b3e4422966ad1452118808e0381ab1e68d69e69689f378237bdc3b9005400da565361d262ef1845469d353fc2dccb4326b5680c5801a6aba50c054d58e8ab31797e84c6c1d367fcd6eb09540c9ec8dcb124a168cc1b485b59c6151fd22feb04937cb6321dfc686d36a0dd94cb342a8d4720689a839ede416372f2588ab6ae0517692543c6975cab939c49fbe3e151f2abf0e17763fee6b88d3a9d06e4eb9d841bed0a727157853a3c3def76f37cd9f862e992888e80764cc66a1a7217feba4fdfadf16818cdb310f24b0123f74c8f17bfc54da82390f7c6a8653ca1713c5bff5f59775f6e15266bba38040b430ca3c053388b29a65ee4175eab734444263f238a563d5b89f0b01e0a6e12c63e404fcf696cbd3e27a01be04dc6e403c4dd6cf3e0af9713930e24d0b06fb1da43f97bd635cc748ba22f2ce26e8a7cd757084276edb65e6aaa1d6bf816fe319b00a3f6f8f133e28d12fcfc1f07278df81c1159a209b95c159fb4ac13d76a359ac6acfb8d5c487e0a75a824198fc07dbe6704f608e3f8740f8dc6950bb493a0fea5646ee802330db0eaff883ede26f658d354038c2f76598a77ca14c77f8b19cb7db2b221a258c7f6936b8fb30b111238dc54fd41bd56bf6cdef7baf5ebafbce7a5076a8ec5001bda551b2d076f3d83420199c3e0b51a49fe0119a1bddfa9cb21e0379fd27d8aed52b0422f5ecef228a601b549531411410a112f4a792ca2f126e15678f0a3ac1926db869a0375a855afafd0217083643abb613afb1f69ba9b9c6191344711a6c0601bfa12a4324fc89d1e5913a0478c6ff3b59a406219ee4db127615fcca8dd5604c23c6425594d5ef14d83bde34190174fb6cd0626e18ac539d928fb800acc29fbba7f122055baa020fce5f8fad2d63173187eb24812929a8de302594c60b35f2bf133724c0c64b3d3916b56f44251907c50fed335d9667a7946c66c7183754d27d83a7818cc2430867d974076d21f83ba8e5265edec2d7a83eb3f9a2c2a754cd5bef8ede238414e71714164392c0035ae60e269a70e3c0c2c426254d8e48a0358d9a5f8f69fdc667eb609cc0fa2d0d198461d8ff58f41b62c436198a066a7e11c6f6216e826a74003ad939eaa28fe3b7194cbd12a74ffbae34df6d71c29ca8b519599b9c4e691c2a34e029fc5715563b6928921905821df8d0526768e207b46bee82c2ca8383a240398147e67e4ce5fb9c7152eea125dc972d0fd9e12b8aba826cd85ae7f92f06cab8a1aaa0bcc08feffbdfa328bf3acb091851dc183b9cc585c7ec20fe01532e03f1a836d03f45c96db660e3abc5a94bbfa83b46d3797858085c01fcd97b0d04c0f6759c2df878529b88f3b552ef2ee9d37ba3a057b3b284ae847241034e29336e33ee91c54f4d4f74b8886eee99ea4c29bb2b3681e4dd021332e16f73fa657f7f1258c54ea8a4f4fe7f46c46b4ebe63563521b76c12927ce3d992f86701d8b2fd588f4dbc43f046c822faa0415cea19d45dea49039e963d56e0e10ad77d0b201b4fc3f339068582b12b79a6e64cb1495e0a5fd049bca4a74244e51693aaf85691d4cb516363ea0c9073d315ac6d13c1d6fa17d5d5f7491846131b356ba3cac6b24b113af4d2f5c607baa6279441081131d32524df6f27594062e7bec654fc9c38053a6271a4b361db66bf3f6126597e696a8bdbc9bb972b1ecad154d3cb950cd116460facfeb81c688a7738f946baaadea75ca6677b603205ca6a263e4db8137f82a4a584f624fc11927f6aeaabfd3916d96012c2c3184ddfc97f9a69efe81f17c1b265253cb947e8037dcc9975c55e03badb88e64998175eddc76811035aa2d56e120b92b5637368427a4ff465cfdae6351a60feb4d44e224940d53fd12178e9fc2b037b0cca4c3c379f3c62d4fdd6fbe1ba039fdf3657fff1497f1213a3a38be0f7e495a0d177f1e092e13e7135afcee7419738d5e3ec3068d60db1357df0c2d60d8f185467f2c775674648ce340a64345728a8f2e4b7cd60a485a8a4a109e86ec208a8855066c2e10caf88c671ba682409795a55ba6f39eff6291730f34923e75ea60dab38071cd41bbc7862bf0fdbcd56b49bff62efbc783e9f45910b63f25f0c3d55e27205f04aa56f09ad6e80e05f0c5f7c647775e048b4bebc25f388a3598a650368d9b07eac535c22eb181c7a4373a12f63421d621ce41d0528705f78305a7ef906943045da6c627fba57818b2280788ff8a791dc8a8a76c760e8568c5fd72f95752ec8f2f627b5a7c487017e46ed05d5ec3d054ca3e1697fefc32bccac7d9536aca5ebc5db1c5315e8b7e14868b503ddcab7cd2238ac0a5cf63784f14faaea2b7a581191cd31068cf440c5362e42dfa545d91531b0db54587c97fd748e47e4874b9cedec0647301670e651ef63fb9777c8b134985d2c57630fb0a39ebe8c1a1d4e8ed6a0977683adcedbebd0eccc2a7026cfddcf0b70922f40ea72ea6acc176309a3f147be58db4c7108d7586d3661e7ad4bb8f513f9ee4931b616fc6311edd3b575c98bad22e033675e339944ec0d51ae09a7aadd01bc13cb0ea9d7c812dce2bce576a74ed489437f95e2e0ca47d6d77435653b61df2077657b4cd09e8e2091baf840909c6ef1ab5481766bff8cd2364b03724c39c0c289691aab33196346166af6a64e51b939f7ce4f0d8f5e94a3b9b720b755242dc0860ae2a0be74ba42bf8f1c367cf5fc8fdb4eb854799a12b30f3f9d355495a339936a9687334279d958777a7ed72874da2bdce86316eb4fb944a217229b702e2bed9603cbebf9dfcda2aabe6149882919d6689b01b6aea52f8b8e3a29a32b83c9d6ede16710f16e79032c25ef062e0948f484ace2afd4a4dfd05759672f7409cb896d769f3bb4b5161e6584e589b3d51e285d3f9209eb52bbca1e5e83979adc7b1742cdecc5e29a0d5fac9125a871a95f86006a6e7b346290305a1be5d3be8b27535a49e90f728e34cea8c0ecc808b563f7ea0e7b647556fde2b2a9a6e0571426a1682e794989b520bf9bf8e8d1a981225b0c5dad0666e263e7d253b0508b1dc0e9b07c5ffc705e47518bb06a95fdf94ee7028f848473efc83feb3fa4e9906e27fee54b19f880d531247cc1cec84f3429f99931f2fd7df6e3077eba9dbe5625b576e978601ae1c6af26257fb15b3693690d03535321537faf6ba3e06aba5ef12b80e59d8d882fe7f5fe9a47f95b77ed12d1bcfa2e77b1abcb60b73a3c4f1915a9014e2b82fe1b62bb8ff9a8203415d6cb4146b888bd5d4aa9609f478690bd27629f13b302c86cd3c87c9afdd9c2a0b5ae2f4728298be2312328619b87422f010ef531e0bb69f7db75a068f4448f635f75e20f6a370201f0de1c5ca628a560d564899f7eef5e7ec54de5d89640592fe58bd6b3ff995c79bd87e3fee3f4331da1778940061f0daaab056b3ef24f2f9f768b4fbf7577ab20e7c0c0d566971949df5215e3437bcf06311157cc01dbf804f60ab43221349fbef34fb66b6927c89387ad85a58e61e36223ead303f1b3a53e1248db4c65041b1558afb7050f0c70ab21871b74b255c8e2e19b07f6131b23e89403185461551db61e352d95ff49e0d26770c5ebbad3ff5f93b4db1b4125215ecbacf62d85b04e68291bebb3a81e4afaa5c5e7828fa346df8ec588539bd1a420218eb64a72d3848904a418144a401904dd9e6b125d5cccf9a4fcd1c64527d3ed7311386f034b3e3d59fec73f9a3e7b0f2b9304325417cd4d7cf72eb9d09653fb6b124a546083d66d1e8351aa008da0344d3d8649f0f8b6236a60334fd43340df0ea1218d3f02e73b315112a41dc33a50ace6c95f066dcbb0414fd2585190a27c4c81c3dc37e017c8e770d7c2d2634ca80b92e1b4612d3cfdf94d910cd5b0dc447d3298946ae2dc2831ca2d7f429b058420015740ac4ce31797dc82e8b565efd8dbcfee12c88e6cc37c83ee88f06a244ea1bd8df309c66ecbe6dc4dced96356eba313ff7262ff335c22bbab32668bf7d517c2c6f45df76b5ff3aae74b06f5a7e2dbd8b2e50474b30c2ea327a6cd8b78beee5eac1c3342e099d17327c7f4d528d51500b15587358b0bdb51d24a8a800260e8916e78020a9e40c21fc127326d3d032bebc6b2f4fdd6f2c0635e7a3ad57122495b25f65af4c62494694d90173b844f0a69597451ebad176f5f6531e5676bc8a9a49d329c5cc0f19b1074d648f4c3bfaaf3d982cf3f8af4466daf18ac74b9173c6e33b77222e8de580c551f62f0bb915b0c23c408402adf9b4a525e326af1d11acdf8bac0e0f6b6b80b734cc5cbb0c01a218ef2490690dda2c94887bc21faf276dc4c8e2cea0648b4532a889276e82c898cd5b2a46349b9b7e518a7f22af81e167ac73bc33ca31657f239e840756f258491b74c363e8c72856a5299ef62a45305fc64e7ef5fa1041b6fa2e3458be2cd9054884650030b92c4b0bb452365fcfbbc2367bfb6aefbe1d4d71136d0c0201ad15b5197da7b2dd0c16b7e9bd1bda0da2141b28c552c69c8ecbdc2ecc0058e8a3c6fa67ed7911792236b1aa4fa67476688111fb064d1ededed057cd965860e8143e5223723dfcf319dc1c740d1983d6b51113ec86890948d74746df40bf14ff3c85e9119628e3d330e5b843bbf0fb2e3dd85530f4ad7ba3350df363b3606a4b413987c9c28552d4e95b80edd1451869e25640d9c781f2b2b46e85ef6a670c07f9854784139e676eecc25893ae6e36a1a9e3263552dbc4eca5b2f41e56be1b707eee126fea0886844159a4c6daa951e8d20629ca22f3b84a1868b8272bd7e894981f5d08e02a148feb6b332b4176ea220165513310ca4eaf06a1104a9d91671343a5b1d599d8240ba5492e5043a05dd2e05f332a27df483f6a58b544</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 日常摸鱼 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随便写写 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>绘画课学习笔记</title>
      <link href="/2023/12/04/%E7%BB%98%E7%94%BB%E8%AF%BE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/04/%E7%BB%98%E7%94%BB%E8%AF%BE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="b7e77d36dcade690e2dec41f442e2711c73be13516ccc4cb87220589d290906d">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 日常摸鱼 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI学习——基础知识</title>
      <link href="/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
      <url>/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>AI学习记录1<br> <span id="more"></span></p><h1 id="part1-数学基础"><a href="#part1-数学基础" class="headerlink" title="part1:数学基础"></a>part1:数学基础</h1><h2 id="范数："><a href="#范数：" class="headerlink" title="范数："></a>范数：</h2><p>向量范数是将向量映射到标量的函数<em>f</em>，以下代码表示一个<em>L2</em>范数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure><h2 id="梯度："><a href="#梯度：" class="headerlink" title="梯度："></a>梯度：</h2><p>一个多元函数对其所有变量的偏导数。一个函数<em>f(x)</em>的输入是n维的向量，输出是一个标量。则函数<em>f(x)</em>相对于<em>x</em>的梯度是一个包含n个偏导数的向量，由于深度学习中，多元函数往往是复合的，所以一般使用链式法则来对复合函数求微分<br/><br>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。以下代码是一个简单的例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4.0</span>)</span><br><span class="line"><span class="comment">#创建梯度，标量函数关于向量x的梯度是向量，且具有相同形状</span></span><br><span class="line">x.requires_grad_(<span class="literal">True</span>)  <span class="comment"># 等价于x=torch.arange(4.0,requires_grad=True)</span></span><br><span class="line">y = <span class="number">2</span> * torch.dot(x, x) <span class="comment">#计算x和x的点积</span></span><br><span class="line">y.backward()<span class="comment">#4x</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()<span class="comment">#1</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><br>书上的梯度相关的数学的结论如下图：<br/><br><img src="梯度常用结论.png" alt="梯度结论"></p><h3 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h3><p><a href="https://zhuanlan.zhihu.com/p/273729929">参考链接-知乎</a><br/><br>通过该链接，学习到矩阵求导的基本规则如下图：<img src="矩阵布局.png" alt="矩阵求导图"><br><a href="https://zhuanlan.zhihu.com/p/262751195?utm_oi=628596077562040320&amp;utm_id=0">矩阵求导全解-知乎</a><br/><br>该链接详细介绍了矩阵求导的规范：<br/></p><h4 id="矩阵求导布局（视为规定）"><a href="#矩阵求导布局（视为规定）" class="headerlink" title="矩阵求导布局（视为规定）"></a>矩阵求导布局（视为规定）</h4><p>分子布局：导数的维度以分子为主<br/><br>分母布局：倒数的维度以分母为主<br/><br>在机器学习的算法推导里，通常遵循以下布局的规范：<br/></p><ul><li>如果向量或者矩阵对标量求导，则以分子布局为准。</li><li>如果标量对向量或者矩阵求导，则以分母布局为准。</li><li>对于向量对对向量求导，有些分歧，一般以分子布局的雅克比矩阵为主。<br><img src="布局图.png" alt="布局图"></li></ul><h4 id="标量对矩阵求导"><a href="#标量对矩阵求导" class="headerlink" title="标量对矩阵求导"></a>标量对矩阵求导</h4><p>算法如下图：<br/><br><img src="标量对矩阵求导算法.png" alt="标量对矩阵求导算法"></p><h4 id="矩阵对矩阵求导"><a href="#矩阵对矩阵求导" class="headerlink" title="矩阵对矩阵求导"></a>矩阵对矩阵求导</h4><p>算法如下图：<br/><br><img src="向量对向量求导算法.png" alt="向量对向量求导算法"></p><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p><a href="https://zhuanlan.zhihu.com/p/261375491">参考链接-知乎</a><br/><br>$x \leftarrow x^2 - \eta\frac{df(x)}{dx}$<br/><br>梯度作为一个向量，代表一个方向，朝着梯度的反方向变化可以使得函数值变小，$\eta$代表学习率</p><h3 id="反向传播算法-Back-propagation"><a href="#反向传播算法-Back-propagation" class="headerlink" title="反向传播算法(Back propagation)"></a>反向传播算法(Back propagation)</h3><p><a href="https://zhuanlan.zhihu.com/p/71892752">参考链接-知乎</a><br/><br>反向传播仅指用于计算梯度的算法,其具体流程如下图所示，能够从最后一层的梯度算出前面层数的梯度<br><img src="反向传播.png" alt="反向传播"><br>深度学习框架通过自动计算导数，即自动微分来加快求导。实践中，根据设计好的模型，系统会构建一个计算图，来跟踪计算是<strong>哪些数据通过哪些操作组合起来来产生输出</strong>。自动微分使得系统能够随后反向传播梯度，即意味着跟踪整个计算图，填充关于每个参数的偏导数</p><h2 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h2><ol><li>联合概率(joint probability): $P(A=a,B=b)$</li><li>条件概率(conditional probability): $P(A=a|B=b)$</li><li>贝叶斯定理(multiplication rule): $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$</li><li>边际化: $P(B) = \sum_{A} P(A,B)$，一般和贝叶斯定理结合使用</li><li>独立性：$P(A|B)=\frac{P(A,B)}{P(B)}=P(A)$</li></ol><h1 id="part2-线性神经网络"><a href="#part2-线性神经网络" class="headerlink" title="part2:线性神经网络"></a>part2:线性神经网络</h1><h2 id="线性回归与线性模型"><a href="#线性回归与线性模型" class="headerlink" title="线性回归与线性模型"></a>线性回归与线性模型</h2><p>线性回归基于几个简单的假设：假设自变量<em>x</em>和因变量<em>y</em>之间的关系是线性的， 即<em>y</em>可以表示为<em>x</em>中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。<br/><br>数据集称为训练数据集（training data set） 或训练集（training set）。每行数据（比如一次房屋交易相对应的数据）称为样本（sample），也可以称为数据点（data point）或数据样本（data instance）。试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。<br/><br>基本的模型如下：<br/></p><script type="math/tex; mode=display">price = w_{area} * area + w_{age} * age + b</script><p>其中<em>b</em>是输入特征的仿射变换，其特点是通过加权和对特征进行线性变换。并通过偏置项进行平移（translation）。<br/><br>预测模型可用点积表示：</p><script type="math/tex; mode=display">\hat{y}=\boldsymbol{w}^T\boldsymbol{x}+b</script><p>或用矩阵向量积表示：</p><script type="math/tex; mode=display">\hat{\boldsymbol{y}}=\boldsymbol{X}\boldsymbol{w}+b</script><p>其中 $\boldsymbol{X}\in{\mathbb{R}^{n*d}}$ 其每一行是一个样本，一共有n个预测结果。该模型的模型参数为$\boldsymbol{w}$和$b$,为找到最佳的参数，还需要两个工具：<br/></p><ol><li>一种模型质量的度量方式</li><li>一种能够更新模型以提高模型预测质量的方法</li></ol><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>使用损失函数量化目标的<strong><em>实际值</em></strong>与<strong><em>预测值</em></strong>之间的差距，通常选择非负数作为损失，数值越小越好，完美预测时损失为0。线性回归往往用平方误差作为损失函数：</p><script type="math/tex; mode=display">\ell^{(i)}(\boldsymbol{w},b) = \frac{1}{2} \left( \hat{y}^{(i)} - y^{(i)} \right)^2</script><p>为了度量模型在整个数据集上的预测质量，需要计算在训练集<em>n</em>个样本上的损失均值:</p><script type="math/tex; mode=display">L(\boldsymbol{w},b)=\frac{1}{n}\sum_{i=1}^n\ell^{(i)}(\boldsymbol{w},b)=\frac{1}{n}\sum_{i=1}^n(\boldsymbol{w}^T\boldsymbol{x}^{(i)}+b-y^{(i)})</script><p>训练模型的目的是找到一组参数，这组参数能最小化在所有训练样本上的总损失</p><h2 id="解析解"><a href="#解析解" class="headerlink" title="解析解"></a>解析解</h2><p><a href="https://blog.csdn.net/a755199443/article/details/104198033">参考链接-CSDN</a><br/><br>线性回归的解可用一个公式简单的表示，将<strong><em>b</em></strong>合并到<strong><em>w</em></strong>中，最小化$||\boldsymbol{y}-\boldsymbol{X}\boldsymbol{w}||^2$，得到解析解</p><script type="math/tex; mode=display">w^* = (X'X)^{-1}X'y</script><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>在无法得到解析解的情况下，我们也可以有效地训练模型。<strong><em>梯度下降</em></strong>（gradient descent）的通过不断地在损失函数递减的方向上更新参数来降低误差，但是每次更新参数都必须遍历整个数据集，为提高速度，通常在每次需要计算更新的时候随机抽取一小批样本。这种方法称为<strong><em>小批量随机梯度下降</em></strong>（minibatch stochastic gradient descent）。<br/></p><p>每次迭代抽取一个小批量$\mathcal{B}$，其由固定数量的训练样本组成。然后计算小批量的平均损失关于模型参数的导数。最后将梯度乘以一个预先确定的参数$\eta$，并从当前参数值减掉。</p><p>算法的步骤如下：（1）初始化模型参数的值（随机初始化）（2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。</p><script type="math/tex; mode=display">(\mathbf{w}, b) \leftarrow(\mathbf{w}, b)-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w}, b)} l^{(i)}(\mathbf{w}, b)</script><p>其中$|\mathcal{B}|$表示小批量中的样本数，即批量大小，而$\eta$表示学习率。这两个都是手动预先自定的，这些可以调整但不再训练过程中更新的参数成为<em>超参数</em>（hyperparameter）。<em>调参</em>（hyperparameter tuning）是选择超参数的过程。</p><h2 id="正态分布与平方损失"><a href="#正态分布与平方损失" class="headerlink" title="正态分布与平方损失"></a>正态分布与平方损失</h2><p>正态分布概率密度函数如下：</p><script type="math/tex; mode=display">p(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right)</script><p>均方误差损失函数可以用于线性回归的一个原因是：假设观测中包含了噪声且噪声服从正态分布，即</p><script type="math/tex; mode=display">y=\mathbf{w}^{\top} \mathbf{x}+b+\epsilon</script><p>其中，$\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)$.<br>现在可以写出通过给定的<strong>x</strong>观测到特定<em>y</em>的似然函数：</p><script type="math/tex; mode=display">P(y \mid \mathbf{x})=\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{1}{2 \sigma^2}\left(y-\mathbf{w}^{\top} \mathbf{x}-b\right)^2\right)</script><p>根据<a href="https://zhuanlan.zhihu.com/p/26614750">极大似然估计法</a>，参数<strong>w</strong>和<strong>b</strong>的最优值是使得整个数据集似然最大的值:</p><script type="math/tex; mode=display">P(\mathbf{y} \mid \mathbf{X})=\prod_{i=1}^n p\left(y^{(i)} \mid \mathbf{x}^{(i)}\right)</script><p>用最小化负对数进行进一步推导：</p><script type="math/tex; mode=display">-\log P(\mathbf{y} \mid \mathbf{X})=\sum_{i=1}^n \frac{1}{2} \log \left(2 \pi \sigma^2\right)+\frac{1}{2 \sigma^2}\left(y^{(i)}-\mathbf{w}^{\top} \mathbf{x}^{(i)}-b\right)^2</script><p>这样就使得最小化均方误差等价于对线性模型的极大似然估计了</p><h2 id="线性回归从零开始实现"><a href="#线性回归从零开始实现" class="headerlink" title="线性回归从零开始实现"></a>线性回归从零开始实现</h2><p>在每次迭代中，读取小批量训练，通过模型来获得一组预测。计算完损失后开始反向传播，存储每个参数的梯度。最后调用优化算法来更新模型参数。训练的轮数epoch和学习率lr都是超参数，从0开始实现线性回归的算法代码如下，具体参看书的3.2节<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##3.2.1生成数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    <span class="comment">#normal生成了一个张量，包含从均值为0，标准差为1的正态分布中随机抽取的随机数，num_examples表示生成张量的行数,len(w)表示生成张量的列数</span></span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    <span class="comment">#matmul用于矩阵乘法运算</span></span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape) <span class="comment">#加上高斯噪音，标准差是0.01</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"><span class="comment">#features每一行是一个二维数据样本，labels每一行包含一个标签值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;features:&#x27;</span>, features[<span class="number">0</span>],<span class="string">&#x27;\nlabel:&#x27;</span>, labels[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">d2l.plt.scatter(features[:, (<span class="number">1</span>)].detach().numpy(), labels.detach().numpy(), <span class="number">1</span>) <span class="comment">#绘制散点图</span></span><br><span class="line"><span class="comment">#d2l.plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.2读取数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    <span class="comment"># 这些样本是随机读取的，没有特定的顺序</span></span><br><span class="line">    random.shuffle(indices)<span class="comment">#打乱的数据顺序</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):<span class="comment">#batch_size为每个小批量的大小</span></span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])<span class="comment">#取出一组batch_size的索引值</span></span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class="line">        <span class="comment">#yield将一个函数转换为生成器函数，在调用生成器函数的过程中，每次遇到yield语句时，函数会暂停并保存当前所有的运行信息（保留局部变量），返回yield的值</span></span><br><span class="line">        </span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">##3.2.3初始化模型参数</span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.4定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br><span class="line">    <span class="comment">#当我们用一个向量加一个标量时，标量会被加到向量的每个分量上</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.5定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.6定义优化算法</span></span><br><span class="line"><span class="comment">#小批量随机梯度下降更新</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():<span class="comment">#禁用梯度计算，所有的require_grad被自动设置为false</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size <span class="comment">#lr是学习率,用batch_size批量大小来规范化步长</span></span><br><span class="line">            param.grad.zero_()<span class="comment">#清除梯度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.7训练</span></span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.03</span> <span class="comment">#学习率</span></span><br><span class="line">num_epochs = <span class="number">3</span> <span class="comment">#训练轮数</span></span><br><span class="line">net = linreg <span class="comment">#线性回归模型</span></span><br><span class="line">loss = squared_loss <span class="comment">#损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):<span class="comment">#data_iter遍历数据集，将训练数据集中的所有样本都使用一次（1000/10)</span></span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        <span class="comment"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()<span class="comment">#调用反向传播算法进行计算</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p><h2 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h2><p>对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量net，它是一个Sequential类的实例。 Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。这样的一个层称为全连接层，因为<strong>它的每一个输入都通过矩阵-向量乘法得到它的每个输出</strong><br/><br>通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。 我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。 当我们需要更复杂的模型时，高级API的优势将大大增加。 当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。下面是对比<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成数据集</span></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"><span class="comment">#调用框架中现有的API来读取数据，将特征和标签作为API的参数床底，通过数据迭代器指定batch_size</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line"><span class="comment">#定义模型，利用Sequential类直接完成</span></span><br><span class="line">  <span class="comment"># nn是神经网络的缩写</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#使用optim模块中的小批量随机梯度下降算法初始化SGD实例，指定要优化的参数（net.parameters()）以及优化算法所需的超参数字典</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        trainer.zero_grad()<span class="comment">#清零梯度</span></span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span> <span class="comment">#学习率</span></span><br><span class="line">num_epochs = <span class="number">3</span> <span class="comment">#训练轮数</span></span><br><span class="line">net = linreg <span class="comment">#线性回归模型</span></span><br><span class="line">loss = squared_loss <span class="comment">#损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):<span class="comment">#data_iter遍历数据集，将训练数据集中的所有样本都使用一次（1000/10)</span></span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        <span class="comment"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，</span></span><br><span class="line">        <span class="comment"># 并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>回归用于预测多少的问题，分类关心“哪一个”的问题。<br/><br>通过<strong>独热编码</strong>（one-hot encoding）可以进行分类数据的表示,softmax回归解决的就是数据份额里的问题。为了解决线性模型的分类问题，需要一个多个输出的模型和与输出一样多的<strong>仿射函数</strong>（affine function）。softmax回归也是一个单层的神经网络结构，如下图所示：<br/><br><img src="softmax回归.png" alt="softmax回归"><br>对于任何具有<em>d</em>个输入和 <em>q</em>个输出的全连接层，参数开销是$O(dq)$，这个开销可以减少到$O(dq/n)$，<em>n</em>可由我们灵活指定。<br/><br>我们希望模型的输出$\hat{y}_{j}$可以视为属于类$j$的概率,所以必须保证在任何数据上的输出都是非负的且总和为1。这时候就需要使用softmax函数如下式：<br/><br>$\begin{aligned} &amp; o_1=x_1 w_{11}+x_2 w_{12}+x_3 w_{13}+x_4 w_{14}+b_1 \\ &amp; o_2=x_1 w_{21}+x_2 w_{22}+x_3 w_{23}+x_4 w_{24}+b_2 \\ &amp; o_3=x_1 w_{31}+x_2 w_{32}+x_3 w_{33}+x_4 w_{34}+b_3\end{aligned}$<br/><br>$\hat{\mathbf{y}}=\operatorname{softmax}(\mathbf{o})$<br>其中：$\hat{y}_j=\frac{\exp \left(o_j\right)}{\sum_k \exp \left(o_k\right)}$<br/><br>softmax运算不会改变未规范化的预测<strong>o</strong>之间的大小次序，只会确定分配给每个类别的概率，尽管它是一个非线性函数，但其回归的输出仍然由输入特征的仿射变换决定，所以其仍是一个<strong>线性模型</strong>。<br/><br><br/><br>接下来需要一个损失函数，同样使用上文提到过的的<a href="https://zhuanlan.zhihu.com/p/26614750">极大似然估计</a>。极大似然估计的原理就是：<strong>利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值</strong>。<br/><br>极大似然估计中的采样必须满足独立同分布。对于一些情况，样本太多，无法得出分布的参数值，可以采样小样本后，利用极大似然估计获取假设中分布的参数值。<br/><br>假设整个数据集有{$\mathbf{X}$,$\mathbf{Y}$}具有n个样本，其中索引为<em>i</em>的样本由特征向量$\mathbf{x}^{(i)}$和独热标签向量$\mathbf{y}^{(i)}$组成。</p><script type="math/tex; mode=display">P(\mathbf{Y} \mid \mathbf{X})=\prod_{i=1}^n P\left(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}\right)</script><p>根据极大似然估计，可以最大化$P(Y|X)$,相当于最小化负对数似然：</p><script type="math/tex; mode=display">-\log P(\mathbf{Y} \mid \mathbf{X})=\sum_{i=1}^n-\log P\left(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}\right)=\sum_{i=1}^n l\left(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}\right)</script><p>其中对于任何标签$y$和模型预测$\hat{y}$，损失函数为：</p><script type="math/tex; mode=display">l(\mathbf{y}, \hat{\mathbf{y}})=-\sum_{j=1}^q y_j \log \hat{y}_j</script><p>这种损失函数成为交叉熵损失（cross-entropy loss）,它是一个两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数<br/><br><br/><br>对softmax的损失函数求导过程如下：<br/></p><script type="math/tex; mode=display">\begin{aligned} l(\mathbf{y}, \hat{\mathbf{y}}) & =-\sum_{j=1}^q y_j \log \frac{\exp \left(o_j\right)}{\sum_{k=1}^q \exp \left(o_k\right)} \\ & =\sum_{j=1}^q y_j \log \sum_{k=1}^q \exp \left(o_k\right)-\sum_{j=1}^q y_j o_j \\ & =\log \sum_{k=1}^q \exp \left(o_k\right)-\sum_{j=1}^q y_j o_j \end{aligned}</script><script type="math/tex; mode=display">\partial_{o_j}l(\mathbf{y},\hat{\mathbf{y}})=\frac{exp(o_j)}{\sum_{k=1}^{q}exp(o_k)}-y_j=softmax(\mathbf{(o)_j}-y_j</script><h2 id="softmax回归从零开始实现"><a href="#softmax回归从零开始实现" class="headerlink" title="softmax回归从零开始实现"></a>softmax回归从零开始实现</h2><p>此处将$28<em>28$像素的样本图像平展为784的向量（不利用图像空间结构的特征），由于数据集有10个类别，所以权重将构成$784</em>10$的矩阵，偏置为$1*10$的行向量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>接下来定义softmax的操作，利用sum的keepdim参数，指定保持在原始张量的轴数（参数1表示行求和）</p><script type="math/tex; mode=display">\mathrm{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k \exp(\mathbf{X}_{ik})}.</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)<span class="comment">#行求和且保持为一个列向量</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制，将partition进行了扩展</span></span><br></pre></td></tr></table></figure><p>接下来定义softmax回归模型，利用reshape将图像展平为向量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape((-<span class="number">1</span>, W.shape[<span class="number">0</span>])), W) + b)</span><br></pre></td></tr></table></figure><br>然后定义损失函数（交叉熵）：</p><script type="math/tex; mode=display">l(\mathbf{y}, \hat{\mathbf{y}})=-\sum_{j=1}^q y_j \log \hat{y}_j</script><p>y_hat包含两个样本在三个类别上的预测概率，标签则是y,则使用PyTorch张量的花式索引写法，将y作为y_hat中概率的索引有：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line">y_hat[[<span class="number">0</span>, <span class="number">1</span>], y]</span><br><span class="line"></span><br><span class="line"><span class="comment">#[out]:tensor([0.1000, 0.5000])</span></span><br></pre></td></tr></table></figure><br>实现交叉熵函数如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"></span><br><span class="line">cross_entropy(y_hat, y)</span><br></pre></td></tr></table></figure><br>分类精度是正确预测数和总预测数之比，直接优化精度或许会很困难，但精度是我们最关心的性能度量标准，使用如下函数计算得到预测正确的数量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(y_hat)</span></span><br><span class="line">    <span class="comment">#print(y_hat.type(y.dtype))</span></span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><br>同样，对于任意数据迭代器data_iter可访问的数据集，我们可以评估在任意模型net的精度。Accumulator创建了两个变量用于存储正确预测数量和预测总数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><br>训练模型的一个迭代周期的代码逻辑如下，updater是更新模型参数的常用函数，它接受批量大小作为参数。 它可以是d2l.sgd函数，也可以是框架的内置优化函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            <span class="comment"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用定制的优化器和损失函数</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><br>可与上一节内容进行对比<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        trainer.zero_grad()<span class="comment">#清零梯度</span></span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><br>训练函数如下,在train_iter访问到的训练数据集上训练一个模型net，改训练函数会运行多个迭代周期（由num_epochs指定）。每个迭代周期结束时，利用test_iter问到的测试数据集对模型进行评估：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    <span class="keyword">assert</span> train_loss &lt; <span class="number">0.5</span>, train_loss</span><br><span class="line">    <span class="keyword">assert</span> train_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> train_acc &gt; <span class="number">0.7</span>, train_acc</span><br><span class="line">    <span class="keyword">assert</span> test_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> test_acc &gt; <span class="number">0.7</span>, test_acc</span><br><span class="line"><span class="comment">#使用sgd</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span> d2l.sgd([W, b], lr, batch_size)</span><br><span class="line"><span class="comment">#正式训练</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br></pre></td></tr></table></figure></p><h2 id="softmax回归的简洁实现"><a href="#softmax回归的简洁实现" class="headerlink" title="softmax回归的简洁实现"></a>softmax回归的简洁实现</h2><p>由于softmax回归的输出层是一个全连接层，所以我们只需要在Sequential中添加一个带有10个输出的全连接层即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化模型训练参数</span></span><br><span class="line"><span class="comment"># PyTorch不会隐式地调整输入的形状。因此，</span></span><br><span class="line"><span class="comment"># 我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><br>若$o_k$中的一些数值非常大，那么$\exp(o_k)$可能大于数据类型容许的最大数字，即<em>上溢</em>（overflow）。解决这个问题的一个技巧是：在继续softmax计算之前，先从所有$o_k$中减去$\max(o_k)$。</p><script type="math/tex; mode=display">\begin{aligned}\hat y_j & =  \frac{\exp(o_j - \max(o_k))\exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k))\exp(\max(o_k))} \\& = \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.\end{aligned}</script><p>在减法和规范化步骤之后，可能有些$o_j - \max(o_k)$具有较大的负值。<br>由于精度受限，$\exp(o_j - \max(o_k))$将有接近零的值，即<em>下溢</em>（underflow）。<br>通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。</p><script type="math/tex; mode=display">\begin{aligned}\log{(\hat y_j)} & = \log\left( \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\& = \log{(\exp(o_j - \max(o_k)))}-\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} \\& = o_j - \max(o_k) -\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)}.\end{aligned}</script><p>交叉熵损失、优化算法、训练过程如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)<span class="comment">#自带了softmax运算</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment">#trainer可以通用</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"><span class="comment">#替换了net,loss,trainer模块</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日常学习debug记录1</title>
      <link href="/2023/11/29/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0debug%E8%AE%B0%E5%BD%951/"/>
      <url>/2023/11/29/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0debug%E8%AE%B0%E5%BD%951/</url>
      
        <content type="html"><![CDATA[<p>记录日常的遇到的一些问题以及解决方法<br> <span id="more"></span></p><h1 id="AI-studying"><a href="#AI-studying" class="headerlink" title="AI studying"></a>AI studying</h1><ol><li><p>虚拟环境<br/><br>在anaconda Prompt中，使用<code>conda create --name d2l python=3.9 -y</code>创建虚拟环境，使用<code>activate pytorch</code>激活虚拟环境，用<code>pip list</code>查看安装列表，用<code>conda deactivate</code>来退出环境<br/><br>在虚拟环境中安装AI全家桶时注意版本的对应，使用<code>print(torch.__version__)</code>查看torch的版本；使用<code>python --version</code>查看python版本；使用<code>nvidia-smi</code>查看CUDA版本,若安装CUDA Toolkit则使用<code>nvcc -V</code>查看运行时API版本</p></li><li><p>cuda的安装<br/><br>参考连接如下：<br/><br><a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">conda换源-清华</a><br/><br><a href="https://blog.csdn.net/jhsignal/article/details/111401628">cuda和cuDNN的安装</a><br/><br><a href="https://blog.csdn.net/qq_46126258/article/details/112708781">torch的GPU版本安装的系列问题</a>,<a href="https://blog.csdn.net/youtiankeng/article/details/127759029">版本对应图</a><br/><br>1.13.0版本的pytorch对应0.14.0的torchvision和3.7到3.10的python以及0.13.0的torchaudio，为了安装GPU正确版本（0.13.0及以上才有gpu 版本）的pytorch,先删除CPU版本的pytorch,使用<code>conda uninstall pytorch</code>或是使用<code>pip uninstall torch</code>去卸载（前者未换国内源或会失败）<br/><br>然后去官网安装对应的版本<a href="https://download.pytorch.org/whl/torch/">torch</a>，<a href="https://download.pytorch.org/whl/torchvision/">torchvision</a>，<a href="https://download.pytorch.org/whl/torchaudio/">torchaudio</a><br/><br>torch-1.13.0+cu116-cp37-cp37m-win_amd64.whl，意为torch1.13.0、cuda11.6、python3.7、win 64位系统的版本,故我们安装<code>torch-1.13.0+cu117-cp37-cp37m-win_amd64.whl</code>和<code>torchvision-0.14.0+cu117-cp37-cp37m-win_amd64</code>和<code>torchaudio-0.13.0+cu117-cp37-cp37m-win_amd64</code>三个whl文件,然后进入虚拟环境安装，分别使用<code>pip install torch-1.13.0.whl+cu117-cp37-cp37m-win_amd64.whl</code>和<code>pip install torchvision-0.14.0+cu117-cp37-cp37m-win_amd64.whl</code>以及<code>pip install torchaudio-0.13.0+cu117-cp37-cp37m-win_amd64.whl</code>安装</p></li><li><p>vscode配置python运行调试环境</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 日常debug </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建记录</title>
      <link href="/2023/11/29/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/"/>
      <url>/2023/11/29/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[ <span id="more"></span><p>该文章记录了操作该博客系统所需要的指令和注意事项（自用）</p><h1 id="主要参考教程"><a href="#主要参考教程" class="headerlink" title="主要参考教程"></a>主要参考教程</h1><p><a href="https://www.zhihu.com/search?type=content&amp;q=%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA">知乎博客搭建详解</a><br/></p><h1 id="相关指令"><a href="#相关指令" class="headerlink" title="相关指令"></a>相关指令</h1><ol><li>使用<code>hexo new &lt;title&gt;</code>更新文章</li><li>使用<code>hexo clean</code>清除旧的数据；使用<code>hexo g</code>生成新页面；使用<code>hexo d</code>部署到网页上，即<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></li><li>用<code>hexo s</code>预览网页效果</li><li>通过<code>https://mztom3745.com.cn</code>访问博客页面</li></ol><hr><h1 id="markdown写作"><a href="#markdown写作" class="headerlink" title="markdown写作"></a>markdown写作</h1><p>主要参考:<br/><br><a href="https://markdown.com.cn/cheat-sheet.html#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95">markdown官方教程</a><br/><br><a href="https://blog.csdn.net/wzk4869/article/details/126863936">markdown公式语法</a><br/><br><a href="https://zhuanlan.zhihu.com/p/544093361">hexo插入公式</a><br/><br><a href="https://blog.csdn.net/qq_42951560/article/details/123596899">hexo-markdown渲染器</a><br/><br>发现hexo的mathjax不支持求和、属于等公式<br><a href="https://bugwz.com/2019/09/17/hexo-markdown-renderer/#1-2%E3%80%81hexo-renderer-kramed">各个渲染版本</a><br/><br><a href="https://zhuanlan.zhihu.com/p/35988761">高级公式支持</a><br/><br><a href="https://blog.csdn.net/weixin_44441126/article/details/119745642">冲突解决</a><br/><br><a href="https://www.cnblogs.com/iHeling/p/14095986.html">kramed语义冲突</a><br/><br><a href="https://blog.csdn.net/qq_43401552/article/details/104805466">markdown+hexo+next图片插入</a><br/><br><a href="https://blog.csdn.net/qq_42951560/article/details/122552109">博客加密码</a><br/></p>]]></content>
      
      
      <categories>
          
          <category> 网页搭建 </category>
          
          <category> 个人博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
