<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>读书日记</title>
      <link href="/2023/12/04/%E8%AF%BB%E4%B9%A6%E6%97%A5%E8%AE%B0/"/>
      <url>/2023/12/04/%E8%AF%BB%E4%B9%A6%E6%97%A5%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="6bca059ada886f3a2e57ca4863afed60c9f0773d1ad45c07847457300ecd3cfb">a46f637dae47b598d230519d9010266293819c88a8abe3cbfbfcc78e1dc08b39058b734fb434ef34ee66f7a794d422713f42161f910faa3c5de2c7a177634b30598d3327c66f3184ae1eda097c1cf49bd4fc705d3addcad4bd6e852d674eb83855b019572acada797b1f8d1563bd9b41cc937328e13b951df95de74e31dc9c75a2eb9c412e41f210b44bd397f794c137670522d646c2db55748b5c1e88dde019d75e7397b1aa101fd9d0484419aaf2002599d104e3d70be28879533bbf5bd430eff0b8959fa34d5d51315fb42dd66c7a6973fe4fa5bc32aa0d140124b56194024f0f6f07349912d64287b52a4f1b50fcbd8d73e8852e0840d81d10184789e93f77b45ad84de88323af443568b6ae126c5766d05f4a63acb396c4990ccc4f70dae047fdbdc803e34182663aaafd617f352314643980ee75b6a327ae592f0862f17c67b089cccdc88eb5d43ffd1ee12840daf21d7108f4493a7c95fbf8caba8d8182c4021d88c0c0c5354d82e6098e1f8bda32b019cb4dc5c13d17914b195b930d6bdbde3f769024c2d1628df269307f8646ad782849f84759e6b342b171097a65d9882263980a68da33eed3b9c33005246696defd8488d4ffa2f7e331738890868c92953590eda7e0f91db2b99653b52a0bf3617fd7167aa22717520522ee7f9f2a6f7d8fedd978faae06e14fe36b59cf0cbae228e46b189fb44cce0bde39445225845de481dd2aa5170eb483a63184f5372ab905d2025f1f6ec755a0552c5a4b1d21f136aa471985d254834854cf188c59dd5d1d78850a1ad5841e83bec9b85933d274d04eacf98d6ac3dd25f45b081746367a8dd2ddb1d2752a387ac5b86f170414044e251b14608620f5160cb6554c81fc9b79e54f6e7828afab518ef5d6483506fefabc7f92230afcc951ca52ea702b7a16a3b985f1991425c3bdccef207010d8e8698ab40d9420498abcfce0209e4dc3a0003c27c9193cb7221546d339aca11849a9b8585b93eaa54230175590adb001aaff634a7ed503be97fb7b4f43fce4e496cd668aa9b61544efe9dd1181fb2bc1b906df3b3f789bbc0a1d1e54d0515f5d8c80dd04489798b249b6cba12fb462b70afac7be919e75ea5ec621e04bf3c99543957cd78abcebfdf1b7c732fe010c4c413eee9cc8763481c876e268763dce5ea7d72ed98767387f27424ceef37e6e095b2ab74549659f0d252d552c164505ed9c84417cf94fb4d700bb115559c4a274fa5a468be2f6adea07d316a4e8af0adf71096e74048a0ff3ab10a2accb100b5ec138003151539cfe5a18979f80f69f536c3b5c75190a62c7a5ada2698d98a32b797cceec697dfe1622885acce4265726f7a68c54bdfe980f60edcda7cf583a55450280bf13d80b7c874ad822e9065fd34845576c6377ae9de9207aaeaedfea3a65414d341669ff44ff623ffe013347271075025d32c7442ffd929eb158d33277b26263914484d86bacb9075bd739857bdaa60db761499a37032d26bcc4f85a848ec154cda0011ad7ef00f6aacf346d6f6d41735290463991ae099cd1ec95270ad617746c599fdef3eefd59a913d0091fa4fb4cf0cdccdd483e41977ec29f6292df90afa3fc682a6ac2db9757bed4bafd016c572561d2fc71ea5c071f3a532a83f57f2d54670069b2a7efc55d5b3f282430dab5a36d29e96ae87dd7aefbda43dbdd20275894fb4f7c29ed69b57df6345ced16cb9cc6d7b00ed678670a8ae7b8e64e023e328f018503c098a69b17d0e84291d664426d3070a4bfec7daa3cb000720a1023a51daf600a8ab777a80a07948b240c5163ad1a6ca16bb0daf3391bce572dcd818b6d6f71f17286d498fc5ca3970b39ee7b6621514dc549e1d6959959e686215b6caada276bbb4d5e7d8b60c9cd96c829c77387d0878b11afd0da46aa760db78188f602ed1d9c4bd16355654a537f608dc2f02d46c3c06edcc0e6f34196838a31e5b14bb0ffe02d99e92bb4f0f23673d16eaf6a24bd994c79af3446cc11c60f3f112ea15ee136410b223d5b791b245a8e086376d9b3f2628f067a044342afc4a6bb2fb05456a945b2f4d354479147770ea93e65249c8cca755da1c8dd0a600a5042c518025675557ef721365cf418da9ad9d1fedb6435c38090133b527f9db745d2c568289041a091a06b98811b2dca419aa10a6c3fe806bfc1c9e14589d990451a138d69d7e743d53d174fe2e53ccf3ed03d6a7bd8504cc15144e294bd893767c1a214935425c6733087938a303ed5036b896cba445be205fd3a2dfa2d1acf3da85ea8c51afb8d1fbeac402386aa96765eab200ab9f46a2102d3c9da955bd37233ffc78abb1c79cef17c8144fe70fcf2f00e7d5894bb3d1b244dbb095fd7d15e716ebbf271f65cfba93f0b337fe53a7c14683f30362f3d28cc656a50b11cdb23015350f58547d2fe31e260dfe2569fa7f507bc0a3dad28d4a1c4d1ec906e01738e67bf0569525dbe8445e7f88d2d2cf1601626467ef35f7e37213c89830f757ed240c0b9a893f053eb2ddb803a20095ed2a8b381ad96fc7899e503aa89858e3891650b748641a32958b85c37bbf52c49ae7d71a67a2d14e2879573811947f2e2c7692e13c6514a4795bf702be489319910c0b16d256b9d71adbe9beb2c88fc71dc9c8d4d508d95c7bb4bf2fca47edabdf254325839d1549a5751bd1d3e14e039ce41bb78c0bc1fb671ac4ebd88a93fcd068b8de99906c17d45a765d2c35d74819125cbce7c6e8c88f27bfe35ba02a11cbcd7aa26220350c6cc0c400ba45353d4a17ea02a90d7a9d2df58cda129415501a90a290b0692744d39769bd4b2a169bf7ca9472e4e486248e93a9b68d1d45ab9394dd46945a20211172f28990c2c984530ed918445c7192bc1e27271f1d0f6d231074a16f8b60fd1bddfce58567ba3f66d95cf82f41b9ba9c687fc46885e61878a0a22d2a04beb8ad60683bc6915c732df3a3351990b24293835b1e54744e99ae12106d8023aa6a700923cd444e67a782b1b745d173b6c553a1c043884c3a8c4bd287e018b060865e8d9b8</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 日常摸鱼 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随便写写 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>绘画课学习笔记</title>
      <link href="/2023/12/04/%E7%BB%98%E7%94%BB%E8%AF%BE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/04/%E7%BB%98%E7%94%BB%E8%AF%BE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="77b1c85eddfcedc15bbb1f7b12d0a8a19de03f56fed201f0d496546304ef8bf3">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 日常摸鱼 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI学习1——基础知识</title>
      <link href="/2023/12/01/AI%E5%AD%A6%E4%B9%A01%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
      <url>/2023/12/01/AI%E5%AD%A6%E4%B9%A01%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>AI学习记录1<br> <span id="more"></span></p><h1 id="part1-数学基础"><a href="#part1-数学基础" class="headerlink" title="part1:数学基础"></a>part1:数学基础</h1><h2 id="范数："><a href="#范数：" class="headerlink" title="范数："></a>范数：</h2><p>向量范数是将向量映射到标量的函数<em>f</em>，以下代码表示一个<em>L2</em>范数<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = torch.tensor([3.0, -4.0])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure></p><h2 id="梯度："><a href="#梯度：" class="headerlink" title="梯度："></a>梯度：</h2><p>一个多元函数对其所有变量的偏导数。一个函数<em>f(x)</em>的输入是n维的向量，输出是一个标量。则函数<em>f(x)</em>相对于<em>x</em>的梯度是一个包含n个偏导数的向量，由于深度学习中，多元函数往往是复合的，所以一般使用链式法则来对复合函数求微分<br/><br>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。以下代码是一个简单的例子<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(4.0)</span><br><span class="line">#创建梯度，标量函数关于向量x的梯度是向量，且具有相同形状</span><br><span class="line">x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)</span><br><span class="line">y = 2 * torch.dot(x, x) #计算x和x的点积</span><br><span class="line">y.backward()#4x</span><br><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x.sum()</span><br><span class="line">y.backward()#1</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><br>书上的梯度相关的数学的结论如下图：<br/><br><img src="梯度常用结论.png" alt="梯度结论"></p><h3 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h3><p><a href="https://zhuanlan.zhihu.com/p/273729929">参考链接-知乎</a><br/><br>通过该链接，学习到矩阵求导的基本规则如下图：<img src="矩阵布局.png" alt="矩阵求导图"><br><a href="https://zhuanlan.zhihu.com/p/262751195?utm_oi=628596077562040320&amp;utm_id=0">矩阵求导全解-知乎</a><br/><br>该链接详细介绍了矩阵求导的规范：<br/></p><h4 id="矩阵求导布局（视为规定）"><a href="#矩阵求导布局（视为规定）" class="headerlink" title="矩阵求导布局（视为规定）"></a>矩阵求导布局（视为规定）</h4><p>分子布局：导数的维度以分子为主<br/><br>分母布局：倒数的维度以分母为主<br/><br>在机器学习的算法推导里，通常遵循以下布局的规范：<br/></p><ul><li>如果向量或者矩阵对标量求导，则以分子布局为准。</li><li>如果标量对向量或者矩阵求导，则以分母布局为准。</li><li>对于向量对对向量求导，有些分歧，一般以分子布局的雅克比矩阵为主。<br><img src="布局图.png" alt="布局图"></li></ul><h4 id="标量对矩阵求导"><a href="#标量对矩阵求导" class="headerlink" title="标量对矩阵求导"></a>标量对矩阵求导</h4><p>算法如下图：<br/><br><img src="标量对矩阵求导算法.png" alt="标量对矩阵求导算法"></p><h4 id="矩阵对矩阵求导"><a href="#矩阵对矩阵求导" class="headerlink" title="矩阵对矩阵求导"></a>矩阵对矩阵求导</h4><p><img src="向量对向量求导算法.png" alt="向量对向量求导算法"></p><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p><a href="https://zhuanlan.zhihu.com/p/261375491">参考链接-知乎</a><br/><br>$x \leftarrow x^2 - \eta\frac{df(x)}{dx}$<br/><br>梯度作为一个向量，代表一个方向，朝着梯度的反方向变化可以使得函数值变小，$\eta$代表学习率</p><h3 id="反向传播算法-Back-propagation"><a href="#反向传播算法-Back-propagation" class="headerlink" title="反向传播算法(Back propagation)"></a>反向传播算法(Back propagation)</h3><p><a href="https://zhuanlan.zhihu.com/p/71892752">参考链接-知乎</a><br/><br>反向传播仅指用于计算梯度的算法</p><h2 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h2><ol><li>联合概率(joint probability): $P(A=a,B=b)$</li><li>条件概率(conditional probability): $P(A=a|B=b)$</li><li>贝叶斯定理(multiplication rule): $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$</li><li>边际化: $P(B) = \sum_{A} P(A,B)$，一般和贝叶斯定理结合使用</li><li>独立性：$P(A|B)=\frac{P(A,B)}{P(B)}=P(A)$</li></ol><h2 id="遗留问题"><a href="#遗留问题" class="headerlink" title="遗留问题"></a>遗留问题</h2><p>P43 2.5.2非标量变量的反向传播、2.5.3分离计算</p><h1 id="part2-线性神经网络"><a href="#part2-线性神经网络" class="headerlink" title="part2:线性神经网络"></a>part2:线性神经网络</h1><h2 id="线性回归与线性模型"><a href="#线性回归与线性模型" class="headerlink" title="线性回归与线性模型"></a>线性回归与线性模型</h2><p>线性回归基于几个简单的假设：假设自变量<em>x</em>和因变量<em>y</em>之间的关系是线性的， 即<em>y</em>可以表示为<em>x</em>中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。<br/><br>数据集称为训练数据集（training data set） 或训练集（training set）。每行数据（比如一次房屋交易相对应的数据）称为样本（sample），也可以称为数据点（data point）或数据样本（data instance）。试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。<br/><br>基本的模型如下：<br/></p><script type="math/tex; mode=display">price = w_{area} * area + w_{age} * age + b</script><p>其中<em>b</em>是输入特征的仿射变换，其特点是通过加权和对特征进行线性变换。并通过偏置项进行平移（translation）。<br/><br>预测模型可用点积表示：</p><script type="math/tex; mode=display">\hat{y}=\boldsymbol{w}^T\boldsymbol{x}+b</script><p>或用矩阵向量积表示：</p><script type="math/tex; mode=display">\hat{\boldsymbol{y}}=\boldsymbol{X}\boldsymbol{w}+b</script><p>其中 $\boldsymbol{X}\in{\mathbb{R}^{n*d}}$ 其每一行是一个样本，一共有n个预测结果。该模型的模型参数为$\boldsymbol{w}$和$b$,为找到最佳的参数，还需要两个工具：<br/></p><ol><li>一种模型质量的度量方式</li><li>一种能够更新模型以提高模型预测质量的方法</li></ol><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>使用损失函数量化目标的<strong><em>实际值</em></strong>与<strong><em>预测值</em></strong>之间的差距，通常选择非负数作为损失，数值越小越好，完美预测时损失为0。线性回归往往用平方误差作为损失函数：</p><script type="math/tex; mode=display">\ell^{(i)}(\boldsymbol{w},b) = \frac{1}{2} \left( \hat{y}^{(i)} - y^{(i)} \right)^2</script><p>为了度量模型在整个数据集上的预测质量，需要计算在训练集<em>n</em>个样本上的损失均值:</p><script type="math/tex; mode=display">L(\boldsymbol{w},b)=\frac{1}{n}\sum_{i=1}^n\ell^{(i)}(\boldsymbol{w},b)=\frac{1}{n}\sum_{i=1}^n(\boldsymbol{w}^T\boldsymbol{x}^{(i)}+b-y^{(i)})</script><p>训练模型的目的是找到一组参数，这组参数能最小化在所有训练样本上的总损失</p><h2 id="解析解"><a href="#解析解" class="headerlink" title="解析解"></a>解析解</h2><p><a href="https://blog.csdn.net/a755199443/article/details/104198033">参考链接-CSDN</a><br/><br>线性回归的解可用一个公式简单的表示，将<strong><em>b</em></strong>合并到<strong><em>w</em></strong>中，最小化$||\boldsymbol{y}-\boldsymbol{X}\boldsymbol{w}||^2$，得到解析解</p><script type="math/tex; mode=display">w^* = (X'X)^{-1}X'y</script><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>在无法得到解析解的情况下，我们也可以有效地训练模型。<strong><em>梯度下降</em></strong>（gradient descent）的通过不断地在损失函数递减的方向上更新参数来降低误差，但是每次更新参数都必须遍历整个数据集，为提高速度，通常在每次需要计算更新的时候随机抽取一小批样本。这种方法称为<strong><em>小批量随机梯度下降</em></strong>（minibatch stochastic gradient descent）。</p>]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日常学习debug记录1</title>
      <link href="/2023/11/29/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0debug%E8%AE%B0%E5%BD%951/"/>
      <url>/2023/11/29/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0debug%E8%AE%B0%E5%BD%951/</url>
      
        <content type="html"><![CDATA[<p>记录日常的遇到的一些问题以及解决方法<br> <span id="more"></span></p><h1 id="AI-studying"><a href="#AI-studying" class="headerlink" title="AI studying"></a>AI studying</h1><ol><li><p>虚拟环境<br/><br>在anaconda Prompt中，使用<code>conda create --name d2l python=3.9 -y</code>创建虚拟环境，使用<code>activate pytorch</code>激活虚拟环境，用<code>pip list</code>查看安装列表，用<code>conda deactivate</code>来退出环境<br/><br>在虚拟环境中安装AI全家桶时注意版本的对应，使用<code>print(torch.__version__)</code>查看torch的版本；使用<code>python --version</code>查看python版本；使用<code>nvidia-smi</code>查看CUDA版本,若安装CUDA Toolkit则使用<code>nvcc -V</code>查看运行时API版本</p></li><li><p>cuda的安装<br/><br>参考连接如下：<br/><br><a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">conda换源-清华</a><br/><br><a href="https://blog.csdn.net/jhsignal/article/details/111401628">cuda和cuDNN的安装</a><br/><br><a href="https://blog.csdn.net/qq_46126258/article/details/112708781">torch的GPU版本安装的系列问题</a>,<a href="https://blog.csdn.net/youtiankeng/article/details/127759029">版本对应图</a><br/><br>1.13.0版本的pytorch对应0.14.0的torchvision和3.7到3.10的python以及0.13.0的torchaudio，为了安装GPU正确版本（0.13.0及以上才有gpu 版本）的pytorch,先删除CPU版本的pytorch,使用<code>conda uninstall pytorch</code>或是使用<code>pip uninstall torch</code>去卸载（前者未换国内源或会失败）<br/><br>然后去官网安装对应的版本<a href="https://download.pytorch.org/whl/torch/">torch</a>，<a href="https://download.pytorch.org/whl/torchvision/">torchvision</a>，<a href="https://download.pytorch.org/whl/torchaudio/">torchaudio</a><br/><br>torch-1.13.0+cu116-cp37-cp37m-win_amd64.whl，意为torch1.13.0、cuda11.6、python3.7、win 64位系统的版本,故我们安装<code>torch-1.13.0+cu117-cp37-cp37m-win_amd64.whl</code>和<code>torchvision-0.14.0+cu117-cp37-cp37m-win_amd64</code>和<code>torchaudio-0.13.0+cu117-cp37-cp37m-win_amd64</code>三个whl文件,然后进入虚拟环境安装，分别使用<code>pip install torch-1.13.0.whl+cu117-cp37-cp37m-win_amd64.whl</code>和<code>pip install torchvision-0.14.0+cu117-cp37-cp37m-win_amd64.whl</code>以及<code>pip install torchaudio-0.13.0+cu117-cp37-cp37m-win_amd64.whl</code>安装</p></li><li><p>vscode配置python运行调试环境</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 日常debug </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建记录</title>
      <link href="/2023/11/29/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/"/>
      <url>/2023/11/29/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>该文章记录了操作该博客系统所需要的指令和注意事项<br> <span id="more"></span></p><h1 id="相关指令"><a href="#相关指令" class="headerlink" title="相关指令"></a>相关指令</h1><ol><li>使用<code>hexo new &lt;title&gt;</code>更新文章</li><li>使用<code>hexo clean</code>清除旧的数据；使用<code>hexo g</code>生成新页面；使用<code>hexo d</code>部署到网页上，即<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></li><li>用<code>hexo s</code>预览网页效果</li><li>通过<code>https://mztom3745.com.cn</code>访问博客页面</li></ol><hr><h1 id="markdown写作"><a href="#markdown写作" class="headerlink" title="markdown写作"></a>markdown写作</h1><p>主要参考:<br/><br><a href="https://markdown.com.cn/cheat-sheet.html#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95">markdown官方教程</a><br/><br><a href="https://blog.csdn.net/wzk4869/article/details/126863936">markdown公式语法</a><br/><br><a href="https://zhuanlan.zhihu.com/p/544093361">hexo插入公式</a><br/><br><a href="https://blog.csdn.net/qq_42951560/article/details/123596899">hexo-markdown渲染器</a><br/><br>发现hexo的mathjax不支持求和、属于等公式<br><a href="https://bugwz.com/2019/09/17/hexo-markdown-renderer/#1-2%E3%80%81hexo-renderer-kramed">各个渲染版本</a><br/><br><a href="https://zhuanlan.zhihu.com/p/35988761">高级公式支持</a><br/><br><a href="https://blog.csdn.net/weixin_44441126/article/details/119745642">冲突解决</a><br/><br><a href="https://www.cnblogs.com/iHeling/p/14095986.html">kramed语义冲突</a><br/><br><a href="https://blog.csdn.net/qq_43401552/article/details/104805466">markdown+hexo+next图片插入</a><br/><br><a href="https://blog.csdn.net/qq_42951560/article/details/122552109">博客加密码</a><br/></p>]]></content>
      
      
      <categories>
          
          <category> 网页搭建 </category>
          
          <category> 个人博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
