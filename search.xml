<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>ResNet</title>
      <link href="/2023/12/22/ResNet/"/>
      <url>/2023/12/22/ResNet/</url>
      
        <content type="html"><![CDATA[<p>ResNet论文精读<br> <span id="more"></span><br><a href="https://www.zhihu.com/zvideo/1434795406001180672">视频</a></p><h1 id="Gpt"><a href="#Gpt" class="headerlink" title="Gpt"></a>Gpt</h1><p>ResNet（残差网络）的残差学习方法是一种在网络层之间加入残差单元（residual unit）的框架，使得网络可以学习输入与输出之间的残差（即差异部分）。这种学习方式有助于网络捕捉更深层次的特征，并减少了深层网络中的梯度消失问题。以下是详细介绍ResNet残差学习方法的几个关键点：</p><ol><li><strong>解决深度网络性能退化问题</strong>：随着网络层数的增加，传统的深度学习模型（如VGG）会出现性能退化的问题，即训练误差和测试误差同时增加。ResNet通过引入残差学习框架，有效地解决了这一问题，使得网络的性能随着层数的增加而持续提升。</li><li><strong>残差学习框架</strong>：ResNet的核心思想是残差学习，即在传统的网络层之间加入残差单元（residual unit），使得网络可以学习输入与输出之间的残差（即差异部分）。这种学习方式有助于网络捕捉更深层次的特征，并减少了深层网络中的梯度消失问题。</li><li><strong>易于优化</strong>：由于残差学习的引入，ResNet的优化变得更为容易。实验表明，ResNet可以在训练过程中更快地收敛，并且对超参数（如学习率）的选择更为鲁棒。</li><li><strong>广泛的应用</strong>：ResNet在多个领域都表现出了卓越的性能，包括图像分类、目标检测、语义分割等。其强大的特征提取能力和易于优化的特性使其成为了许多深度学习任务的基准模型。</li><li><strong>影响深远</strong>：ResNet的提出对后续的深度学习研究产生了深远的影响，许多后续的模型（如DenseNet、ResNeXt、MobileNets等）都在不同程度上受到了ResNet的启发。</li><li><strong>AlphaGo的助力</strong>：ResNet也是AlphaGo等高级人工智能系统的基础组件之一，它在处理复杂任务和提高模型性能方面发挥了关键作用。</li></ol>]]></content>
      
      
      <categories>
          
          <category> AI论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vit_basic</title>
      <link href="/2023/12/21/Vit_basic/"/>
      <url>/2023/12/21/Vit_basic/</url>
      
        <content type="html"><![CDATA[<p>Vit基础<br><span id="more"></span><br><a href="https://www.zhihu.com/zvideo/1437034536677404672">视频</a><br><a href="https://clichong.blog.csdn.net/article/details/118859875">笔记</a><br>首先是最初的Transformer论文</p><h2 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h2><p>序列到序列的生成，主流的模型是在卷积神经网络（CNN）或循环神经网络（RNN）中的encoder+decoder中添加注意力机制。Transformer仅使用了注意力机制且取得了很好的效果。</p><p>PS:<strong>注意力机制</strong>是一种使得模型能够自动关注输入数据中最重要部分的机制。通过学习得到的权重，模型可以自适应地调整关注的焦点，以更好地完成当前的任务。</p><p>PS:RNN和CNN的主要区别在于：</p><ol><li>应用领域：RNN主要用于序列数据的处理，如NLP和语音识别；而CNN主要用于图像数据的处理，如图像分类和物体检测。</li><li>网络结构：RNN具有循环连接，能够记忆和利用之前的信息；而CNN具有卷积层和池化层，用于提取图像中的局部特征。</li><li>参数共享：CNN中的神经元通常使用共享的卷积核，而RNN没有这种参数共享的特性。</li></ol><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>卷积能做多个输出——Multi-Head<br>自注意力机制——Self-attention</p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>编码器将文本转化为机器学习可以理解的向量，解码器一个一个词地产生新的序列（自回归模型），其中解码器过去时刻是输出会作为当前时刻的输入</p><h3 id="Encoder-and-Decoder"><a href="#Encoder-and-Decoder" class="headerlink" title="Encoder and Decoder"></a>Encoder and Decoder</h3><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>Layer=2*sub-layers=multi-head self attention mechanism + mlp</p><script type="math/tex; mode=display">LayerNorm(x+Sublayer(x))</script><p>LayerNorm：(对比BN),对每个样本进行均值0，方差1的操作，和BN方向不同。在时序的样本中，每个样本的长度可能不同，LayerNorm只对样本做操作。</p><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>解码器作了自回归，当前输入是上一个时刻的输出。由于注意力机制能看到完整的输入<br>Masked Multi-Head Attetion：保证在t时刻不会看到之后的输入。</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>注意力函数是将a query和a set of key-value对映射成输出的函数，输出是value（这些都是向量）的加权和，value的权重是通过query和key的相似度（a compatibility function）计算得来的。</p><p>Transformer采用的是 Scaled Dot-Product Attention，将queries和keys作内积算相似度</p><p>Multi-Head Attetion的使用：</p><ol><li>编码输入n*d的向量，Multi-Head Attetion三个输入分别表示K V Q，同样的输入分成三个，输出是输入的加权和（实际上有一个投影的过程）</li><li>解码器mask的作用是将不该访问的权重设为0</li><li>编码器的K,V和解码器的Q作为解码器第二个Multi-Head Attetion的输入，意思是对每一个解码器的每一个Q，需要算一个输出。根据解码器的不同输入，从编码器中挑感兴趣的东西</li></ol><h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed-Forward"></a>Feed-Forward</h3><p>将一个mlp对每一个词作用一次，每个词作相同的mlp。<br>在经过Attention之后，每个输出向量都汇聚了序列中感兴趣的信息。<br>所以mlp可以分开作</p><p>在序列信息的使用上对比RNN，RNN是将上一层的输出传回下一个输入进行mlp运算，但Attetion是全局地进行汇聚</p><h3 id="Embedding-and-Softmax"><a href="#Embedding-and-Softmax" class="headerlink" title="Embedding and Softmax"></a>Embedding and Softmax</h3><p>将token（小词）映射为向量</p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>将时序信息加入到输入中，便于Attention全局使用</p><h1 id="Vit"><a href="#Vit" class="headerlink" title="Vit"></a>Vit</h1><p><a href="https://www.zhihu.com/zvideo/1449195245754380288">视频</a><br><a href="https://clichong.blog.csdn.net/article/details/119205006">笔记</a><br>titie:AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</p><p>图片处理存在大序列的问题，Vit将图片分割为patch。将image patches当作tokens。<br>Transformers相对于CNN，缺乏归纳偏置（inductive biases=locality_相邻区域有相似特征+translation equivariance_先平移先卷积是一致的，这些都是先验信息，所需参数能更少）</p>]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对比学习basic</title>
      <link href="/2023/12/21/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0basic/"/>
      <url>/2023/12/21/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0basic/</url>
      
        <content type="html"><![CDATA[<p>对比学习综述学习<br> <span id="more"></span></p><h1 id="对比学习引入"><a href="#对比学习引入" class="headerlink" title="对比学习引入"></a>对比学习引入</h1><p>模型对比着进行学习，它无需知道样本的标签，只需要知道哪些样本是类似的，哪些样本是不类似的。将这些样本通过一个网络进行编码，得到对应的特征。在特征空间中，我们期望类似图片的特征尽可能接近，不同图片的特征尽可能原理。</p><p>我们需要代理任务（pretask）来定义图片的相似和不相似,提供一个监督信号去训练模型，即自监督学习。<br>Instance discrimination(个体判别)是一种经典的代理任务，<br>出自论文Unsupervised Feature Learning via Non-Parametric Instance Discrimination<br>对比学习的好处在于其<strong>灵活性</strong>，通过代理任务定义好正负样本后，就是标准化的流程，即通过一个模型学习到特征，用对比学习的一些典型损失去进行训练（如MoCo提到的NCE loss）</p><h1 id="MoCo论文精读"><a href="#MoCo论文精读" class="headerlink" title="MoCo论文精读"></a>MoCo论文精读</h1><p>MoCo:Momentum Contrast for Unsupervised Visual Representation Learning<br>参考：<br/><br><a href="https://www.zhihu.com/zvideo/1454723120678936576">讲解</a><br/><br><a href="https://blog.csdn.net/weixin_44751294/article/details/122720525">对应笔记</a></p><p>下面是一些简单的随记：</p><p>动量：加权移动平均，让当前时刻的输出不完全依赖于当前输入<br/><br>$y_t=m<em>y_{y-1}+(1-m)</em>x_t$<br/><br>MoCo利用动量特性，缓慢更新编码器，让字典特征尽量保持一致<br/></p><p>MoCo将对比学习看成字典查询任务，建立一个动态字典=队列+移动平均编码器。<br/><br>结果上MoCo在ImageNet数据上的的linear protocol评估具有很好的效果。linear protocol将骨干网络冻住，只学习全连接层，相当于把提前训练好的模型当成特征提取器，这样就能很好地对比预训练模型的效果。<br>MoCo具有很好的迁移性，能在下游任务中有很好的效果。<br/></p><p>视觉中无监督远不如有监督。<br/><br>pretext：代理任务，为了学习好的特征，去充当标签信息，有监督学习是一定的。<br/><br>loss functions：对比学习相似相近，相异就远，对比学习的目标target是在变化的，这不同于有监督学习。<br/><br>这两个是和有监督不一样的地方，是该文章的重点。</p><p>MoCo强调使用的字典是<strong>大</strong>（large）且<strong>一致的</strong>（consistent）。<br/><br>方法的核心之一是<strong>Dictionary as a queue</strong>，用队列可以重用训练得到key,这些key是之前的mini-batch训练中所得到的。<br/><br>之二是<strong>Momentum update</strong>，用动量的思想更新编码器。这个生成key的编码器无法通过反向传播的方式去更新参数。所以通过动量的方式从q的编码器中提取参数变化进行缓慢更新。</p><p>目前架构有：端到端的学习end-to-end：字典能实时更新，但字典不能太大因为塞不下、第二种是memory bank：从memroy bank中去抽样去训练，但缺乏特征的一致性。然后就是MoCo。</p><h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p><a href="https://www.zhihu.com/zvideo/1460828005077164032">视频</a><br><a href="https://blog.csdn.net/weixin_44751294/article/details/122728621">笔记</a></p><h2 id="阶段一：百花齐放"><a href="#阶段一：百花齐放" class="headerlink" title="阶段一：百花齐放"></a>阶段一：百花齐放</h2><h3 id="InstDisc"><a href="#InstDisc" class="headerlink" title="InstDisc"></a>InstDisc</h3><p>提出了个体判别任务，将每一张图片都看作是一个类别，通过一个卷积神经网络来将图片进行编码成一个低维特征，然后使得这些特征在特征空间上都尽可能的区分开（特征用memory bank存储）</p><h3 id="InstSpread"><a href="#InstSpread" class="headerlink" title="InstSpread"></a>InstSpread</h3><p>视为SimCLR的前身，使用mini-batch中的数据作为负样本，使用一个编码器进行端到端的学习。但由于字典不够大，负样本不够多，未使用SimCLR投影函数和多样的数据增广导致其效果并不是很好。</p><h3 id="CPC"><a href="#CPC" class="headerlink" title="　CPC"></a>　CPC</h3><p>属于机器学习中生成式模型，提出了一个通用的结构，能够处理连续的序列，并进行合理的预测。</p><h3 id="CMC"><a href="#CMC" class="headerlink" title="CMC"></a>CMC</h3><p>提出一个物体的许多视角都可以看作正样本。它将不同的view作为正例，将其他图片以及其他图片的views作为负例子，进行训练。</p><p>第一阶段介绍以上四篇工作，可以看到以上的工作代理任务不尽相同，其中有个体判别，有预测未来，还有多视角多模态。使用的目标函数也不尽相同，有NCE，infoNCE以及其变体。使用的模型也可以是不同的，比如InvaSpread使用的是相同的编码器对key和query进行编码，CMC对key和query使用的是不同的编码，是百花齐放的。</p><h2 id="阶段二：CV双雄"><a href="#阶段二：CV双雄" class="headerlink" title="阶段二：CV双雄"></a>阶段二：CV双雄</h2><h3 id="MoCov1"><a href="#MoCov1" class="headerlink" title="MoCov1"></a>MoCov1</h3><p>主要贡献就是把之前对比学习的一些方法归纳为一个字典查询问题。提出了一个队列，一个动量编码器，从而形成一个又大又一致的字典，帮助更好的进行对比学习。MoCo对InstDise的改进可以说是简单又有效,其提出用队列替换memory bank以及提出了动量更新的方式，对效果有显著的提升，同时对后续工作也产生了深远的影响。</p><h3 id="SimCLRv1"><a href="#SimCLRv1" class="headerlink" title="SimCLRv1"></a>SimCLRv1</h3><p>方法比较简单，概念上也更容易理解，方法上也很容易解释。其优化是1.采用了更多的数据增强（裁剪和色彩变幻）2.采用了非线性的投影头（）3.用了更多的batch-size和更久的训练。</p><h3 id="MoCov2"><a href="#MoCov2" class="headerlink" title="MoCov2"></a>MoCov2</h3><p>借鉴了SimCLR，选用了MLP的投影层，更多的数据增强方式，cosine learning rate schedule，以及更多的迭代epoch。SimCLR在batch较少的情况下无法发挥效果，在batch多的情况下才可以出效果，但是算力要求太高了。所以MOCO是一个对于计算资源要求不是很高，但是却很有效的模型。</p><h3 id="SimCLRv2"><a href="#SimCLRv2" class="headerlink" title="SimCLRv2"></a>SimCLRv2</h3><p>SimCLRv2相比SimCLRv1就是用了更大的模型，加深了projection head，最后用了半监督编码器。</p><h3 id="SWaV-Swap-Assignment-Views"><a href="#SWaV-Swap-Assignment-Views" class="headerlink" title="SWaV(Swap Assignment Views)"></a>SWaV(Swap Assignment Views)</h3><p>将对比学习和之前的聚类的方法合在的一起。<br>聚类也是无监督特征表示学习的方法，而且它也希望相似的物体都聚集在一个聚类中心附近，不相似的物体推到别的聚类中心。以下是SwAV的思想</p><ol><li>如果是和负例进行对比的话，需要和成千上万个负例进行对比，即使是MOCO中6w个负例，也只是一个近似的值，但是聚类的话，就仅仅需要和三千个聚类核心即可。</li><li>这些聚类中心是有含义的，而如果像之前一样用负样本进行对比学习的话，有的负样本不均衡，有的还可能是正样本，不如聚类中心有效。</li><li>Multi-crop：选择了两个160×160的crop去注意全局特征，选择四个96×96的crop去注意局部特征。这样在计算量变化不大的情况下，可以获取更多的正样本，也可以同时注意了全局特征与局部的特征。</li></ol><p>到了第二阶段，其实很多细节都趋于统一了，比如目标函数都是使用infoNCE，模型都归一为用一个encoder+projection head了，大家都采用了一个更强的数据增强，都想用一个动量编码器，也都尝试训练更久，最后在ImageNet上的准确度也逐渐逼近于有监督的基线模型。</p><h2 id="阶段三：不用负样本"><a href="#阶段三：不用负样本" class="headerlink" title="阶段三：不用负样本"></a>阶段三：不用负样本</h2><h3 id="BYOL"><a href="#BYOL" class="headerlink" title="BYOL"></a>BYOL</h3><p>将配对的问题换成了预测的问题，<br>即用自己视角的特征去预测另一个视角的特征，通过预测的方式去完成模型<br>后面有篇博客认为BatchNorm是BYOL成功的关键，相当于提供了一种隐式的正负样本学习<br>再后面作者自证并非如此</p><h3 id="SimSiam"><a href="#SimSiam" class="headerlink" title="SimSiam"></a>SimSiam</h3><p>对比学习堆太多了，SimSiam 不需要用负样本，不需要大的batchsize，不需要动量编码器，即使在这种条件下，SimSiam不仅没有模型坍塌，反而取得了很好的模型效果。强调停止梯度的重要性,通过逐步更新的方式避免坍塌。</p><p>SimSiam能够成功训练的原因，不会发生模型坍塌，主要就是因为有stop gradient这个操作的存在。由于stop gradient，可以将SimSiam的结构看成是一个EM算法，相当于是在解决两个子问题，而模型更新也在交替进行，相当于不断的更新聚类中心。</p><h2 id="阶段四：Transformer"><a href="#阶段四：Transformer" class="headerlink" title="阶段四：Transformer"></a>阶段四：Transformer</h2><h3 id="MoCov3"><a href="#MoCov3" class="headerlink" title="MoCov3"></a>MoCov3</h3><p>改进了Vit自监督学习训练方法。<br>是MoCoV2+SimSiam的结合。<br>训练Vit时出现大batch-size掉点的问题，通过查看回传梯度，发现梯度也会在第一层出现波动（patch projection）。发现把patch projection给冻住会有更好的效果。</p><h3 id="DINO"><a href="#DINO" class="headerlink" title="DINO"></a>DINO</h3><p>同样谈及了Vit自监督学习训练方法。<br>Self-distillation with no labels<br>自蒸馏框架，在目标函数的地方做了一个额外的工作centering，避免模型的坍塌</p>]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI学习——卷积神经网络</title>
      <link href="/2023/12/20/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2023/12/20/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<p>AI学习记录5<br>ketword:卷积、特诊<br> <span id="more"></span></p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><em>卷积神经网络</em>（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。</p><p>卷积神经网络需要的参数少于全连接架构的网络，而且卷积也很容易用GPU并行计算。<br>因此卷积神经网络除了能够高效地采样从而获得精确的模型，还能够高效地计算。</p><p>接下来将介绍构成所有卷积网络主干的基本元素。包括卷积层本身、填充（padding）和步幅（stride）的基本细节、用于在相邻区域汇聚信息的汇聚层（pooling）、在每一层中多通道（channel）的使用，以及有关现代卷积网络架构的仔细讨论。 </p><h1 id="全连接到卷积"><a href="#全连接到卷积" class="headerlink" title="全连接到卷积"></a>全连接到卷积</h1><p>对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。 此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。</p><p>卷积神经网络能克服多层感知机的弱势，利用图像中本就拥有丰富的结构进行学习。</p><h2 id="不变性"><a href="#不变性" class="headerlink" title="不变性"></a>不变性</h2><p>卷积神经网络将<em>空间不变性</em>（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。存在以下两个有益的概念：</p><ol><li><em>平移不变性</em>（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li><li><em>局部性</em>（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</li></ol><p>下面由多层感知机开始进行推导：<br/><br>多层感知机的输入是二维图像$\mathbf{X}$，其隐藏表示$\mathbf{H}$在数学上是一个矩阵，在代码中表示为二维张量。<br>其中$\mathbf{X}$和$\mathbf{H}$具有相同的形状。</p><p>使用$[\mathbf{X}]_{i, j}$和$[\mathbf{H}]_{i, j}$分别表示输入图像和隐藏表示中位置（$i$,$j$）处的像素。<br>为了使每个隐藏神经元都能接收到每个输入像素的信息，我们将参数从权重矩阵（如同我们先前在多层感知机中所做的那样）替换为四阶权重张量$\mathsf{W}$。假设$\mathbf{U}$包含偏置参数，我们可以将全连接层形式化地表示为:</p><script type="math/tex; mode=display">\begin{aligned} \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}\\ &=  [\mathbf{U}]_{i, j} +\sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned}</script><p>索引$a$和$b$通过在正偏移和负偏移之间移动覆盖了整个图像。<br>对于隐藏表示中任意给定位置（$i$,$j$）处的像素值$[\mathbf{H}]_{i, j}$，可以通过在$x$中以$(i, j)$为中心对像素进行加权求和得到，加权使用的权重为$[\mathsf{V}]_{i, j, a, b}$。</p><h2 id="平移不变性"><a href="#平移不变性" class="headerlink" title="平移不变性"></a>平移不变性</h2><p>现在引用上述的第一个原则：平移不变性。<br>这意味着检测对象在输入$\mathbf{X}$中的平移，应该仅导致隐藏表示$\mathbf{H}$中的平移。即$\mathsf{V}$和$\mathbf{U}$（两个参数）实际上不依赖于$(i, j)$的值，即$[\mathsf{V}]_{i, j, a, b} = [\mathbf{V}]_{a, b}$。并且$\mathbf{U}$是一个常数，比如$u$。因此，我们可以简化$\mathbf{H}$定义为：</p><script type="math/tex; mode=display">[\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b} [\mathbf{X}]_{i+a, j+b}</script><p>这就是<em>卷积</em>（convolution）。我们是在使用系数$[\mathbf{V}]_{a, b}$对位置$(i, j)$附近的像素$(i+a, j+b)$进行加权得到$[\mathbf{H}]_{i, j}$。</p><p>通过平移不变性，我们将$[\mathbf{V}]_{a, b}$转化为了$[\mathsf{V}]_{i, j, a, b}$，大大减少了系数数目</p><h2 id="局部性"><a href="#局部性" class="headerlink" title="局部性"></a>局部性</h2><p>现在引用上述的第二个原则：局部性。如上所述，为了收集用来训练参数$[\mathbf{H}]_{i, j}$的相关信息，我们不应偏离到距$(i, j)$很远的地方。这意味着在$|a|&gt; \Delta$或$|b| &gt; \Delta$的范围之外，我们可以设置$[\mathbf{V}]_{a, b} = 0$。因此，我们可以将$[\mathbf{H}]_{i, j}$重写为</p><script type="math/tex; mode=display">[\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}</script><p>上式被称为一个<em>卷积层</em>（convolutional layer），而卷积神经网络是包含卷积层的一类特殊的神经网络。而$\mathbf{V}$被称为<em>卷积核</em>（convolution kernel）或者<em>滤波器</em>（filter），亦或简单地称之为该卷积层的<em>权重</em>，通常该权重是可学习的参数。</p><p>以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。<br>参数大幅减少的代价是，我们的特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。</p><p>以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到未知数据中。但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据。</p><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>在数学上，卷积被定义为：</p><script type="math/tex; mode=display">(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}</script><p>即卷积是当把一个函数“翻转”并移位$\mathbf{x}$时，测量$f$和$g$之间的重叠。当为离散对象时，积分就变成求和。例如，对于由索引为$\mathbb{Z}$的、平方可和的、无限维向量集合中抽取的向量，我们得到以下定义：</p><script type="math/tex; mode=display">(f * g)(i) = \sum_a f(a) g(i-a)</script><p>对于二维张量，则为$f$的索引$(a, b)$和$g$的索引$(i-a, j-b)$上的对应加和：</p><script type="math/tex; mode=display">(f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b)</script><p>对比上式，主要区别是不是使用$(i+a, j+b)$，而是使用差值。但这仅是表面上的差异。</p><h2 id="通道"><a href="#通道" class="headerlink" title="通道"></a>通道</h2><p>事实上，图像一般包含三个通道/三种原色（红、绿、蓝），它并非二维张量，而是一个由高度、宽度和颜色组成的三维张量，比如包含$1024 \times 1024 \times 3$个像素。<br>前两个轴与像素的空间位置有关，而第三个轴可以看作每个像素的多维表示。<br>因此，我们将$\mathsf{X}$索引为$[\mathsf{X}]_{i, j, k}$。由此卷积相应地调整为$[\mathsf{V}]_{a,b,c}$，而不是$[\mathbf{V}]_{a,b}$。<br>同时，隐藏表示$\mathsf{H}$也采用三维张量。对于每一个空间位置，我们想要采用一组而不是一个隐藏表示。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。<br>因此，我们可以把隐藏表示想象为一系列具有二维张量的<em>通道</em>（channel）。<br>这些通道有时也被称为<em>特征映射</em>（feature maps），因为每个通道都向后续层提供一组空间化的学习特征。<br>直观上可以想象在靠近输入的底层，一些通道专门识别边缘，而一些通道专门识别纹理。</p><p>为了支持输入$\mathsf{X}$和隐藏表示$\mathsf{H}$中的多个通道，我们可以在$\mathsf{V}$中添加第四个坐标，即$[\mathsf{V}]_{a, b, c, d}$。综上所述，</p><script type="math/tex; mode=display">[\mathsf{H}]_{i,j,d} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a, b, c, d} [\mathsf{X}]_{i+a, j+b, c}</script><p>其中隐藏表示$\mathsf{H}$中的索引$d$表示输出通道，而随后的输出将继续以三维张量$\mathsf{H}$作为输入进入下一个卷积层。<br>所以该式定义具有多个通道的卷积层，而其中$\mathsf{V}$是该卷积层的权重。</p><h1 id="图像卷积"><a href="#图像卷积" class="headerlink" title="图像卷积"></a>图像卷积</h1><p>严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是<em>互相关运算</em>（cross-correlation），而不是卷积运算。<br>在卷积层中，输入张量和核张量通过<strong>互相关运算</strong>产生输出张量。</p><p>二维互相关运算流程如下：<br><img src="互相关运算.png" alt="互相关运算"><br>卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。<br>当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到一个单一的标量值，由此我们得出了这一位置的输出张量值。</p><p>输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1，<br>而卷积核只与图像中每个大小完全适合的位置进行互相关运算。<br>所以，输出大小等于输入大小$n_h \times n_w$减去卷积核大小$k_h \times k_w$，即：</p><script type="math/tex; mode=display">(n_h-k_h+1) \times (n_w-k_w+1).</script><p>实现一个二维互相关运算函数如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><br>实现上述互相关运算如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">corr2d(X, K)</span><br></pre></td></tr></table></figure></p><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。<br>所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。<br>就像我们之前随机初始化全连接层一样，在训练基于卷积层的模</p><p>下面基于上面定义的<code>corr2d</code>函数<strong>实现二维卷积层</strong>。<br>在<code>__init__</code>构造函数中，将<code>weight</code>和<code>bias</code>声明为两个模型参数。前向传播函数调用<code>corr2d</code>函数并添加偏置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure></p><h2 id="图像中目标的边缘检测"><a href="#图像中目标的边缘检测" class="headerlink" title="图像中目标的边缘检测"></a>图像中目标的边缘检测</h2><p>如下是<strong>卷积层的一个简单应用：</strong>通过找到像素变化的位置，来(<strong>检测图像中不同颜色的边缘</strong>)。<br>首先，我们构造一个$6\times 8$像素的黑白图像。中间四列为黑色（$0$），其余像素为白色（$1$）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br></pre></td></tr></table></figure><br>接下来，我们构造一个高度为$1$、宽度为$2$的卷积核<code>K</code>。<br>当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br></pre></td></tr></table></figure><br>现在，我们对参数<code>X</code>（输入）和<code>K</code>（卷积核）执行互相关运算。<br>如下所示，<strong>输出<code>Y</code>中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘</strong>，其他情况的输出为$0$。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y = corr2d(X, K)</span><br></pre></td></tr></table></figure><br>现在我们将输入的二维图像转置，再进行如上的互相关运算。<br>之前检测到的垂直边缘消失了。说明这个卷积核K只可以检测垂直边缘，无法检测水平边缘。</p><h2 id="学习卷积核"><a href="#学习卷积核" class="headerlink" title="学习卷积核"></a>学习卷积核</h2><p>有了更复杂数值的卷积核或连续的卷积层时，不可能手动设计滤波器。下面通过<strong>学习由<code>X</code>生成<code>Y</code>的卷积核</strong></p><p>首先闲构造一个卷积层，让其卷积核初始化为随机张量。<br>在每次迭代中，我们比较Y与卷积层输出的平方误差，然后计算梯度来更新卷积核。<br>为了简单起见，我们在此使用内置的二维卷积层，并忽略偏置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span></span><br><span class="line"><span class="comment"># 其中批量大小和通道数都为1</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    <span class="comment"># 迭代卷积核</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><br>十次迭代后，学习到的卷积核权重非常接近我们之前定义的卷积核K。</p><h2 id="互相关和卷积"><a href="#互相关和卷积" class="headerlink" title="互相关和卷积"></a>互相关和卷积</h2><p>需要水平和垂直翻转二维卷积核张量，然后对输入张量执行互相关运算，<br>才是真正的卷积运算。</p><p>由于卷积核是从数据中学习到的，因此无论这些层执行严格的卷积运算还是互相关运算，卷积层的输出都不会受到影响。</p><h2 id="特征映射和感受野"><a href="#特征映射和感受野" class="headerlink" title="特征映射和感受野"></a>特征映射和感受野</h2><p>卷积层有时被称为<em>特征映射</em>（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。</p><p>在卷积神经网络中，对于某一层的任意元素$x$，其<em>感受野</em>（receptive field）是指在前向传播期间可能影响$x$计算的所有元素（来自所有先前层）。</p><p>感受野可能大于输入的实际大小<br><img src="互相关运算.png" alt="互相关运算"><br>给定$2 \times 2$卷积核，阴影输出元素值$19$的感受野是输入阴影部分的四个元素。<br>假设之前输出为$\mathbf{Y}$，其大小为$2 \times 2$，现在我们在其后附加一个卷积层，该卷积层以$\mathbf{Y}$为输入，输出单个元素$z$。<br>在这种情况下，$\mathbf{Y}$上的$z$的感受野包括$\mathbf{Y}$的所有四个元素，而输入的感受野包括最初所有九个输入元素。<br>因此，当一个特征图中的任意元素需要检测<strong>更广</strong>区域的输入特征时，我们可以构建一个<strong>更深</strong>的网络。</p><h1 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h1>]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI学习——深度学习计算</title>
      <link href="/2023/12/19/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/"/>
      <url>/2023/12/19/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<p>AI学习记录4<br> <span id="more"></span></p><h1 id="层与块"><a href="#层与块" class="headerlink" title="层与块"></a>层与块</h1><p>线性模型中，它<br>（1）接收一些输入；<br>（2）生成相应的标量输出；<br>（3）具有一组相关参数。<br>考虑多个输出的网络时，层<br>（1）接受一组输入；<br>（2）生成相应的输出；<br>（3）由一组可调整参数描述。</p><p>为了实现更为复杂的网络，引入神经网络块的概念。<br><em>块</em>（block）可以描述单个层、由多个层组成的组件或整个模型本身。<br>使用块进行抽象的一个好处是可以将一些块组合成更大的组件，<br>这一过程通常是递归的，<br>可以通过定义代码来按需生成任意复杂度的块，<br>我们可以通过简洁的代码实现复杂的神经网络。</p><p>回顾之前的多层感知机,其中<code>net(X)</code>实际上是<code>net.__call__(X)</code>的简写<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">20</span>)</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><br>下面的代码实现了一个多层感知机，其具有256个单元的隐藏层和10维的输出层。实现需要提供构造函数和前向传播函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span></span><br><span class="line">        <span class="comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#1.初始化nn.Module类中定义的任何属性</span></span><br><span class="line">        <span class="comment">#2.执行父类的构造逻辑</span></span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># 隐藏层</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br></pre></td></tr></table></figure></p><h1 id="顺序块"><a href="#顺序块" class="headerlink" title="顺序块"></a>顺序块</h1><p>接下来看如何实现我们自己的Sequential类，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">            <span class="comment"># 这里，module是Module子类的一个实例。我们把它保存在&#x27;Module&#x27;类的成员</span></span><br><span class="line">            <span class="comment"># 变量_modules中。_module的类型是OrderedDict</span></span><br><span class="line">            self._modules[<span class="built_in">str</span>(idx)] = module</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><br>其中每一个Module都有个_modules属性，其主要的优点是：在模块的参数初始化过程中， 系统知道在_modules字典中查找需要初始化参数的子块。</p><p>有时我们可能希望合并既不是上一层的结果也不是可更新参数的项，<br>我们称之为<em>常数参数</em>（constant parameter）。<br>例如，我们需要一个计算函数<br>$f(\mathbf{x},\mathbf{w}) = c \cdot \mathbf{w}^\top \mathbf{x}$的层，<br>其中$\mathbf{x}$是输入，<br>$\mathbf{w}$是参数，<br>$c$是某个在优化过程中没有更新的指定常量。<br>所以我们实现了一个<code>FixedHiddenMLP</code>类，如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FixedHiddenMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span></span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 使用创建的常量参数以及relu和mm函数</span></span><br><span class="line">        X = F.relu(torch.mm(X, self.rand_weight) + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 复用全连接层。这相当于两个全连接层共享参数</span></span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 控制流</span></span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:</span><br><span class="line">            X /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><br>在这个FixedHiddenMLP模型中，我们实现了一个隐藏层， 其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 <br/><br>这个权重不是一个模型参数，因此它永远不会被反向传播更新<br>然后，神经网络将这个固定层的输出通过一个全连接层。</p><p>在返回输出之前，模型做了一些不寻常的事情：<br>它运行了一个while循环，在$L_1$范数大于$1$的条件下，<br>将输出向量除以$2$，直到它满足条件为止。<br>最后，模型返回了<code>X</code>中所有项的和。<br>（这些行为仅供代码添加方法展示，并无理论上的含义）</p><p>我们还可以<strong>混合搭配各种组合块的方法</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)<span class="comment">#nn.Linear是全连接层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure></p><h1 id="参数-parameters-管理"><a href="#参数-parameters-管理" class="headerlink" title="参数(parameters)管理"></a>参数(parameters)管理</h1><h2 id="参数访问"><a href="#参数访问" class="headerlink" title="参数访问"></a>参数访问</h2><p>在选择了架构并设置了超参数后，我们就进入了训练阶段。<br>此时，我们的目标是找到使损失函数最小化的模型参数值。<br>经过训练后，我们将需要使用这些参数来做出未来的预测。</p><p>下面以具有单隐藏层的多层感知机为例来介绍参数管理：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><br>可以使用<code>print(net[2].state_dict())</code>获得第0层的参数</p><p>以下代码从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)<span class="comment">#获取值、梯度和额外信息</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br></pre></td></tr></table></figure><br>参数是复合的对象，包含值、梯度和额外信息。 这就是我们需要显式参数值的原因。<br>由于还未调用反向传播，所以参数的梯度还处于初始状态,即<code>net[2].weight.grad == None</code>为真</p><p>以下代码能够一次性地访问所有的参数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br></pre></td></tr></table></figure></p><p>同时块也能实现一些嵌套：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 在这里嵌套</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br></pre></td></tr></table></figure><br>可以通过<code>print(rgnet)</code>去查看嵌套的细节</p><h2 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h2><p>内置的初始化器如下,其将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_normal)</span><br></pre></td></tr></table></figure><br>也可以对某些块应用不同的初始化方法:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_xavier</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_42</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">net[<span class="number">0</span>].apply(init_xavier)</span><br><span class="line">net[<span class="number">2</span>].apply(init_42)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data)</span><br></pre></td></tr></table></figure></p><p>也可以自定义初始化方法对参数进行初始化，举例如下：<br><img src="自定义初始化.png" alt="自定义初始化"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Init&quot;</span>, *[(name, param.shape)</span><br><span class="line">                        <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>])</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line">net[<span class="number">0</span>].weight[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure><br>也能随时对参数进行更改<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data[:] += <span class="number">1</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">42</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p><p>也可以在多个层之间共享参数:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><h1 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h1><p>可以构造一个不带参数的自定义层如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br></pre></td></tr></table></figure><br>也可以定义带有参数的层，可以使用内置函数来管理参数，<br>这样就不需要为每个自定义层编写自定义的序列化程序。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br></pre></td></tr></table></figure><br>可实例化MyLinear类并访问其模型参数。并自定义层执行前向传播<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linear = MyLinear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">linear.weight</span><br><span class="line">linear(torch.rand(<span class="number">2</span>, <span class="number">5</span>))</span><br></pre></td></tr></table></figure><br>也可以向使用内置的全连接层一样使用自定义层<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(MyLinear(<span class="number">64</span>, <span class="number">8</span>), MyLinear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(torch.rand(<span class="number">2</span>, <span class="number">64</span>))</span><br></pre></td></tr></table></figure></p><h1 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h1><p>在PyTorch中，CPU和GPU可以用<code>torch.device(&#39;cpu&#39;)</code><br>和<code>torch.device(&#39;cuda&#39;)</code>表示。<br>应该注意的是，<code>cpu</code>设备意味着所有物理CPU和内存，<br>这意味着PyTorch的计算将尝试使用所有CPU核心。<br>然而，<code>gpu</code>设备只代表一个卡和相应的显存。<br>如果有多个GPU，我们使用<code>torch.device(f&#39;cuda:&#123;i&#125;&#39;)</code><br>来表示第$i$块GPU（$i$从0开始）。<br>另外，<code>cuda:0</code>和<code>cuda</code>是等价的。</p><p>可以显示地将张量存储在GPU上，如使用以下代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>注意如果要实现X+Y，需要在同一个设备（GPU）上执行。<br>不要简单地X加上Y，因为这会导致异常， 运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。<br><img src="GPU.png" alt="GPU"><br>所以需要将X复制到第二块GPU处,然后再进行运算<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Z = X.cuda(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure><br>如果Z已经存在，调用Z.cuda(1)会返回Z且不会分配新的内存</p><p>单个GPU相对运行速度快，但在设备（CPU、GPU和其他机器）之间传输数据比计算慢得多。这也使得并行化变得更加困难。经验而言，多个小操作比一个大操作糟糕得多。且一次执行几个操作比代码中散布的许多单个操作要好得多。 如果一个设备必须等待另一个设备才能执行其他操作， 那么这样的操作可能会阻塞。 </p><p>当我们打印张量或将张量转换为NumPy格式时， 如果数据不在内存中，框架会首先将其复制到内存中， 这会导致额外的传输开销。 更糟糕的是，它现在受制于全局解释器锁，使得一切都得等待Python完成。</p><p>同样，神经网络模型可以指定设备。 下面的代码将模型参数放在GPU上。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=try_gpu())</span><br></pre></td></tr></table></figure><br>当输入为GPU上的张量时，模型将在同一GPU上计算结果。</p>]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI学习——多层感知机</title>
      <link href="/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2023/12/13/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>AI学习记录2<br> <span id="more"></span></p><h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><h2 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h2><p>如果标签通过放射变换后确实和输入数据相关，那么上一章的内容就以及足够。但是实际上线性的假设往往难以成立：线性意味着单调，任何特征的增大都会导致模型输出的增大或者模型输出的减小，但如同<em>用体温预测死亡率</em>，对<em>猫狗图像分类</em>的这一系列问题，线性的假设是难以成立的。<br/><br>所以我们需要考虑特征之间的相关交互作用，即使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。可以通过在网络中加入一个或多个隐藏层来突破线性模型的限制。<br><img src="多层感知机.png" alt="多层感知机"><br>该多层感知机由4个输入、3个输出，隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。所以层数为2，且两个层是全连接的，参数开销很大。<br/><br>隐藏层的输出用$\mathbf{H} \in \mathbb{R}^{n \times h}$表示，称为<em>隐藏表示</em>（hidden representations）。也称<em>隐藏层变量</em>（hidden-layer variable）或<em>隐藏变量</em>（hidden variable）。<br>因为隐藏层和输出层都是全连接的，<br>所以我们有隐藏层权重$\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$<br>和隐藏层偏置$\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$<br>以及输出层权重$\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$<br>和输出层偏置$\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$。<br>形式上，我们按如下方式计算单隐藏层多层感知机的输出<br>$\mathbf{O} \in \mathbb{R}^{n \times q}$：</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\end{aligned}</script><p>但如果仅仅如此的话相较于单层模型是没有益处的，因为本质上仍是仿射函数：</p><script type="math/tex; mode=display">\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.</script><p>所以需要非线性的<em>激活函数</em>（activation function）$\sigma$。<br>激活函数的输出（例如，$\sigma(\cdot)$）被称为<em>活性值</em>（activations）。<br>一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\\end{aligned}</script><p>由于$\mathbf{X}$中的每一行对应于小批量中的一个样本，<br>出于记号习惯的考量，<br>我们定义非线性函数$\sigma$也以按行的方式作用于其输入，<br>即一次计算一个样本。<br>但激活函数往往<em>可以按元素操作</em>，意味着在计算每一层的线性部分之后，我们可以计算每个活性值，<br>而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="1-ReLU函数"><a href="#1-ReLU函数" class="headerlink" title="1. ReLU函数"></a>1. ReLU函数</h3><p>ReLU全名修正激活单元（rectified linear unit,ReLU）其实现简单，表现良好。它要么让参数消失，要么让参数通过，缓解了以往神经网络的梯度消失问题。可以采用参数化ReLU（Parameterized ReLU，<em>pReLU</em>）使得负的参数也能够通过。</p><script type="math/tex; mode=display">\operatorname{ReLU}(x) = \max(x, 0)</script><script type="math/tex; mode=display">\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)</script><p><img src="Relu.png" alt="Relu"></p><h3 id="2-sigmoid函数"><a href="#2-sigmoid函数" class="headerlink" title="2. sigmoid函数"></a>2. sigmoid函数</h3><p><strong>对于一个定义域在$\mathbb{R}$中的输入，<em>sigmoid函数</em>将输入变换为区间(0, 1)上的输出</strong></p><script type="math/tex; mode=display">\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}</script><p><img src="sigmoid-1.png" alt="sigmoid-1"><br>其导数是</p><script type="math/tex; mode=display">\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right)</script><p><img src="sigmoid-2.png" alt="sigmoid-2"></p><h3 id="3-tanh函数"><a href="#3-tanh函数" class="headerlink" title="3. tanh函数"></a>3. tanh函数</h3><p>该函数类似于sigmoid,将输入压缩转换到区间(-1,1)上。</p><script type="math/tex; mode=display">\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}</script><p><img src="tanh.png" alt="tanh"></p><h2 id="多层感知机从零开始实现"><a href="#多层感知机从零开始实现" class="headerlink" title="多层感知机从零开始实现"></a>多层感知机从零开始实现</h2><p>代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = nn.Parameter(torch.randn(</span><br><span class="line">    num_inputs, num_hiddens, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="literal">True</span>))</span><br><span class="line">W2 = nn.Parameter(torch.randn(</span><br><span class="line">    num_hiddens, num_outputs, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"><span class="comment">#激活函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X, a)</span><br><span class="line"><span class="comment">#模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(X@W1 + b1)  <span class="comment"># 这里“@”代表矩阵乘法</span></span><br><span class="line">    <span class="keyword">return</span> (H@W2 + b2)</span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">num_epochs, lr = <span class="number">10</span>, <span class="number">0.1</span></span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></table></figure></p><h2 id="多层感知机的简洁实现"><a href="#多层感知机的简洁实现" class="headerlink" title="多层感知机的简洁实现"></a>多层感知机的简洁实现</h2><p>模型实现如下，与softmax可以进行对比<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line">net.apply(init_weights)<span class="comment">#;</span></span><br></pre></td></tr></table></figure><br>对比softmax代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></p><h1 id="模型选择、欠拟合和过拟合"><a href="#模型选择、欠拟合和过拟合" class="headerlink" title="模型选择、欠拟合和过拟合"></a>模型选择、欠拟合和过拟合</h1><p>机器学习的目标是发现某些模式（pattern），这些模式捕获到训练集潜在的总体规律。如何发现可以泛化的模式是机器学习的根本问题。<br/><br>将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。<br/><br><br/><br><em>训练误差</em>（training error）是指，<br>模型在训练数据集上计算得到的误差。<br><em>泛化误差</em>（generalization error）是指，<br>模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。但我们永远不能准确地计算出泛化误差。<br/></p><h2 id="1-统计学习理论"><a href="#1-统计学习理论" class="headerlink" title="1.统计学习理论"></a>1.统计学习理论</h2><p>训练数据和测试数据往往假设是从相同的分布中独立提取的。这通常被称为<em>独立同分布假设</em>，意味着对数据进行抽样的过程没有进行“记忆”。但是实际上这样的假设难以成立，如不同地区，不同时间的抽样往往会影响样本，从而违背独立同分布假设。</p><h2 id="2-模型复杂性"><a href="#2-模型复杂性" class="headerlink" title="2.模型复杂性"></a>2.模型复杂性</h2><p>统计学家认为，能够轻松解释任意事实的模型是复杂的，<br>而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。</p><p>事实上，具有更多参数的模型可能被认为更复杂，<br>参数有更大取值范围的模型可能更为复杂。<br>通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂，<br>而需要<em>早停</em>（early stopping）的模型（即较少训练迭代周期）就不那么复杂。</p><h2 id="3-验证集"><a href="#3-验证集" class="headerlink" title="3.验证集"></a>3.验证集</h2><p>在确定超参数之前，不希望用到测试集，即不能依靠测试数据来进行模型选择。<br>但也不能仅仅依靠训练数据来选择模型，因为这样便无法估计训练数据的泛化误差。</p><p>常见做法是将数据分成三份，除了训练和测试数据集之外，还需要增加<em>验证数据集</em>（validation dataset），也称<em>验证集</em>（validation set），虽然实际上验证集和测试集的边界十分模糊。</p><p>在训练数据稀缺时，存在一种解决方案是是采用$K$<em>折交叉验证</em>。<br>这里，原始训练数据被分成$K$个不重叠的子集。<br>然后执行$K$次模型训练和验证，每次在$K-1$个子集上进行训练，<br>并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。<br>最后，通过对$K$次实验的结果取平均来估计训练和验证误差。</p><h1 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h1><p>下面用多项式的例子来说明过拟合和模型复杂性的经典直觉。<br>给定由单个特征$x$和对应实数标签$y$组成的训练数据，<br>我们试图找到下面的$d$阶多项式来估计标签$y$。</p><script type="math/tex; mode=display">\hat{y}= \sum_{i=0}^d x^i w_i</script><p>这是一个简单的线性回归问题，而高阶多项式函数的复杂性则会比低阶多项式函数复杂得多，其<strong>参数较多</strong>，<strong>模型函数的选择范围较广</strong>。<br>因此在固定训练数据集的情况下，<br>高阶多项式函数相对于低阶多项式的训练误差应该始终更低。<br>事实上，当数据样本包含了$x$的不同值时， 函数阶数等于数据样本数量的多项式函数可以完美拟合训练集。<br><img src="模型复杂度对拟合程度的影响.png" alt="模型复杂度对拟合程度的影响"></p><p>同时数据集的大小会影响拟合程度。训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。给出更多的数据，我们可能会尝试拟合一个更复杂的模型。</p><p>对应的代码如下：<br>第一步生成测试数据，采用以下三阶多项式来生成训练和测试数据的标签</p><script type="math/tex; mode=display">y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{ where }\epsilon \sim \mathcal{N}(0, 0.1^2).</script><p>在优化的过程中，我们通常希望避免非常大的梯度值或损失值。<br>这就是我们将特征从$x^i$调整为$\frac{x^i}{i!}$的原因，<br>这样可以避免很大的$i$带来的特别大的指数值。<br>我们将为训练集和测试集各生成100个样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">max_degree = <span class="number">20</span>  <span class="comment"># 多项式的最大阶数</span></span><br><span class="line">n_train, n_test = <span class="number">100</span>, <span class="number">100</span>  <span class="comment"># 训练和测试数据集大小</span></span><br><span class="line">true_w = np.zeros(max_degree)  <span class="comment"># 分配大量的空间</span></span><br><span class="line">true_w[<span class="number">0</span>:<span class="number">4</span>] = np.array([<span class="number">5</span>, <span class="number">1.2</span>, -<span class="number">3.4</span>, <span class="number">5.6</span>])</span><br><span class="line"></span><br><span class="line">features = np.random.normal(size=(n_train + n_test, <span class="number">1</span>))<span class="comment">#偏置1</span></span><br><span class="line">np.random.shuffle(features)</span><br><span class="line">poly_features = np.power(features, np.arange(max_degree).reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_degree):</span><br><span class="line">    poly_features[:, i] /= math.gamma(i + <span class="number">1</span>)  <span class="comment"># gamma(n)=(n-1)!</span></span><br><span class="line"><span class="comment"># labels的维度:(n_train+n_test,)</span></span><br><span class="line">labels = np.dot(poly_features, true_w)</span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>, size=labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy ndarray转换为tensor</span></span><br><span class="line">true_w, features, poly_features, labels = [torch.tensor(x, dtype=</span><br><span class="line">    torch.float32) <span class="keyword">for</span> x <span class="keyword">in</span> [true_w, features, poly_features, labels]]</span><br></pre></td></tr></table></figure><p>下面对模型进行训练和测试<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#评估损失</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 损失的总和,样本数量</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#定义训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features, test_features, train_labels, test_labels,</span></span><br><span class="line"><span class="params">          num_epochs=<span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 不设置偏置，因为我们已经在多项式中实现了它</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, <span class="number">1</span>, bias=<span class="literal">False</span>))</span><br><span class="line">    batch_size = <span class="built_in">min</span>(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                                batch_size)</span><br><span class="line">    test_iter = d2l.load_array((test_features, test_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                               batch_size, is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">1e-3</span>, <span class="number">1e2</span>],</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>, net[<span class="number">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure><br>最后是拟合情况：</p><ol><li>三阶多项式函数拟合<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">4</span>], poly_features[n_train:, :<span class="number">4</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure><img src="三阶多项式.png" alt="三阶多项式"></li><li>线性函数拟合（欠拟合）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前2个维度，即1和x</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure><img src="线性函数.png" alt="线性函数"></li><li>高阶多项式函数拟合（过拟合）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选取所有维度</span></span><br><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br></pre></td></tr></table></figure><img src="高阶多项式函数.png" alt="高阶多项式函数"></li></ol><h1 id="权重衰减（-L-2-正则化）"><a href="#权重衰减（-L-2-正则化）" class="headerlink" title="权重衰减（$L_2$正则化）"></a>权重衰减（$L_2$正则化）</h1><p>假设我们已经拥有了尽可能多的高质量数据，就可以将重点放在正则化技术上来防止过拟合。<br>限制特征的数量是环节过拟合的一种常用技术，简单地丢弃特征对这项工作而言太过于生硬，需要更为细粒度的工具来调整复杂度。</p><p>一种简单的方法是通过线性函数<br>$f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$<br>中的权重向量的某个范数来度量其复杂性，<br>例如$| \mathbf{w} |^2$。<br>要保证权重向量比较小，<br>最常用方法是将其范数作为惩罚项加到最小化损失的问题中。<br>将原来的训练目标<em>最小化训练标签上的预测损失</em>，<br>调整为<em>最小化预测损失和惩罚项之和</em>。<br>现在，如果我们的权重向量增长的太大，<br>我们的学习算法可能会更集中于最小化权重范数$| \mathbf{w} |^2$。</p><p>举之前的线性函数的例子，损失如下：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2</script><p>$\mathbf{x}^{(i)}$是样本$i$的特征，<br>$y^{(i)}$是样本$i$的标签，<br>$(\mathbf{w}, b)$是权重和偏置参数。<br>通过<em>正则化常数</em>$\lambda$来平衡$| \mathbf{w} |^2$描述的损失，使用验证数据拟合：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2,</script><p>使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。<br>这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。<br>在实践中，这可能使它们对单个变量中的观测误差更为稳定。<br>相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上，<br>而将其他权重清除为零。<br>这称为<em>特征选择</em>（feature selection），这可能是其他场景下需要的。</p><p>$L_2$正则化回归的小批量随机梯度下降更新如下式，一般网络输出层的偏置项不会被正则化：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).\end{aligned}</script><p>接下来用一个简单例子来展现权重衰减：</p><p>首先是生成数据,问题的位数增加到$d = 200$，<br>并使用一个只包含20个样本的小训练集。</p><script type="math/tex; mode=display">y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where }\epsilon \sim \mathcal{N}(0, 0.01^2)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_train, n_test, num_inputs, batch_size = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">5</span></span><br><span class="line">true_w, true_b = torch.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line">train_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class="line">train_iter = d2l.load_array(train_data, batch_size)</span><br><span class="line">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class="line">test_iter = d2l.load_array(test_data, batch_size, is_train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>初始化模型参数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>():</span><br><span class="line">    w = torch.normal(<span class="number">0</span>, <span class="number">1</span>, size=(num_inputs, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure><br>定义$L_2$范数乘法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">l2_penalty</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(w.<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">2</span></span><br></pre></td></tr></table></figure><br>训练代码如下，其将模型拟合训练数据集，并在测试数据集上进行评估：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">lambd</span>):</span><br><span class="line">    w, b = init_params()</span><br><span class="line">    net, loss = <span class="keyword">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># 增加了L2范数惩罚项，</span></span><br><span class="line">            <span class="comment"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span></span><br><span class="line">            l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item())</span><br></pre></td></tr></table></figure><br>使用权重衰减进行拟合后效果更好。</p><p>可以在实例化优化器时直接通过weight_decay指定weight decay超参数。<br>默认情况下，PyTorch同时衰减权重和偏移。<br>这里我们只为权重设置了weight_decay，所以偏置参数不会衰减。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    <span class="comment"># 偏置参数没有衰减</span></span><br><span class="line">    trainer = torch.optim.SGD([</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].weight,<span class="string">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].bias&#125;], lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>,</span><br><span class="line">                         (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                          d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数：&#x27;</span>, net[<span class="number">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure></p><h1 id="暂退法（Dropout）"><a href="#暂退法（Dropout）" class="headerlink" title="暂退法（Dropout）"></a>暂退法（Dropout）</h1><p>面对更多特征而样本不足时，线性模型往往会过拟合。<br>给出更多样本而不是特征时，线性模型不会过拟合。<br>但线性模型没有考虑到特征之间的相互作用。<br>对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p><p>与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。<br>例如，神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件，但单独出现则不表示垃圾邮件。<br>而且即使我们有比特征多得多的样本，深度神经网络也有可能过拟合。</p><p>经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。<br>简单性以<strong>较小维度</strong>的形式展现，在权重衰减的讨论中可以看到参数的范数也代表了一种有用的简单性度量<br>简单性的另一个角度是<strong>平滑性</strong>，即函数不应该对其输入的微小变化敏感。基于此，学者提出了一个想法：在前向传播过程中，计算每一内部层的同时注入噪声，这就是我们要提到的<em>暂退法</em>（dropout）。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。</p><p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。<br>换言之，每个中间活性值$h$以<em>暂退概率</em>$p$由随机变量$h’$替换，如下所示：</p><script type="math/tex; mode=display">\begin{aligned}h' =\begin{cases}    0 & \text{ 概率为 } p \\    \frac{h}{1-p} & \text{ 其他情况}\end{cases}\end{aligned}</script><p>根据此模型的设计，其期望值保持不变，即$E[h’] = h$。对之前的多层感知机运用效果如下图：<br><img src="暂退法.png" alt="暂退法"></p><p>代码流程如下：</p><p>实现dropout_layer函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X, dropout</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= dropout &lt;= <span class="number">1</span></span><br><span class="line">    <span class="comment"># 在本情况中，所有元素都被丢弃</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="comment"># 在本情况中，所有元素都被保留</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    mask = (torch.rand(X.shape) &gt; dropout).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> mask * X / (<span class="number">1.0</span> - dropout)</span><br></pre></td></tr></table></figure><br>定义模型参数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br></pre></td></tr></table></figure><br>定义模型，可以为每一层分别设置暂退概率，常见的技巧是在靠近输入层的地方设置较低的暂退概率，暂退法只在训练期间有效：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">dropout1, dropout2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span></span><br><span class="line"><span class="params">                 is_training = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.training = is_training</span><br><span class="line">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class="line">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class="line">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        H1 = self.relu(self.lin1(X.reshape((-<span class="number">1</span>, self.num_inputs))))</span><br><span class="line">        <span class="comment"># 只有在训练模型时才使用dropout</span></span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">            H1 = dropout_layer(H1, dropout1)</span><br><span class="line">        H2 = self.relu(self.lin2(H1))</span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">            H2 = dropout_layer(H2, dropout2)</span><br><span class="line">        out = self.lin3(H2)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></table></figure><br>训练与测试类似前文。而简洁实现中，只需在每个全连接层之后添加一个Dropout层， 将暂退概率作为唯一的参数传递给它的构造函数。在训练时，Dropout层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。在测试时，Dropout层仅传递数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment">#net.apply(init_weights);</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure></p><h1 id="前向传播、反向传播和计算图"><a href="#前向传播、反向传播和计算图" class="headerlink" title="前向传播、反向传播和计算图"></a>前向传播、反向传播和计算图</h1><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。</p><script type="math/tex; mode=display">\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x}</script><p>其中$\mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$<br>是隐藏层的权重参数。<br>将中间变量$\mathbf{z}\in \mathbb{R}^h$通过激活函数$\phi$后，<br>我们得到长度为$h$的隐藏激活向量：</p><script type="math/tex; mode=display">\mathbf{h}= \phi (\mathbf{z})</script><p>隐藏变量$\mathbf{h}$也是一个中间变量。<br>假设输出层的参数只有权重$\mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$，<br>我们可以得到输出层变量，它是一个长度为$q$的向量：</p><script type="math/tex; mode=display">\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}.</script><p>根据$L_2$正则化的定义，给定超参数$\lambda$，正则化项为：</p><script type="math/tex; mode=display">s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2\right)</script><p>其中矩阵的Frobenius范数是将矩阵展平为向量后应用的$L_2$范数。<br>最后，模型在给定数据样本上的正则化损失为：</p><script type="math/tex; mode=display">J = L + s.</script><p>我们将$J$称为<em>目标函数</em>（objective function）。前向传播的计算图如下：<br> 其中正方形表示变量，圆圈表示操作符。<br> 左下角表示输入，右上角表示输出。<br> 注意显示数据流的箭头方向主要是向右和向上的。<br><img src="前向传播计算图.png" alt="前向传播计算图"></p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="　反向传播"></a>　反向传播</h2><p><em>反向传播</em>（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。<br>简言之，该方法根据微积分中的<em>链式规则</em>，按相反的顺序从输出层到输入层遍历网络。<br>该算法存储了计算某些参数梯度时所需的<strong>任何中间变量</strong>（偏导数）。</p><p>假设我们有函数$\mathsf{Y}=f(\mathsf{X})$和$\mathsf{Z}=g(\mathsf{Y})$，<br>其中输入和输出$\mathsf{X}, \mathsf{Y}, \mathsf{Z}$是任意形状的张量。<br>利用链式法则，我们可以计算$\mathsf{Z}$关于$\mathsf{X}$的导数</p><script type="math/tex; mode=display">\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right)</script><p>在这里，我们使用$\text{prod}$运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。<br>对于向量，这很简单，它只是矩阵-矩阵乘法。<br>对于高维张量，我们使用适当的对应项。<br>运算符$\text{prod}$指代了所有的这些符号。<br>回想一下，在上文计算图中的单隐藏层简单网络的参数是<br>$\mathbf{W}^{(1)}$和$\mathbf{W}^{(2)}$。<br>反向传播的目的是计算梯度$\partial J/\partial \mathbf{W}^{(1)}$和<br>$\partial J/\partial \mathbf{W}^{(2)}$。<br>为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。<br>计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。具体流程查看<a href="https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html">反向传播</a></p><h2 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h2><p>在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。</p><p>以上述简单网络为例：一方面，在前向传播期间计算正则项取决于模型参数$\mathbf{W}^{(1)}$和$\mathbf{W}^{(2)}$的当前值。<br>它们是由优化算法根据最近迭代的反向传播给出的。<br>另一方面，反向传播期间参数 :eqref:<code>eq_backprop-J-h</code>的梯度计算，<br>取决于由前向传播给出的隐藏变量$\mathbf{h}$的当前值。</p><p>因此，在训练神经网络时，在初始化模型参数后，<br>我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。<br>注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。<br>带来的影响之一是我们需要保留中间值，直到反向传播完成。<br>这也是训练比单纯的预测需要更多的内存（显存）的原因之一。<br>此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。<br>因此，使用更大的批量来训练更深层次的网络更容易导致<em>内存不足</em>（out of memory）错误。</p><h1 id="数值稳定性和模型初始化"><a href="#数值稳定性和模型初始化" class="headerlink" title="数值稳定性和模型初始化"></a>数值稳定性和模型初始化</h1><p>若选取的初始参数不当，容易威胁到优化算法的稳定性：要么是要么是梯度爆炸（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛； 要么是梯度消失（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p><p>sigmoid函数容易发生梯度消失的问题，可采用ReLU函数</p><p>神经网络中存在参数化固有对成型的问题，可以采用暂退法正则化打破这样的对称性</p><p>参数初始化可以考虑Xavier初始化，通常，Xavier初始化从均值为零，方差<br>$\sigma^2 = \frac{2}{n_\mathrm{in} + n_\mathrm{out}}$<br>的高斯分布中采样权重。<br>我们也可以将其改为选择从均匀分布中抽取权重时的方差。<br>注意均匀分布$U(-a, a)$的方差为$\frac{a^2}{3}$。<br>将$\frac{a^2}{3}$代入到$\sigma^2$的条件中，将得到初始化值域：</p><script type="math/tex; mode=display">U\left(-\sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}, \sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}\right)</script><p>虽然有“不存在非线性”的假设，但实践证明是有效的。</p><h1 id="环境与分布偏移"><a href="#环境与分布偏移" class="headerlink" title="环境与分布偏移"></a>环境与分布偏移</h1><p>有时，根据测试集的精度衡量，模型表现得非常出色。<br>但是当数据分布突然改变时，模型在部署中会出现灾难性的失败。<br>更隐蔽的是，有时模型的部署本身就是扰乱数据分布的催化剂。<br>机器学习的许多应用中都存在类似的问题：<br>通过将基于模型的决策引入环境，我们可能会破坏模型。</p><h2 id="偏移类型"><a href="#偏移类型" class="headerlink" title="偏移类型"></a>偏移类型</h2><p>偏移的类型有如下几种：</p><ol><li>协变量偏移：虽然输入的分布可能随时间而改变，<br>但标签函数（即条件分布$P(y \mid \mathbf{x})$）没有改变，<br>在我们认为$\mathbf{x}$导致$y$的情况下，协变量偏移是一种自然假设。</li><li>标签偏移：假设标签边缘概率$P(y)$可以改变，<br>但是类别条件分布$P(\mathbf{x} \mid y)$在不同的领域之间保持不变。<br>当我们认为$y$导致$\mathbf{x}$时，标签偏移是一个合理的假设。</li><li>概念偏移：类别会随着不同时间的用法而发生变化。<br>精神疾病的诊断标准、所谓的时髦、以及工作头衔等等，都是概念偏移的日常映射。</li></ol><h2 id="分布偏移纠正"><a href="#分布偏移纠正" class="headerlink" title="分布偏移纠正"></a>分布偏移纠正</h2><p>_ramain to be understood_</p>]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读方法</title>
      <link href="/2023/12/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95/"/>
      <url>/2023/12/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读书日记</title>
      <link href="/2023/12/04/%E8%AF%BB%E4%B9%A6%E6%97%A5%E8%AE%B0/"/>
      <url>/2023/12/04/%E8%AF%BB%E4%B9%A6%E6%97%A5%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="e588c51d4195b9af20b262e0649af27c1f55708f40191b9c953ac5c4e94a44f4">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 日常摸鱼 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随便写写 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>绘画课学习笔记</title>
      <link href="/2023/12/04/%E7%BB%98%E7%94%BB%E8%AF%BE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/12/04/%E7%BB%98%E7%94%BB%E8%AF%BE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="b7e77d36dcade690e2dec41f442e2711c73be13516ccc4cb87220589d290906d">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 日常摸鱼 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI学习——基础知识</title>
      <link href="/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
      <url>/2023/12/01/AI%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>AI学习记录1<br> <span id="more"></span></p><h1 id="part1-数学基础"><a href="#part1-数学基础" class="headerlink" title="part1:数学基础"></a>part1:数学基础</h1><h2 id="范数："><a href="#范数：" class="headerlink" title="范数："></a>范数：</h2><p>向量范数是将向量映射到标量的函数<em>f</em>，以下代码表示一个<em>L2</em>范数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure><h2 id="梯度："><a href="#梯度：" class="headerlink" title="梯度："></a>梯度：</h2><p>一个多元函数对其所有变量的偏导数。一个函数<em>f(x)</em>的输入是n维的向量，输出是一个标量。则函数<em>f(x)</em>相对于<em>x</em>的梯度是一个包含n个偏导数的向量，由于深度学习中，多元函数往往是复合的，所以一般使用链式法则来对复合函数求微分<br/><br>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。以下代码是一个简单的例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4.0</span>)</span><br><span class="line"><span class="comment">#创建梯度，标量函数关于向量x的梯度是向量，且具有相同形状</span></span><br><span class="line">x.requires_grad_(<span class="literal">True</span>)  <span class="comment"># 等价于x=torch.arange(4.0,requires_grad=True)</span></span><br><span class="line">y = <span class="number">2</span> * torch.dot(x, x) <span class="comment">#计算x和x的点积</span></span><br><span class="line">y.backward()<span class="comment">#4x</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()<span class="comment">#1</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><br>书上的梯度相关的数学的结论如下图：<br/><br><img src="梯度常用结论.png" alt="梯度结论"></p><h3 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h3><p><a href="https://zhuanlan.zhihu.com/p/273729929">参考链接-知乎</a><br/><br>通过该链接，学习到矩阵求导的基本规则如下图：<img src="矩阵布局.png" alt="矩阵求导图"><br><a href="https://zhuanlan.zhihu.com/p/262751195?utm_oi=628596077562040320&amp;utm_id=0">矩阵求导全解-知乎</a><br/><br>该链接详细介绍了矩阵求导的规范：<br/></p><h4 id="矩阵求导布局（视为规定）"><a href="#矩阵求导布局（视为规定）" class="headerlink" title="矩阵求导布局（视为规定）"></a>矩阵求导布局（视为规定）</h4><p>分子布局：导数的维度以分子为主<br/><br>分母布局：倒数的维度以分母为主<br/><br>在机器学习的算法推导里，通常遵循以下布局的规范：<br/></p><ul><li>如果向量或者矩阵对标量求导，则以分子布局为准。</li><li>如果标量对向量或者矩阵求导，则以分母布局为准。</li><li>对于向量对对向量求导，有些分歧，一般以分子布局的雅克比矩阵为主。<br><img src="布局图.png" alt="布局图"></li></ul><h4 id="标量对矩阵求导"><a href="#标量对矩阵求导" class="headerlink" title="标量对矩阵求导"></a>标量对矩阵求导</h4><p>算法如下图：<br/><br><img src="标量对矩阵求导算法.png" alt="标量对矩阵求导算法"></p><h4 id="矩阵对矩阵求导"><a href="#矩阵对矩阵求导" class="headerlink" title="矩阵对矩阵求导"></a>矩阵对矩阵求导</h4><p>算法如下图：<br/><br><img src="向量对向量求导算法.png" alt="向量对向量求导算法"></p><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p><a href="https://zhuanlan.zhihu.com/p/261375491">参考链接-知乎</a><br/><br>$x \leftarrow x^2 - \eta\frac{df(x)}{dx}$<br/><br>梯度作为一个向量，代表一个方向，朝着梯度的反方向变化可以使得函数值变小，$\eta$代表学习率</p><h3 id="反向传播算法-Back-propagation"><a href="#反向传播算法-Back-propagation" class="headerlink" title="反向传播算法(Back propagation)"></a>反向传播算法(Back propagation)</h3><p><a href="https://zhuanlan.zhihu.com/p/71892752">参考链接-知乎</a><br/><br>反向传播仅指用于计算梯度的算法,其具体流程如下图所示，能够从最后一层的梯度算出前面层数的梯度<br><img src="反向传播.png" alt="反向传播"><br>深度学习框架通过自动计算导数，即自动微分来加快求导。实践中，根据设计好的模型，系统会构建一个计算图，来跟踪计算是<strong>哪些数据通过哪些操作组合起来来产生输出</strong>。自动微分使得系统能够随后反向传播梯度，即意味着跟踪整个计算图，填充关于每个参数的偏导数</p><h2 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h2><ol><li>联合概率(joint probability): $P(A=a,B=b)$</li><li>条件概率(conditional probability): $P(A=a|B=b)$</li><li>贝叶斯定理(multiplication rule): $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$</li><li>边际化: $P(B) = \sum_{A} P(A,B)$，一般和贝叶斯定理结合使用</li><li>独立性：$P(A|B)=\frac{P(A,B)}{P(B)}=P(A)$</li></ol><h1 id="part2-线性神经网络"><a href="#part2-线性神经网络" class="headerlink" title="part2:线性神经网络"></a>part2:线性神经网络</h1><h2 id="线性回归与线性模型"><a href="#线性回归与线性模型" class="headerlink" title="线性回归与线性模型"></a>线性回归与线性模型</h2><p>线性回归基于几个简单的假设：假设自变量<em>x</em>和因变量<em>y</em>之间的关系是线性的， 即<em>y</em>可以表示为<em>x</em>中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。<br/><br>数据集称为训练数据集（training data set） 或训练集（training set）。每行数据（比如一次房屋交易相对应的数据）称为样本（sample），也可以称为数据点（data point）或数据样本（data instance）。试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。<br/><br>基本的模型如下：<br/></p><script type="math/tex; mode=display">price = w_{area} * area + w_{age} * age + b</script><p>其中<em>b</em>是输入特征的仿射变换，其特点是通过加权和对特征进行线性变换。并通过偏置项进行平移（translation）。<br/><br>预测模型可用点积表示：</p><script type="math/tex; mode=display">\hat{y}=\boldsymbol{w}^T\boldsymbol{x}+b</script><p>或用矩阵向量积表示：</p><script type="math/tex; mode=display">\hat{\boldsymbol{y}}=\boldsymbol{X}\boldsymbol{w}+b</script><p>其中 $\boldsymbol{X}\in{\mathbb{R}^{n*d}}$ 其每一行是一个样本，一共有n个预测结果。该模型的模型参数为$\boldsymbol{w}$和$b$,为找到最佳的参数，还需要两个工具：<br/></p><ol><li>一种模型质量的度量方式</li><li>一种能够更新模型以提高模型预测质量的方法</li></ol><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>使用损失函数量化目标的<strong><em>实际值</em></strong>与<strong><em>预测值</em></strong>之间的差距，通常选择非负数作为损失，数值越小越好，完美预测时损失为0。线性回归往往用平方误差作为损失函数：</p><script type="math/tex; mode=display">\ell^{(i)}(\boldsymbol{w},b) = \frac{1}{2} \left( \hat{y}^{(i)} - y^{(i)} \right)^2</script><p>为了度量模型在整个数据集上的预测质量，需要计算在训练集<em>n</em>个样本上的损失均值:</p><script type="math/tex; mode=display">L(\boldsymbol{w},b)=\frac{1}{n}\sum_{i=1}^n\ell^{(i)}(\boldsymbol{w},b)=\frac{1}{n}\sum_{i=1}^n(\boldsymbol{w}^T\boldsymbol{x}^{(i)}+b-y^{(i)})</script><p>训练模型的目的是找到一组参数，这组参数能最小化在所有训练样本上的总损失</p><h2 id="解析解"><a href="#解析解" class="headerlink" title="解析解"></a>解析解</h2><p><a href="https://blog.csdn.net/a755199443/article/details/104198033">参考链接-CSDN</a><br/><br>线性回归的解可用一个公式简单的表示，将<strong><em>b</em></strong>合并到<strong><em>w</em></strong>中，最小化$||\boldsymbol{y}-\boldsymbol{X}\boldsymbol{w}||^2$，得到解析解</p><script type="math/tex; mode=display">w^* = (X'X)^{-1}X'y</script><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>在无法得到解析解的情况下，我们也可以有效地训练模型。<strong><em>梯度下降</em></strong>（gradient descent）的通过不断地在损失函数递减的方向上更新参数来降低误差，但是每次更新参数都必须遍历整个数据集，为提高速度，通常在每次需要计算更新的时候随机抽取一小批样本。这种方法称为<strong><em>小批量随机梯度下降</em></strong>（minibatch stochastic gradient descent）。<br/></p><p>每次迭代抽取一个小批量$\mathcal{B}$，其由固定数量的训练样本组成。然后计算小批量的平均损失关于模型参数的导数。最后将梯度乘以一个预先确定的参数$\eta$，并从当前参数值减掉。</p><p>算法的步骤如下：（1）初始化模型参数的值（随机初始化）（2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。</p><script type="math/tex; mode=display">(\mathbf{w}, b) \leftarrow(\mathbf{w}, b)-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w}, b)} l^{(i)}(\mathbf{w}, b)</script><p>其中$|\mathcal{B}|$表示小批量中的样本数，即批量大小，而$\eta$表示学习率。这两个都是手动预先自定的，这些可以调整但不再训练过程中更新的参数成为<em>超参数</em>（hyperparameter）。<em>调参</em>（hyperparameter tuning）是选择超参数的过程。</p><h2 id="正态分布与平方损失"><a href="#正态分布与平方损失" class="headerlink" title="正态分布与平方损失"></a>正态分布与平方损失</h2><p>正态分布概率密度函数如下：</p><script type="math/tex; mode=display">p(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right)</script><p>均方误差损失函数可以用于线性回归的一个原因是：假设观测中包含了噪声且噪声服从正态分布，即</p><script type="math/tex; mode=display">y=\mathbf{w}^{\top} \mathbf{x}+b+\epsilon</script><p>其中，$\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)$.<br>现在可以写出通过给定的<strong>x</strong>观测到特定<em>y</em>的似然函数：</p><script type="math/tex; mode=display">P(y \mid \mathbf{x})=\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{1}{2 \sigma^2}\left(y-\mathbf{w}^{\top} \mathbf{x}-b\right)^2\right)</script><p>根据<a href="https://zhuanlan.zhihu.com/p/26614750">极大似然估计法</a>，参数<strong>w</strong>和<strong>b</strong>的最优值是使得整个数据集似然最大的值:</p><script type="math/tex; mode=display">P(\mathbf{y} \mid \mathbf{X})=\prod_{i=1}^n p\left(y^{(i)} \mid \mathbf{x}^{(i)}\right)</script><p>用最小化负对数进行进一步推导：</p><script type="math/tex; mode=display">-\log P(\mathbf{y} \mid \mathbf{X})=\sum_{i=1}^n \frac{1}{2} \log \left(2 \pi \sigma^2\right)+\frac{1}{2 \sigma^2}\left(y^{(i)}-\mathbf{w}^{\top} \mathbf{x}^{(i)}-b\right)^2</script><p>这样就使得最小化均方误差等价于对线性模型的极大似然估计了</p><h2 id="线性回归从零开始实现"><a href="#线性回归从零开始实现" class="headerlink" title="线性回归从零开始实现"></a>线性回归从零开始实现</h2><p>在每次迭代中，读取小批量训练，通过模型来获得一组预测。计算完损失后开始反向传播，存储每个参数的梯度。最后调用优化算法来更新模型参数。训练的轮数epoch和学习率lr都是超参数，从0开始实现线性回归的算法代码如下，具体参看书的3.2节<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##3.2.1生成数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    <span class="comment">#normal生成了一个张量，包含从均值为0，标准差为1的正态分布中随机抽取的随机数，num_examples表示生成张量的行数,len(w)表示生成张量的列数</span></span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    <span class="comment">#matmul用于矩阵乘法运算</span></span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape) <span class="comment">#加上高斯噪音，标准差是0.01</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"><span class="comment">#features每一行是一个二维数据样本，labels每一行包含一个标签值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;features:&#x27;</span>, features[<span class="number">0</span>],<span class="string">&#x27;\nlabel:&#x27;</span>, labels[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">d2l.plt.scatter(features[:, (<span class="number">1</span>)].detach().numpy(), labels.detach().numpy(), <span class="number">1</span>) <span class="comment">#绘制散点图</span></span><br><span class="line"><span class="comment">#d2l.plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.2读取数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    <span class="comment"># 这些样本是随机读取的，没有特定的顺序</span></span><br><span class="line">    random.shuffle(indices)<span class="comment">#打乱的数据顺序</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):<span class="comment">#batch_size为每个小批量的大小</span></span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])<span class="comment">#取出一组batch_size的索引值</span></span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class="line">        <span class="comment">#yield将一个函数转换为生成器函数，在调用生成器函数的过程中，每次遇到yield语句时，函数会暂停并保存当前所有的运行信息（保留局部变量），返回yield的值</span></span><br><span class="line">        </span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">##3.2.3初始化模型参数</span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.4定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br><span class="line">    <span class="comment">#当我们用一个向量加一个标量时，标量会被加到向量的每个分量上</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.5定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.6定义优化算法</span></span><br><span class="line"><span class="comment">#小批量随机梯度下降更新</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():<span class="comment">#禁用梯度计算，所有的require_grad被自动设置为false</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size <span class="comment">#lr是学习率,用batch_size批量大小来规范化步长</span></span><br><span class="line">            param.grad.zero_()<span class="comment">#清除梯度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##3.2.7训练</span></span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.03</span> <span class="comment">#学习率</span></span><br><span class="line">num_epochs = <span class="number">3</span> <span class="comment">#训练轮数</span></span><br><span class="line">net = linreg <span class="comment">#线性回归模型</span></span><br><span class="line">loss = squared_loss <span class="comment">#损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):<span class="comment">#data_iter遍历数据集，将训练数据集中的所有样本都使用一次（1000/10)</span></span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        <span class="comment"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()<span class="comment">#调用反向传播算法进行计算</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p><h2 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h2><p>对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量net，它是一个Sequential类的实例。 Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。这样的一个层称为全连接层，因为<strong>它的每一个输入都通过矩阵-向量乘法得到它的每个输出</strong><br/><br>通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。 我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。 当我们需要更复杂的模型时，高级API的优势将大大增加。 当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。下面是对比<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成数据集</span></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"><span class="comment">#调用框架中现有的API来读取数据，将特征和标签作为API的参数床底，通过数据迭代器指定batch_size</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line"><span class="comment">#定义模型，利用Sequential类直接完成</span></span><br><span class="line">  <span class="comment"># nn是神经网络的缩写</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#使用optim模块中的小批量随机梯度下降算法初始化SGD实例，指定要优化的参数（net.parameters()）以及优化算法所需的超参数字典</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        trainer.zero_grad()<span class="comment">#清零梯度</span></span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span> <span class="comment">#学习率</span></span><br><span class="line">num_epochs = <span class="number">3</span> <span class="comment">#训练轮数</span></span><br><span class="line">net = linreg <span class="comment">#线性回归模型</span></span><br><span class="line">loss = squared_loss <span class="comment">#损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):<span class="comment">#data_iter遍历数据集，将训练数据集中的所有样本都使用一次（1000/10)</span></span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        <span class="comment"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，</span></span><br><span class="line">        <span class="comment"># 并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>回归用于预测多少的问题，分类关心“哪一个”的问题。<br/><br>通过<strong>独热编码</strong>（one-hot encoding）可以进行分类数据的表示,softmax回归解决的就是数据份额里的问题。为了解决线性模型的分类问题，需要一个多个输出的模型和与输出一样多的<strong>仿射函数</strong>（affine function）。softmax回归也是一个单层的神经网络结构，如下图所示：<br/><br><img src="softmax回归.png" alt="softmax回归"><br>对于任何具有<em>d</em>个输入和 <em>q</em>个输出的全连接层，参数开销是$O(dq)$，这个开销可以减少到$O(dq/n)$，<em>n</em>可由我们灵活指定。<br/><br>我们希望模型的输出$\hat{y}_{j}$可以视为属于类$j$的概率,所以必须保证在任何数据上的输出都是非负的且总和为1。这时候就需要使用softmax函数如下式：<br/><br>$\begin{aligned} &amp; o_1=x_1 w_{11}+x_2 w_{12}+x_3 w_{13}+x_4 w_{14}+b_1 \\ &amp; o_2=x_1 w_{21}+x_2 w_{22}+x_3 w_{23}+x_4 w_{24}+b_2 \\ &amp; o_3=x_1 w_{31}+x_2 w_{32}+x_3 w_{33}+x_4 w_{34}+b_3\end{aligned}$<br/><br>$\hat{\mathbf{y}}=\operatorname{softmax}(\mathbf{o})$<br>其中：$\hat{y}_j=\frac{\exp \left(o_j\right)}{\sum_k \exp \left(o_k\right)}$<br/><br>softmax运算不会改变未规范化的预测<strong>o</strong>之间的大小次序，只会确定分配给每个类别的概率，尽管它是一个非线性函数，但其回归的输出仍然由输入特征的仿射变换决定，所以其仍是一个<strong>线性模型</strong>。<br/><br><br/><br>接下来需要一个损失函数，同样使用上文提到过的的<a href="https://zhuanlan.zhihu.com/p/26614750">极大似然估计</a>。极大似然估计的原理就是：<strong>利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值</strong>。<br/><br>极大似然估计中的采样必须满足独立同分布。对于一些情况，样本太多，无法得出分布的参数值，可以采样小样本后，利用极大似然估计获取假设中分布的参数值。<br/><br>假设整个数据集有{$\mathbf{X}$,$\mathbf{Y}$}具有n个样本，其中索引为<em>i</em>的样本由特征向量$\mathbf{x}^{(i)}$和独热标签向量$\mathbf{y}^{(i)}$组成。</p><script type="math/tex; mode=display">P(\mathbf{Y} \mid \mathbf{X})=\prod_{i=1}^n P\left(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}\right)</script><p>根据极大似然估计，可以最大化$P(Y|X)$,相当于最小化负对数似然：</p><script type="math/tex; mode=display">-\log P(\mathbf{Y} \mid \mathbf{X})=\sum_{i=1}^n-\log P\left(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}\right)=\sum_{i=1}^n l\left(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}\right)</script><p>其中对于任何标签$y$和模型预测$\hat{y}$，损失函数为：</p><script type="math/tex; mode=display">l(\mathbf{y}, \hat{\mathbf{y}})=-\sum_{j=1}^q y_j \log \hat{y}_j</script><p>这种损失函数成为交叉熵损失（cross-entropy loss）,它是一个两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数<br/><br><br/><br>对softmax的损失函数求导过程如下：<br/></p><script type="math/tex; mode=display">\begin{aligned} l(\mathbf{y}, \hat{\mathbf{y}}) & =-\sum_{j=1}^q y_j \log \frac{\exp \left(o_j\right)}{\sum_{k=1}^q \exp \left(o_k\right)} \\ & =\sum_{j=1}^q y_j \log \sum_{k=1}^q \exp \left(o_k\right)-\sum_{j=1}^q y_j o_j \\ & =\log \sum_{k=1}^q \exp \left(o_k\right)-\sum_{j=1}^q y_j o_j \end{aligned}</script><script type="math/tex; mode=display">\partial_{o_j}l(\mathbf{y},\hat{\mathbf{y}})=\frac{exp(o_j)}{\sum_{k=1}^{q}exp(o_k)}-y_j=softmax(\mathbf{(o)_j}-y_j</script><h2 id="softmax回归从零开始实现"><a href="#softmax回归从零开始实现" class="headerlink" title="softmax回归从零开始实现"></a>softmax回归从零开始实现</h2><p>此处将$28<em>28$像素的样本图像平展为784的向量（不利用图像空间结构的特征），由于数据集有10个类别，所以权重将构成$784</em>10$的矩阵，偏置为$1*10$的行向量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>接下来定义softmax的操作，利用sum的keepdim参数，指定保持在原始张量的轴数（参数1表示行求和）</p><script type="math/tex; mode=display">\mathrm{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k \exp(\mathbf{X}_{ik})}.</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)<span class="comment">#行求和且保持为一个列向量</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制，将partition进行了扩展</span></span><br></pre></td></tr></table></figure><p>接下来定义softmax回归模型，利用reshape将图像展平为向量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape((-<span class="number">1</span>, W.shape[<span class="number">0</span>])), W) + b)</span><br></pre></td></tr></table></figure><br>然后定义损失函数（交叉熵）：</p><script type="math/tex; mode=display">l(\mathbf{y}, \hat{\mathbf{y}})=-\sum_{j=1}^q y_j \log \hat{y}_j</script><p>y_hat包含两个样本在三个类别上的预测概率，标签则是y,则使用PyTorch张量的花式索引写法，将y作为y_hat中概率的索引有：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line">y_hat[[<span class="number">0</span>, <span class="number">1</span>], y]</span><br><span class="line"></span><br><span class="line"><span class="comment">#[out]:tensor([0.1000, 0.5000])</span></span><br></pre></td></tr></table></figure><br>实现交叉熵函数如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"></span><br><span class="line">cross_entropy(y_hat, y)</span><br></pre></td></tr></table></figure><br>分类精度是正确预测数和总预测数之比，直接优化精度或许会很困难，但精度是我们最关心的性能度量标准，使用如下函数计算得到预测正确的数量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(y_hat)</span></span><br><span class="line">    <span class="comment">#print(y_hat.type(y.dtype))</span></span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><br>同样，对于任意数据迭代器data_iter可访问的数据集，我们可以评估在任意模型net的精度。Accumulator创建了两个变量用于存储正确预测数量和预测总数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><br>训练模型的一个迭代周期的代码逻辑如下，updater是更新模型参数的常用函数，它接受批量大小作为参数。 它可以是d2l.sgd函数，也可以是框架的内置优化函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            <span class="comment"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用定制的优化器和损失函数</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><br>可与上一节内容进行对比<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        trainer.zero_grad()<span class="comment">#清零梯度</span></span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><br>训练函数如下,在train_iter访问到的训练数据集上训练一个模型net，改训练函数会运行多个迭代周期（由num_epochs指定）。每个迭代周期结束时，利用test_iter问到的测试数据集对模型进行评估：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    <span class="keyword">assert</span> train_loss &lt; <span class="number">0.5</span>, train_loss</span><br><span class="line">    <span class="keyword">assert</span> train_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> train_acc &gt; <span class="number">0.7</span>, train_acc</span><br><span class="line">    <span class="keyword">assert</span> test_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> test_acc &gt; <span class="number">0.7</span>, test_acc</span><br><span class="line"><span class="comment">#使用sgd</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span> d2l.sgd([W, b], lr, batch_size)</span><br><span class="line"><span class="comment">#正式训练</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br></pre></td></tr></table></figure></p><h2 id="softmax回归的简洁实现"><a href="#softmax回归的简洁实现" class="headerlink" title="softmax回归的简洁实现"></a>softmax回归的简洁实现</h2><p>由于softmax回归的输出层是一个全连接层，所以我们只需要在Sequential中添加一个带有10个输出的全连接层即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化模型训练参数</span></span><br><span class="line"><span class="comment"># PyTorch不会隐式地调整输入的形状。因此，</span></span><br><span class="line"><span class="comment"># 我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><br>若$o_k$中的一些数值非常大，那么$\exp(o_k)$可能大于数据类型容许的最大数字，即<em>上溢</em>（overflow）。解决这个问题的一个技巧是：在继续softmax计算之前，先从所有$o_k$中减去$\max(o_k)$。</p><script type="math/tex; mode=display">\begin{aligned}\hat y_j & =  \frac{\exp(o_j - \max(o_k))\exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k))\exp(\max(o_k))} \\& = \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.\end{aligned}</script><p>在减法和规范化步骤之后，可能有些$o_j - \max(o_k)$具有较大的负值。<br>由于精度受限，$\exp(o_j - \max(o_k))$将有接近零的值，即<em>下溢</em>（underflow）。<br>通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。</p><script type="math/tex; mode=display">\begin{aligned}\log{(\hat y_j)} & = \log\left( \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\& = \log{(\exp(o_j - \max(o_k)))}-\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} \\& = o_j - \max(o_k) -\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)}.\end{aligned}</script><p>交叉熵损失、优化算法、训练过程如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)<span class="comment">#自带了softmax运算</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment">#trainer可以通用</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"><span class="comment">#替换了net,loss,trainer模块</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> AI学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日常学习debug记录1</title>
      <link href="/2023/11/29/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0debug%E8%AE%B0%E5%BD%951/"/>
      <url>/2023/11/29/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0debug%E8%AE%B0%E5%BD%951/</url>
      
        <content type="html"><![CDATA[<p>记录日常的遇到的一些问题以及解决方法<br> <span id="more"></span></p><h1 id="AI-studying"><a href="#AI-studying" class="headerlink" title="AI studying"></a>AI studying</h1><ol><li><p>虚拟环境<br/><br>在anaconda Prompt中，使用<code>conda create --name d2l python=3.9 -y</code>创建虚拟环境，使用<code>activate pytorch</code>激活虚拟环境，用<code>pip list</code>查看安装列表，用<code>conda deactivate</code>来退出环境<br/><br>在虚拟环境中安装AI全家桶时注意版本的对应，使用<code>print(torch.__version__)</code>查看torch的版本；使用<code>python --version</code>查看python版本；使用<code>nvidia-smi</code>查看CUDA版本,若安装CUDA Toolkit则使用<code>nvcc -V</code>查看运行时API版本</p></li><li><p>cuda的安装<br/><br>参考连接如下：<br/><br><a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">conda换源-清华</a><br/><br><a href="https://blog.csdn.net/jhsignal/article/details/111401628">cuda和cuDNN的安装</a><br/><br><a href="https://blog.csdn.net/qq_46126258/article/details/112708781">torch的GPU版本安装的系列问题</a>,<a href="https://blog.csdn.net/youtiankeng/article/details/127759029">版本对应图</a><br/><br>1.13.0版本的pytorch对应0.14.0的torchvision和3.7到3.10的python以及0.13.0的torchaudio，为了安装GPU正确版本（0.13.0及以上才有gpu 版本）的pytorch,先删除CPU版本的pytorch,使用<code>conda uninstall pytorch</code>或是使用<code>pip uninstall torch</code>去卸载（前者未换国内源或会失败）<br/><br>然后去官网安装对应的版本<a href="https://download.pytorch.org/whl/torch/">torch</a>，<a href="https://download.pytorch.org/whl/torchvision/">torchvision</a>，<a href="https://download.pytorch.org/whl/torchaudio/">torchaudio</a><br/><br>torch-1.13.0+cu116-cp37-cp37m-win_amd64.whl，意为torch1.13.0、cuda11.6、python3.7、win 64位系统的版本,故我们安装<code>torch-1.13.0+cu117-cp37-cp37m-win_amd64.whl</code>和<code>torchvision-0.14.0+cu117-cp37-cp37m-win_amd64</code>和<code>torchaudio-0.13.0+cu117-cp37-cp37m-win_amd64</code>三个whl文件,然后进入虚拟环境安装，分别使用<code>pip install torch-1.13.0.whl+cu117-cp37-cp37m-win_amd64.whl</code>和<code>pip install torchvision-0.14.0+cu117-cp37-cp37m-win_amd64.whl</code>以及<code>pip install torchaudio-0.13.0+cu117-cp37-cp37m-win_amd64.whl</code>安装</p></li><li><p>vscode配置python运行调试环境</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 日常debug </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建记录</title>
      <link href="/2023/11/29/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/"/>
      <url>/2023/11/29/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>该文章记录了操作该博客系统所需要的指令和注意事项（自用）<br> <span id="more"></span></p><h1 id="主要参考教程"><a href="#主要参考教程" class="headerlink" title="主要参考教程"></a>主要参考教程</h1><p><a href="https://www.zhihu.com/search?type=content&amp;q=%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA">知乎博客搭建详解</a><br/></p><h1 id="相关指令"><a href="#相关指令" class="headerlink" title="相关指令"></a>相关指令</h1><ol><li>使用<code>hexo new &lt;title&gt;</code>更新文章</li><li>使用<code>hexo clean</code>清除旧的数据；使用<code>hexo g</code>生成新页面；使用<code>hexo d</code>部署到网页上，即<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></li><li>用<code>hexo s</code>预览网页效果</li><li>通过<code>https://mztom3745.com.cn</code>访问博客页面</li></ol><hr><h1 id="markdown写作"><a href="#markdown写作" class="headerlink" title="markdown写作"></a>markdown写作</h1><p>主要参考:<br/><br><a href="https://markdown.com.cn/cheat-sheet.html#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95">markdown官方教程</a><br/><br><a href="https://blog.csdn.net/wzk4869/article/details/126863936">markdown公式语法</a><br/><br><a href="https://zhuanlan.zhihu.com/p/544093361">hexo插入公式</a><br/><br><a href="https://blog.csdn.net/qq_42951560/article/details/123596899">hexo-markdown渲染器</a><br/><br>发现hexo的mathjax不支持求和、属于等公式<br><a href="https://bugwz.com/2019/09/17/hexo-markdown-renderer/#1-2%E3%80%81hexo-renderer-kramed">各个渲染版本</a><br/><br><a href="https://zhuanlan.zhihu.com/p/35988761">高级公式支持</a><br/><br><a href="https://blog.csdn.net/weixin_44441126/article/details/119745642">冲突解决</a><br/><br><a href="https://www.cnblogs.com/iHeling/p/14095986.html">kramed语义冲突</a><br/><br><a href="https://blog.csdn.net/qq_43401552/article/details/104805466">markdown+hexo+next图片插入</a><br/><br><a href="https://blog.csdn.net/qq_42951560/article/details/122552109">博客加密码</a><br/></p>]]></content>
      
      
      <categories>
          
          <category> 网页搭建 </category>
          
          <category> 个人博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习记录 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
